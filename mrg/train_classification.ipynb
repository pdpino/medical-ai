{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.handlers import Timer #, EarlyStopping\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./datasets/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./utils/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./models/classification/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./losses/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./metrics/classification/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step_fn(model, optimizer, loss_fn, training=True, device=DEVICE):\n",
    "    \"\"\"Creates a step function for an Engine.\"\"\"\n",
    "    def step_fn(engine, data_batch):\n",
    "        # Input and sizes\n",
    "        images, labels, names, _, _ = data_batch\n",
    "        n_samples, n_labels = labels.size()\n",
    "        \n",
    "        # Move tensors to GPU\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Enable training\n",
    "        model.train(training)\n",
    "        torch.set_grad_enabled(training) # enable recording gradients\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward, receive outputs from the model and segments (bboxes)\n",
    "        output_tuple = model(images)\n",
    "        outputs = output_tuple[0]\n",
    "\n",
    "        # Compute classification loss\n",
    "        loss = loss_fn(outputs, labels.float())\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "\n",
    "        if training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        return batch_loss, outputs, labels\n",
    "\n",
    "    return step_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader, n_epochs=1, lr=0.0001,\n",
    "               loss_name='wbce'):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss = get_loss_function(loss_name)\n",
    "    \n",
    "    labels = dataloader.dataset.labels\n",
    "\n",
    "    # Create validator engine\n",
    "    validator = Engine(get_step_fn(model, optimizer, loss, training=False))\n",
    "    attach_metrics_classification(validator, labels, loss_name)\n",
    "    \n",
    "    # Create trainer engine\n",
    "    trainer = Engine(get_step_fn(model, optimizer, loss, training=True))\n",
    "    attach_metrics_classification(trainer, labels, loss_name)\n",
    "    \n",
    "    # Create Timer to measure wall time between epochs\n",
    "    timer = Timer(average=True)\n",
    "    timer.attach(trainer, start=Events.EPOCH_STARTED, step=Events.EPOCH_COMPLETED)\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def tb_write_metrics(trainer):\n",
    "        epoch = trainer.state.epoch\n",
    "        max_epochs = trainer.state.max_epochs\n",
    "\n",
    "        # Run on evaluation\n",
    "        validator.run(val_dataloader, 1)\n",
    "\n",
    "        # Common time\n",
    "        wall_time = time.time()\n",
    "\n",
    "        train_loss = trainer.state.metrics.get(loss_name, 0)\n",
    "        val_loss = validator.state.metrics.get(loss_name, 0)\n",
    "        \n",
    "        loss_str = f'loss {train_loss:.4f}, {val_loss:.4f}'\n",
    "        duration_str = duration_to_str(timer._elapsed())\n",
    "        print(f'Finished epoch {epoch}/{max_epochs}, {loss_str} (took {duration_str})')\n",
    "\n",
    "    # Train!\n",
    "    print('-' * 50)\n",
    "    print('Training...')\n",
    "    trainer.run(train_dataloader, n_epochs)\n",
    "    \n",
    "\n",
    "    # Capture time\n",
    "    secs_per_epoch = timer.value()\n",
    "    duration_per_epoch = duration_to_str(secs_per_epoch)\n",
    "    print('Average time per epoch: ', duration_per_epoch)\n",
    "    print('-'*50)\n",
    "\n",
    "    return trainer.state.metrics, validator.state.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset...\n",
      "Loading val dataset...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 14)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = prepare_data_classification(dataset_type='train', max_images=100)\n",
    "val_dataloader = prepare_data_classification(dataset_type='val', max_images=100)\n",
    "train_dataloader.dataset.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_empty_model('resnet', dataloader.dataset.labels).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Training...\n",
      "Finished epoch 1/10, loss 0.2112, 0.2235 (took 0h 0m 4s)\n",
      "Finished epoch 2/10, loss 0.0704, 0.1989 (took 0h 0m 4s)\n",
      "Finished epoch 3/10, loss 0.0544, 0.1791 (took 0h 0m 4s)\n",
      "Finished epoch 4/10, loss 0.0624, 0.1479 (took 0h 0m 4s)\n",
      "Finished epoch 5/10, loss 0.0533, 0.1285 (took 0h 0m 4s)\n",
      "Finished epoch 6/10, loss 0.0559, 0.1255 (took 0h 0m 4s)\n",
      "Finished epoch 7/10, loss 0.0592, 0.1287 (took 0h 0m 4s)\n",
      "Finished epoch 8/10, loss 0.0528, 0.1273 (took 0h 0m 4s)\n",
      "Finished epoch 9/10, loss 0.0526, 0.1253 (took 0h 0m 4s)\n",
      "Finished epoch 10/10, loss 0.0524, 0.1255 (took 0h 0m 4s)\n",
      "Average time per epoch:  0h 0m 4s\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_metrics, val_metrics = train_model(model, train_dataloader, val_dataloader,\n",
    "                                         n_epochs=10, loss_name='bce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('acc_Atelectasis', 0.95),\n",
       " ('acc_Cardiomegaly', 0.99),\n",
       " ('acc_Effusion', 0.89),\n",
       " ('acc_Infiltration', 0.83),\n",
       " ('acc_Mass', 0.95),\n",
       " ('acc_Nodule', 0.97),\n",
       " ('acc_Pneumonia', 0.99),\n",
       " ('acc_Pneumothorax', 0.97),\n",
       " ('acc_Consolidation', 0.93),\n",
       " ('acc_Edema', 0.99),\n",
       " ('acc_Emphysema', 0.98),\n",
       " ('acc_Fibrosis', 1.0),\n",
       " ('acc_Pleural_Thickening', 0.98),\n",
       " ('acc_Hernia', 1.0)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, v) for k, v in val_metrics.items() if 'acc' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "255px",
    "width": "188px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.facecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../models/report_generation/__init__.py\n",
    "%run ../models/checkpoint/__init__.py\n",
    "%run ../utils/files.py\n",
    "%run ../utils/nlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_name = '0716_211601_lstm-att_lr0.0001_densenet-121'\n",
    "# run_name = '0115_175006_h-lstm-att-v2_lr0.001_satt_densenet-121-v2_noes'\n",
    "# run_name = '0115_064249_h-lstm-att-v2_lr0.001_densenet-121-v2_noes_front'\n",
    "# run_name = '0401_222625_mimic-cxr_lstm-v2_lr0.0001_mobilenet-v2_size256'\n",
    "\n",
    "# run_name = '0513_123117' # lstm\n",
    "# run_name = '0513_145846' # lstm-att\n",
    "# run_name = '0513_174148' # h-lstm\n",
    "# run_name = '0513_200618' # h-lstm-att\n",
    "\n",
    "run_name = '0607_002702'\n",
    "\n",
    "run_id = RunId(run_name, False, 'rg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = load_compiled_model_report_generation(run_id, device=DEVICE)\n",
    "_ = compiled_model.model.eval()\n",
    "compiled_model.metadata['decoder_kwargs'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = compiled_model.metadata['decoder_kwargs']['vocab']\n",
    "len(VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPORT_READER = ReportReader(VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_name = compiled_model.metadata['decoder_kwargs']['decoder_name']\n",
    "HIERARCHICAL = is_decoder_hierarchical(decoder_name)\n",
    "HIERARCHICAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../datasets/__init__.py\n",
    "%run ../utils/nlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dataset_kwargs = compiled_model.metadata['dataset_kwargs']\n",
    "dataset_kwargs = {\n",
    "    **model_dataset_kwargs,\n",
    "    'sort_samples': True,\n",
    "    'shuffle': False,\n",
    "    'batch_size': 2,\n",
    "}\n",
    "\n",
    "train_dataloader = prepare_data_report_generation(dataset_type='train', **dataset_kwargs)\n",
    "val_dataloader = prepare_data_report_generation(dataset_type='val', **dataset_kwargs)\n",
    "test_dataloader = prepare_data_report_generation(dataset_type='test', **dataset_kwargs)\n",
    "len(train_dataloader.dataset), len(val_dataloader.dataset), len(test_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../training/report_generation/hierarchical.py\n",
    "%run ../training/report_generation/flat.py\n",
    "%run ../utils/common.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_sample(batch, device=DEVICE, free=False, **kwargs):\n",
    "    # Prepare inputs\n",
    "#     images = item.image.unsqueeze(0).to(device)\n",
    "#     report = item.report\n",
    "#     if HIERARCHICAL:\n",
    "#         reports = split_sentences_and_pad(report)\n",
    "#     else:\n",
    "#         reports = torch.tensor(report)\n",
    "\n",
    "#     reports = reports.unsqueeze(0).to(device)\n",
    "    \n",
    "    images = batch.images.to(device)\n",
    "    reports = batch.reports.to(device)\n",
    "    \n",
    "    # Pass thru model\n",
    "    if not HIERARCHICAL:\n",
    "        del kwargs['max_sentences']\n",
    "    tup = compiled_model.model(images, reports, free=free, **kwargs)\n",
    "    \n",
    "    # Parse outputs\n",
    "    if HIERARCHICAL:\n",
    "        generated = _flatten_gen_reports(tup[0], tup[1])\n",
    "    else:\n",
    "        generated = _clean_gen_reports(tup[0])\n",
    "\n",
    "    return reports, generated, tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_text(reports):\n",
    "    return [REPORT_READER.idx_to_text(r) for r in reports]\n",
    "\n",
    "def print_result(reports, generated):\n",
    "    def print_list(l):\n",
    "        for x in l:\n",
    "            print(f'\\t{x}')\n",
    "\n",
    "    reports = to_text(reports)\n",
    "    generated = to_text(generated)\n",
    "    print('GROUND TRUTH:')\n",
    "    print_list(reports)\n",
    "    print('-'*20)\n",
    "    print('GENERATED:')\n",
    "    print_list(generated)\n",
    "    return reports, generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check stops array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dataloader = iter(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _ in range(0):\n",
    "batch = next(iter_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt, gen, out = eval_sample(batch, free=False, max_sentences=100, max_words=40)\n",
    "gt_str, gen_str = print_result(gt, gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = out[1].detach().cpu()\n",
    "print(stops.size())\n",
    "print(batch.stops.size())\n",
    "stops > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = loss_fn(stops, batch.stops)\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import visualization\n",
    "from skimage.color import gray2rgb\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = out[2].detach().squeeze(0).cpu()\n",
    "scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_color = image.detach().permute(1, 2, 0).cpu().numpy()\n",
    "image_color = arr_to_range(image_color)\n",
    "\n",
    "image_color.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_idx = 1\n",
    "heatmap = scores[sentence_idx].numpy()\n",
    "heatmap = gray2rgb(heatmap)\n",
    "heatmap = resize(heatmap, image_color.shape)\n",
    "heatmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPORT_READER.idx_to_text(out_words[sentence_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.visualize_image_attr_multiple(heatmap,\n",
    "                                            image_color,\n",
    "                                            methods=['original_image',\n",
    "                                                     'blended_heat_map'],\n",
    "                                            signs=['all', 'positive'],\n",
    "                                            cmap='jet',\n",
    "                                            show_colorbar=True,\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Check result-generated reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run ../metrics/report_generation/writer.py\n",
    "%run ../utils/files.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_generated_reports(run_id, free=True):\n",
    "    fpath = _get_outputs_fpath(run_id, free=free)\n",
    "    \n",
    "    df = pd.read_csv(fpath)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "run_ids = [\n",
    "    RunId(name, False, 'rg')\n",
    "    for name in (\n",
    "        '0513_123117', # lstm\n",
    "        '0513_145846', # lstm-att\n",
    "        '0513_174148', # h-lstm\n",
    "        '0513_200618', # h-lstm-att\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TOTAL_DF = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for run_id in run_ids:\n",
    "    df = load_generated_reports(run_id)\n",
    "    del df['epoch']\n",
    "    \n",
    "    gen_col_name = f'gen-{run_id.short_clean_name}'\n",
    "    df.rename(columns={'generated': gen_col_name}, inplace=True)\n",
    "    \n",
    "    if TOTAL_DF is None:\n",
    "        df = df[['image_fname', 'filename', 'dataset_type', 'ground_truth', gen_col_name]]\n",
    "        TOTAL_DF = df\n",
    "    else:\n",
    "        df = df[['image_fname', gen_col_name]]\n",
    "        TOTAL_DF = TOTAL_DF.merge(df, on='image_fname', how='outer')\n",
    "        \n",
    "TOTAL_DF = TOTAL_DF.sort_values(by='ground_truth', key=lambda x: x.str.len())\n",
    "TOTAL_DF.reset_index(drop=True, inplace=True)\n",
    "len(TOTAL_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TOTAL_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def print_sample(idx):\n",
    "    def _print_report(report, name):\n",
    "        print(name)\n",
    "        print(report)\n",
    "        print('-'*30)\n",
    "\n",
    "    row = TOTAL_DF.iloc[idx]\n",
    "    \n",
    "    print(f\"{row['filename']} {row['image_fname']} ({row['dataset_type']})\")\n",
    "    \n",
    "    gt = row['ground_truth']\n",
    "    _print_report(gt, 'GT')\n",
    "\n",
    "    gen_cols = [c for c in TOTAL_DF.columns if c.startswith('gen-')]    \n",
    "    for col in gen_cols:\n",
    "        gen = row[col]\n",
    "        _print_report(gen, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_sample(-101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

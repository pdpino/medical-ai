{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check sentences, looking for templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../metrics/report_generation/nlp/cider_idf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../metrics/report_generation/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_name = '1108_154558' # lstm-att\n",
    "run_name = '1113_185718' # SAT\n",
    "run_id = RunId(run_name, task='rg', debug=False)\n",
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tpl = load_rg_outputs(RunId('1102_190559', False, 'rg'), free=True, labeled=True)\n",
    "df_tpl.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tpl = df_tpl[df_tpl['dataset_type'] == 'test']\n",
    "df_tpl.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_rg_outputs(run_id, free=True, best='bleu4', labeled=True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = df.loc[df['dataset_type'] == 'test']\n",
    "len(subdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top out reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Counter(subdf['generated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sorted(s.items(), key=lambda x: x[1], reverse=True)\n",
    "s[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 'frontal and lateral views of the chest were obtained . the lungs are clear without focal consolidation . no pleural effusion or pneumothorax is seen . the cardiac and mediastinal silhouettes are unremarkable .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 'pa and lateral chest radiographs were obtained . the lungs are well expanded and clear . there is no focal consolidation , effusion , or pneumothorax . the cardiomediastinal silhouette is normal .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHEX_GEN = labels_with_suffix('gen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = subdf.loc[subdf['generated'] == r]\n",
    "len(d)\n",
    "d[CHEX_GEN].apply(Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = apply_labeler_to_column([r])\n",
    "list(zip(CHEXPERT_DISEASES, labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils/nlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_fpath = os.path.join(MIMIC_DIR, 'reports', 'sentences_with_chexpert_labels.csv')\n",
    "SENTENCES_DF = pd.read_csv(_fpath)\n",
    "SENTENCES_DF.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "d = SENTENCES_DF.set_index('sentence').replace({-2: 0, -1: 1}).transpose().to_dict('list')\n",
    "d['no acute intrathoracic process .']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_counter_abn = Counter()\n",
    "# sentence_counter_health = Counter()\n",
    "sentence_counter = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_diseases = labels_with_suffix('gen')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "#     report = row['generated']\n",
    "#     diseases = row[actual_diseases]\n",
    "#     chosen = sentence_counter_abn if diseases.sum() >= 1 else sentence_counter_health\n",
    "#     for sentence in split_sentences_text(report):\n",
    "#         chosen[sentence] += 1\n",
    "\n",
    "for report in list(df['generated']):\n",
    "    for sentence in split_sentences_text(report):\n",
    "        sentence_counter[sentence] += 1\n",
    "        \n",
    "len(sentence_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sorted(sentence_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "s[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_h, s_other = [], []\n",
    "s_by_abn = defaultdict(list)\n",
    "for tup in s:\n",
    "    sentence = tup[0]\n",
    "\n",
    "    labels = d.get(sentence)\n",
    "    if labels is None:\n",
    "        s_other.append(tup)\n",
    "        continue\n",
    "    \n",
    "    if np.sum(labels[1:]) == 0:\n",
    "        s_h.append(tup)\n",
    "        continue\n",
    "\n",
    "    for valoration, disease in zip(labels[1:], actual_diseases):\n",
    "        # TODO: prioritize sentences with more than one valoration == 1??\n",
    "        if valoration == 1:\n",
    "            s_by_abn[disease].append(tup)\n",
    "        \n",
    "len(s_by_abn), len(s_h), len(s_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_by_abn['Lung Lesion-gen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 'again there is enlargement of the cardiac silhouette with some elevation of pulmonary venous pressure and bilateral pleural effusions with compressive atelectasis at the bases .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = apply_labeler_to_column([r])\n",
    "list(zip(CHEXPERT_DISEASES, labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [\n",
    "    r\n",
    "    for r in df['ground_truth']\n",
    "    if 'in the appropriate clinical setting' in r\n",
    "]\n",
    "len(l), l[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.facecolor'] = 'white'\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reports = list(df_train['ground_truth'])\n",
    "doc_freq = compute_doc_freq(reports)\n",
    "len(doc_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = doc_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = sorted(s.items(), key=lambda x: x[1], reverse=True)\n",
    "s[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ngrams = lambda n: [(ngram, amount) for ngram, amount in s if len(ngram) == n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = get_ngrams(1)\n",
    "bigrams = get_ngrams(2)\n",
    "trigrams = get_ngrams(3)\n",
    "quatrigrams = get_ngrams(4)\n",
    "assert len(unigrams) + len(bigrams) + len(trigrams) + len(quatrigrams) == len(doc_freq)\n",
    "len(unigrams), len(bigrams), len(trigrams), len(quatrigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quatrigrams[30:60]\n",
    "# bigrams[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amounts = [amount for _, amount in quatrigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(amounts)\n",
    "plt.yscale('log', base=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [(ngram, amount) for ngram, amount in quatrigrams if amount > 0]\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l[5000:5050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = 'non cardiac edema in the lungs .'\n",
    "# r = 'the hilar and mediastinal contours are within normal limits .'\n",
    "# r = 'pa and lateral chest compared to xxxx normal heart , lungs , hila , mediastinum and pleural surfaces . no evidence of intrathoracic malignancy or infection .'\n",
    "r = 'no evidence of intrathoracic malignancy or infection .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = apply_labeler_to_column([r])\n",
    "list(zip(CHEXPERT_LABELS, labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = ('reflects', ',', 'in', 'part')\n",
    "# target = ('change', 'since', 'prior', 'cxr')\n",
    "# target = ('silhouette', 'is', 'prominent', 'likely')\n",
    "# target = ('there', 'is', 'cardiomegaly', '.')\n",
    "# target = ('effusions', 'are', 'noted', '.')\n",
    "target = ('mediastinal', 'and', 'hilar', 'contours')\n",
    "# rr = defaultdict(list)\n",
    "rr = []\n",
    "target_n = len(target)\n",
    "for report in reports:\n",
    "    words = report.split()\n",
    "    n_words = len(words)\n",
    "    for i in range(0, n_words-target_n):\n",
    "        subreport = set(words[i:i+target_n])\n",
    "        if all(t in subreport for t in target):\n",
    "            rr.append(report)\n",
    "#         count = sum(int(t in subreport) for t in target)\n",
    "#         if count > 0:\n",
    "#             rr[count].append(report)\n",
    "            break\n",
    "len(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils/nlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./analyze_nlp_chexpert/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tpl = add_nlp_metrics_to_df(df_tpl)\n",
    "df_tpl.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = add_nlp_metrics_to_df(subdf)\n",
    "subdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['ground_truth', 'generated', 'ciderD'] + _GEN_LABELS + _GT_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tpl = df_tpl[cols].sort_values('ciderD', ascending=False)\n",
    "df_tpl.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = subdf[cols].sort_values('ciderD', ascending=False)\n",
    "subdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = subdf\n",
    "d = d.loc[((d['No Finding-gen'] == 1) & (d['Support Devices-gen'] == 0))]\n",
    "d.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Enlarged Cardiomediastinum'\n",
    "df_tpl.plot(x=f'{target}-gen', y='ciderD', kind='scatter', title=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_GEN_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_abngroup_col_to_df(df, suffix):\n",
    "    group_col = []\n",
    "    \n",
    "    labels = labels_with_suffix(suffix)[1:] # ignore NF\n",
    "    to_remove = f'-{suffix}'\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        abnormalities = row[labels]\n",
    "        group = \",\".join(\n",
    "            f\"{ABN_SHORTCUTS[abn.replace(to_remove, '')]}={value}\"\n",
    "            for abn, value in abnormalities.iteritems()\n",
    "        )\n",
    "        group_col.append(group)\n",
    "        \n",
    "    return df.assign(**{f'group-{suffix}': group_col})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tpl = add_abngroup_col_to_df(df_tpl, 'gen')\n",
    "df_tpl = add_abngroup_col_to_df(df_tpl, 'gt')\n",
    "df_tpl.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df_tpl\n",
    "# d = d.loc[d['group-gen'] == d['group-gt']]\n",
    "score_by_group = d.groupby('group-gen')['ciderD'].apply(sum).sort_values(ascending=True)\n",
    "score_by_group.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_groups = list(score_by_group.index[:10])\n",
    "target_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df_tpl\n",
    "d = d.loc[d['group-gen'].isin(target_groups)]\n",
    "e = d.loc[d['group-gen'] == d['group-gt']] # Correct ones\n",
    "f'{len(e)}/{len(d)}', target_group, d['ciderD'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df_tpl\n",
    "d = d.sort_values('ciderD', ascending=True)\n",
    "d = d.loc[d['group-gen'] == d['group-gt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = defaultdict(list)\n",
    "for _, row in d.iterrows():\n",
    "    report = row['ground_truth']\n",
    "    ciderD = row['ciderD']\n",
    "    for sentence in split_sentences_text(report):\n",
    "        c[sentence].append(ciderD)\n",
    "len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = sorted(c.items(), key=lambda x: sum(x[1]), reverse=False)\n",
    "cc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc[250:260]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = 'mediastinal and hilar silhouettes are unremarkable and no pleural abnormality .'\n",
    "# r = 'no pleural abnormality .'\n",
    "# r = 'there is no pneumothorax , vascular congestion , or pleural effusion .'\n",
    "# r = 'there is no evidence of pleural effusions .'\n",
    "# r = 'the lungs are clear without infiltrate or effusion .'\n",
    "# r = 'there is no pulmonary edema , consolidation , or pleural effusion .'\n",
    "# r = 'in comparison with the study of xxxx , there is again some enlargement of the cardiac silhouette without definite vascular congestion , pleural effusion , or acute focal pneumonia .'\n",
    "# r = 'the cardiomediastinal silhouette , pulmonary vasculature , and aorta are within normal limits .'\n",
    "# r = 'cardiomediastinal and hilar silhouettes and pleural surfaces are normal .'\n",
    "# r = 'the heart is normal in size and there is no vascular congestion , pleural effusion , or acute focal pneumonia .'\n",
    "r = 'the heart is normal in size , and the mediastinal contours are normal .'\n",
    "l = apply_labeler_to_column([r])\n",
    "list(zip(CHEXPERT_DISEASES, l[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter()\n",
    "for report in d['ground_truth']:\n",
    "    for sentence in split_sentences_text(report):\n",
    "        c[sentence] += 1\n",
    "        pass\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = sorted(c.items(), key=lambda x: x[1], reverse=True)\n",
    "cc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check sentences by most common groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils/nlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_fpath = os.path.join(MIMIC_DIR, 'reports', 'sentences_with_chexpert_labels.csv')\n",
    "SENTENCES_DF = pd.read_csv(_fpath)\n",
    "SENTENCES_DF.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df.loc[df['dataset_type'] == 'test']\n",
    "d.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = list(d['ground_truth'])\n",
    "sentences = [s for r in reports for s in split_sentences_text(r)]\n",
    "len(reports), len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeler = ChexpertLabeler(fill_empty=-2, fill_uncertain=-1, caller_id='notebook')\n",
    "labeler = CacheLookupLabeler(labeler, SENTENCES_DF, text_key='sentence')\n",
    "labeler = NBatchesLabeler(labeler)\n",
    "labeler = AvoidDuplicatedLabeler(labeler)\n",
    "labeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "labels = labeler(sentences)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = pd.DataFrame(labels, index=sentences, columns=CHEXPERT_DISEASES)\n",
    "sentences_df = sentences_df.reset_index().rename(columns={'index': 'sentence'})\n",
    "sentences_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it for convenience\n",
    "_fpath = os.path.join(MIMIC_DIR, 'reports', 'sentences_with_chexpert_labels.new_test.csv')\n",
    "assert not os.path.isfile(_fpath)\n",
    "# sentences_df.to_csv(_fpath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = pd.read_csv(_fpath).replace({-1: 1})\n",
    "sentences_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_abngroup_col_to_df(df, targets=(0,1)):\n",
    "    group_col = []\n",
    "    \n",
    "    labels = CHEXPERT_DISEASES[1:] # ignore NF\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        abnormalities = row[labels]\n",
    "        group = \";\".join(\n",
    "            f\"{ABN_SHORTCUTS[abn]}={value}\"\n",
    "            for abn, value in abnormalities.iteritems()\n",
    "            if value in targets\n",
    "        )\n",
    "        group_col.append(group)\n",
    "        \n",
    "    return df.assign(**{'group': group_col})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = add_abngroup_col_to_df(sentences_df)\n",
    "sentences_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['sentence', 'group']\n",
    "df = sentences_df[cols]\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = df.groupby('sentence')['group'].apply(lambda x: len(set(x.values)))\n",
    "(e == 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['appearances'] = df.groupby('sentence')['sentence'].transform('size')\n",
    "df['appearances'] = df['sentence'].map(df['sentence'].value_counts())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby('sentence').first().reset_index().sort_values('appearances', ascending=False)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos, n_neg = zip(*[\n",
    "    (group.count('1'), group.count('0'))\n",
    "    for group in df['group']\n",
    "])\n",
    "df = df.assign(n_pos=n_pos, n_neg=n_neg)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df\n",
    "ready = set(['Pt=0;PE=0','Cons=0;Pt=0;PE=0','EC=1;Card=1','Cons=0;Pt=0','EC=0;Card=0'])\n",
    "# d = d.loc[~d['group'].isin(ready)]\n",
    "d = d.loc[d['appearances'] >= 2]\n",
    "# d = d.loc[d['n_pos'] >= 2]\n",
    "d = d.loc[d['group'].str.contains(r'\\bE=1')]\n",
    "# d = d.loc[(d['n_neg'] + d['n_pos'] >= 2)]\n",
    "d = d.sort_values(['n_neg', 'appearances'], ascending=False)\n",
    "print(len(d))\n",
    "d.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(d['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tpl['No Finding-gen'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = df_tpl\n",
    "dd = dd.loc[dd['No Finding-gen'] == 0]\n",
    "print(len(dd))\n",
    "dd.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd['No Finding-gt'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd2 = dd\n",
    "# dd2 = dd2.loc[dd2['No Finding-gt'] == 0]\n",
    "# Correct ones:\n",
    "dd2 = dd2.loc[dd2[_GEN_LABELS].eq(dd2[_GT_LABELS].rename(columns=dict(zip(_GT_LABELS, _GEN_LABELS))), axis='index').all(axis=1)]\n",
    "dd2 = dd2.loc[dd2['ciderD'] <= 0.4]\n",
    "print(len(dd2))\n",
    "# dd2.plot(x='No Finding-gt', y='ciderD', kind='hist', bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd2 = dd2.sort_values('ground_truth', key=lambda x: x.str.len(), ascending=True)\n",
    "dd2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = Counter([s for r in dd2['ground_truth'] for s in split_sentences_text(r)])\n",
    "ss = sorted(ss.items(), key=lambda x: x[1], reverse=True)\n",
    "ss[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dd2['ground_truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check train-val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tpl = load_rg_outputs(RunId('1102_190559', False, 'rg'), free=True, labeled=True)\n",
    "df_tpl.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../models/report_generation/templates/chex_group_mimic.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = df_tpl.loc[df_tpl['dataset_type'] == 'train']\n",
    "len(dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_counter = Counter([s for r in dtrain['ground_truth'] for s in split_sentences_text(r)])\n",
    "# sent_counter = dict(sorted(sent_counter.items(), key=lambda x: x[1], reverse=True))\n",
    "sent_counter['m .']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appears = lambda text: sum(c for r, c in sent_counter.items() if text in r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in GROUPS_v8:\n",
    "    sentence = group[2]\n",
    "    print(sentence, appears(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for abn, d in CHEX_mimic_single.items():\n",
    "    print(abn)\n",
    "    for v, sent in d.items():\n",
    "        if sent:\n",
    "            print(f'\\t\"{sent}\": {appears(sent)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_fpath = os.path.join(MIMIC_DIR, 'reports', 'sentences_with_chexpert_labels.csv')\n",
    "SENTENCES_DF = pd.read_csv(_fpath).replace({-1:1})\n",
    "SENTENCES_DF['appearances'] = [sent_counter.get(sent, 0) for sent in SENTENCES_DF['sentence']]\n",
    "SENTENCES_DF.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = SENTENCES_DF\n",
    "target = 'Lung Lesion'\n",
    "others = list(CHEXPERT_DISEASES)\n",
    "others.remove(target)\n",
    "d = d.loc[((d[target] == 1) & (d[others] == -2).all(axis=1))]\n",
    "print(len(d))\n",
    "d = d.sort_values(['appearances', 'sentence'], ascending=False)\n",
    "d = d[['sentence', 'appearances']]\n",
    "d.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(d['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(r['sentence'], r['appearances']) for _, r in d.iterrows() if 'fibrosis' in r['sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCES_DF = add_abngroup_col_to_df(SENTENCES_DF)\n",
    "SENTENCES_DF.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-eval with chex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../models/report_generation/templates/chex_group_mimic.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [s for d in CHEX_mimic_single.values() for s in d.values() if s]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = apply_labeler_to_column(sentences)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [group[2] for group in GROUPS_v8]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = apply_labeler_to_column(sentences)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_single = load_rg_outputs(RunId('1102_190559', False, 'rg'), free=True, labeled=True)\n",
    "df_single = df_single.loc[df_single['dataset_type'] == 'test']\n",
    "df_single.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_grouped = load_rg_outputs(RunId('1129_191853', False, 'rg'), free=True, labeled=True)\n",
    "df_grouped = df_grouped.loc[df_grouped['dataset_type'] == 'test']\n",
    "df_grouped.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['image_fname', 'ground_truth', 'generated', 'No Finding-gt', 'No Finding-gen']\n",
    "df1 = df_single[cols].reset_index(drop=True)\n",
    "df2 = df_grouped[cols].reset_index(drop=True)\n",
    "assert len(df1) == len(df2)\n",
    "len(df1), len(df1['image_fname'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = df1.merge(df2, on=['image_fname', 'ground_truth', 'No Finding-gt'],\n",
    "                   how='inner', validate='one_to_one')\n",
    "assert len(merged) == len(df1), f'{len(merged)} vs {len(df1)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch = merged.loc[merged['No Finding-gen_x'] != merged['No Finding-gen_y']]\n",
    "mismatch.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch['No Finding-gt'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single = list(set(mismatch['generated_x']))\n",
    "grouped = list(set(mismatch['generated_y']))\n",
    "len(single), len(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single = single[0]\n",
    "grouped = grouped[0]\n",
    "single, grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 'there is no focal consolidation , pleural effusion , or pneumothorax . heart size is normal . the cardiomediastinal silhouette is normal . there is no evidence of fibrosis . no displaced fracture is seen . tube in standard placement .'    \n",
    "labels = apply_labeler_to_column([single, grouped, r], fill_empty=-3)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(CHEXPERT_DISEASES, zip(*labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len([r for r in df_single['ground_truth'] if 'tracheostomy' in r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

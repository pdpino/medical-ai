{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.facecolor'] = 'white'\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../utils/__init__.py\n",
    "config_logging(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../datasets/common/constants.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medai.datasets import iu_xray, mimic_cxr\n",
    "IU_DIR = iu_xray.DATASET_DIR\n",
    "MIMIC_DIR = mimic_cxr.DATASET_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = os.path.join(IU_DIR, 'reports', 'sentences_with_chexpert_labels.csv')\n",
    "SENTENCES_DF = pd.read_csv(fpath)\n",
    "SENTENCES_DF.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word appearances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = Counter()\n",
    "for index, row in SENTENCES_DF.iterrows():\n",
    "    appearances = row['appearances']\n",
    "    for word in row['sentence'].split():\n",
    "        word_counter[word] += appearances\n",
    "        \n",
    "len(word_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_appearances = list(word_counter.items())\n",
    "word_appearances = sorted(word_appearances, key=lambda x: x[1], reverse=True)\n",
    "word_appearances[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_appearances[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab keywords from TextRay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../metrics/report_generation/abn_match/textray.py\n",
    "%run ../../datasets/vocab/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = load_vocab(os.path.join(IU_DIR, 'reports'), 'v4')\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = set()\n",
    "\n",
    "def resolve_pattern(pattern):\n",
    "    if isinstance(pattern, str):\n",
    "        for word in vocab:\n",
    "            if re.search(pattern, word):\n",
    "                keywords.add(word)\n",
    "        return\n",
    "    for p in pattern:\n",
    "        resolve_pattern(p)\n",
    "for key, value in _TEXTRAY_PATTERNS.items():\n",
    "    resolve_pattern(value)\n",
    "\n",
    "len(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(vocab) - keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding with keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_to_index = {\n",
    "    word: idx\n",
    "    for idx, word in enumerate(keywords)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings_onehot_keywords(sentences):\n",
    "    embeddings = np.zeros((len(sentences), len(keywords)))\n",
    "\n",
    "    for sentence_index, sentence in enumerate(sentences):\n",
    "        for word in sentence.split():\n",
    "            onehot_index = keyword_to_index.get(word, -1)\n",
    "            if onehot_index != -1:\n",
    "                embeddings[sentence_index, onehot_index] = 1\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = compute_embeddings(list(SENTENCES_DF['sentence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding with RadGlove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../models/report_generation/word_embedding.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radglove = RadGlove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings_radglove(sentences):\n",
    "    embeddings = []\n",
    "    for sentence_index, sentence in enumerate(sentences):\n",
    "        sentence_embedding = np.zeros(100)\n",
    "        \n",
    "        for word in sentence.split():\n",
    "            sentence_embedding += radglove[word].numpy()\n",
    "            \n",
    "        embeddings.append(sentence_embedding)\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = compute_embeddings_radglove(SENTENCES_DF['sentence'])\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_cluster(cluster_instance, sentences):\n",
    "    clusters = defaultdict(list)\n",
    "    assert len(sentences) == len(cluster_instance.labels_)\n",
    "    for sentence, label in zip(sentences, cluster_instance.labels_):\n",
    "        clusters[label].append(sentence)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=40, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "kmeans.fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = group_by_cluster(kmeans, SENTENCES_DF['sentence'])\n",
    "\n",
    "{k:len(v) for k, v in clusters.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dbscan = DBSCAN(eps=20) # , metric='manhattan'\n",
    "dbscan.fit(embeddings)\n",
    "len(dbscan.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = group_by_cluster(dbscan, SENTENCES_DF['sentence'])\n",
    "\n",
    "len(clusters), len(clusters[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outsider_sentences = clusters[-1]\n",
    "emb2 = compute_embeddings(outsider_sentences)\n",
    "emb2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dbscan = DBSCAN(eps=1, metric='manhattan')\n",
    "dbscan.fit(emb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters2 = group_by_cluster(dbscan, outsider_sentences)\n",
    "\n",
    "len(clusters2), len(clusters2[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters2[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(SENTENCES_DF['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s for s in sentences if re.search(r'reflecting', s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../metrics/report_generation/chexpert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = apply_labeler_to_column([\n",
    "    'no focal airspace consolidations',\n",
    "    'no focal airspace consolidations.',\n",
    "    'no consolidations',\n",
    "    'no consolidations .',\n",
    "    'no focal consolidations',\n",
    "    'no focal consolidations .',\n",
    "    'no airspace consolidations',\n",
    "    'no airspace consolidations .',\n",
    "], caller_id='testing-consolidations')\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By vocab embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(SENTENCES_DF['sentence'])\n",
    "l[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_overlap(sentence1, sentence2):\n",
    "    s1 = sentence1.split()\n",
    "    s2 = sentence2.split()\n",
    "    intersection = set(s1).intersection(set(s2))\n",
    "    union = len(s1) + len(s2)\n",
    "    return 2 * len(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = l[0], l[2]\n",
    "a, b, word_overlap(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlap_matrix(sentences):\n",
    "    def _word_overlap(s1, s2):\n",
    "        intersection = s1.intersection(s2)\n",
    "        union = len(s1) + len(s2)\n",
    "        return 2 * len(intersection) / (union + 1e-5)\n",
    "    sentences = [\n",
    "        set([word for word in sentence.split()]) # if word in keywords\n",
    "        for sentence in sentences\n",
    "    ]\n",
    "    n_sentences = len(sentences)\n",
    "    embeddings = np.zeros((n_sentences, n_sentences))\n",
    "    for i in tqdm(range(n_sentences)):\n",
    "        si = sentences[i]\n",
    "        for j in range(i+1, n_sentences):\n",
    "            sj = sentences[j]\n",
    "            overlap = _word_overlap(si, sj)\n",
    "            \n",
    "            embeddings[i, j] = embeddings[j, i] = overlap\n",
    "            \n",
    "        embeddings[i, i] = 1\n",
    "            \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlaps = compute_overlap_matrix(SENTENCES_DF['sentence'])\n",
    "overlaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distances = 1 - overlaps\n",
    "distances = overlaps.copy()\n",
    "distances[distances==0] = 1\n",
    "distances = 1/distances - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WIP: selecting a \"handful\" of keywords for sentence clustering\n",
    "# 'aorta',\n",
    "# 'aortic',\n",
    "# 'aorticopulmonary',\n",
    "# 'aorto',\n",
    "# 'arthritic',\n",
    "# 'arthritis',\n",
    "# 'atelectasis',\n",
    "# 'atelectatic',\n",
    "# 'blunted',\n",
    "# 'blunting',\n",
    "# 'bochdalek',\n",
    "# 'bone',\n",
    "# 'bony',\n",
    "# 'bronchial',\n",
    "# 'bronchiectatic',\n",
    "# 'bronchopleural',\n",
    "# 'bronchopulmonary',\n",
    "# 'bronchovascular',\n",
    "# 'calcific',\n",
    "# 'calcification',\n",
    "# 'calcifications',\n",
    "# 'calcified',\n",
    "# 'cardiac',\n",
    "# 'cardiomediastinal',\n",
    "# 'cardiomegaly',\n",
    "# 'catheter',\n",
    "# 'chf',\n",
    "# 'cholecystectomy',\n",
    "# 'clips',\n",
    "# 'congestion',\n",
    "# 'consolidated',\n",
    "# 'consolidating',\n",
    "# 'consolidation',\n",
    "# 'consolidations',\n",
    "# 'consolidative',\n",
    "# 'contour',\n",
    "# 'contours',\n",
    "# 'costodiaphragmatic',\n",
    "# 'costophrenic',\n",
    "# 'cyst',\n",
    "# 'cystic',\n",
    "# 'degenerative',\n",
    "# 'dextrocurvature',\n",
    "# 'dextroscoliosis',\n",
    "# 'diaphragm',\n",
    "# 'diaphragmatic',\n",
    "# 'diaphragms',\n",
    "# 'ectasia',\n",
    "# 'ectatic',\n",
    "# 'edema',\n",
    "# 'effusion',\n",
    "# 'effusions',\n",
    "# 'elevated',\n",
    "# 'elevation',\n",
    "# 'endotracheal',\n",
    "# 'enlarged',\n",
    "# 'enlargement',\n",
    "# 'epigastric',\n",
    "# 'epigastrium',\n",
    "# 'esophagogastric',\n",
    "# 'fibronodular',\n",
    "# 'fibrotic',\n",
    "# 'fissure',\n",
    "# 'fissures',\n",
    "# 'fluid',\n",
    "# 'fracture',\n",
    "# 'fractured',\n",
    "# 'fractures',\n",
    "# 'gastric',\n",
    "# 'gastroesophageal',\n",
    "# 'gastrostomy',\n",
    "# 'granuloma',\n",
    "# 'granulomas',\n",
    "# 'granulomata',\n",
    "# 'granulomatous',\n",
    "# 'heart',\n",
    "# 'hemidiaphragm',\n",
    "# 'hemidiaphragms',\n",
    "# 'hernia',\n",
    "# 'hiatal',\n",
    "# 'hiatus',\n",
    "# 'hilar',\n",
    "# 'histoplasmosis',\n",
    "# 'hydropneumothorax',\n",
    "# 'hyperexpanded',\n",
    "# 'hyperexpansion',\n",
    "# 'hyperinflated',\n",
    "# 'hyperinflation',\n",
    "# 'icd',\n",
    "# 'infiltrate',\n",
    "# 'infiltrates',\n",
    "# 'infrahilar',\n",
    "# 'interstitial',\n",
    "# 'intervertebral',\n",
    "# 'ivc',\n",
    "# 'juxtahilar',\n",
    "# 'kyphosis',\n",
    "# 'kyphotic',\n",
    "# 'large',\n",
    "# 'largely',\n",
    "# 'larger',\n",
    "# 'largest',\n",
    "# 'levocurvature',\n",
    "# 'levoscoliosis',\n",
    "# 'line',\n",
    "# 'lines',\n",
    "# 'liver',\n",
    "# 'lymph',\n",
    "# 'lymphadenopathy',\n",
    "# 'lymphangitic',\n",
    "# 'lymphoma',\n",
    "# 'lymphoproliferative',\n",
    "# 'mass',\n",
    "# 'masses',\n",
    "# 'masslike',\n",
    "# 'mediastinal',\n",
    "# 'mediastinum',\n",
    "# 'mitral',\n",
    "# 'morgagni',\n",
    "# 'narrow',\n",
    "# 'narrowed',\n",
    "# 'narrowing',\n",
    "# 'nasogastric',\n",
    "# 'nodular',\n",
    "# 'nodularity',\n",
    "# 'nodule',\n",
    "# 'nodules',\n",
    "# 'noncalcified',\n",
    "# 'nonenlarged',\n",
    "# 'nonrib',\n",
    "# 'opacification',\n",
    "# 'opacities',\n",
    "# 'opacity',\n",
    "# 'orthopedic',\n",
    "# 'osseous',\n",
    "# 'osteoarthritis',\n",
    "# 'osteopenia',\n",
    "# 'osteophyte',\n",
    "# 'osteophytes',\n",
    "# 'osteoporosis',\n",
    "# 'pacemaker',\n",
    "# 'parahilar',\n",
    "# 'paramediastinal',\n",
    "# 'paratracheal',\n",
    "# 'pattern',\n",
    "# 'peribronchial',\n",
    "# 'perihilar',\n",
    "# 'peritracheal',\n",
    "# 'picc',\n",
    "# 'pleural',\n",
    "# 'pneumothoraces',\n",
    "# 'pneumothorax',\n",
    "# 'portacatheter',\n",
    "# 'postop',\n",
    "# 'postoperative',\n",
    "# 'pretracheal',\n",
    "# 'prevertebral',\n",
    "# 'prominence',\n",
    "# 'prominent',\n",
    "# 'prosthetic',\n",
    "# 'pseudofissure',\n",
    "# 'pulmonary',\n",
    "# 'radiodensity',\n",
    "# 'redistribution',\n",
    "# 'reticulonodular',\n",
    "# 'retrocardiac',\n",
    "# 'retrohilar',\n",
    "# 'revascularization',\n",
    "# 'rib',\n",
    "# 'ribs',\n",
    "# 'scoliosis',\n",
    "# 'scoliotic',\n",
    "# 'silhouette',\n",
    "# 'silhouettes',\n",
    "# 'skeletal',\n",
    "# 'soft',\n",
    "# 'sternotomy',\n",
    "# 'subdiaphragmatic',\n",
    "# 'subpleural',\n",
    "# 'suprahilar',\n",
    "# 'svc',\n",
    "# 'thickening',\n",
    "# 'tissue',\n",
    "# 'tissues',\n",
    "# 'top',\n",
    "# 'tortuosity',\n",
    "# 'tortuous',\n",
    "# 'trachea',\n",
    "# 'tube',\n",
    "# 'uncalcified',\n",
    "# 'unchanged',\n",
    "# 'unfolded',\n",
    "# 'unfolding',\n",
    "# 'unremarkable',\n",
    "# 'upper',\n",
    "# 'valve',\n",
    "# 'vascular',\n",
    "# 'vascularity',\n",
    "# 'vertebrae',\n",
    "# 'vertebral',\n",
    "# 'widened',\n",
    "# 'widening'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level1 = keywords\n",
    "level0 = None # any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [\n",
    "    keywords,\n",
    "    None,\n",
    "]\n",
    "base = 10\n",
    "max_exp = len(levels) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_h_overlap_matrix(sentences):\n",
    "    def _prepare_sentence(sentence):\n",
    "        return [\n",
    "            set([\n",
    "                word\n",
    "                for word in sentence.split() if level is None or word in level\n",
    "            ])\n",
    "            for level in levels\n",
    "        ]\n",
    "    sentences = [\n",
    "        _prepare_sentence(sentence)\n",
    "        for sentence in sentences\n",
    "    ]\n",
    "    def _word_overlap(s1, s2):\n",
    "        intersection = s1.intersection(s2)\n",
    "        union = len(s1) + len(s2)\n",
    "        return 2 * len(intersection) / (union + 1e-5)\n",
    "    n_sentences = len(sentences)\n",
    "    embeddings = np.zeros((n_sentences, n_sentences))\n",
    "    for i in tqdm(range(n_sentences)):\n",
    "        si_by_level = sentences[i]\n",
    "        for j in range(i+1, n_sentences):\n",
    "            sj_by_level = sentences[j]\n",
    "\n",
    "            overlap = 0\n",
    "            for index, (si, sj) in enumerate(zip(si_by_level, sj_by_level)):\n",
    "                overlap += _word_overlap(si, sj) * base ** (max_exp - index)\n",
    "            \n",
    "            embeddings[i, j] = embeddings[j, i] = overlap\n",
    "            \n",
    "        embeddings[i, i] = 1\n",
    "            \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(SENTENCES_DF['sentence'])\n",
    "# l = l[:100]\n",
    "overlaps = compute_h_overlap_matrix(l)\n",
    "overlaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = overlaps.copy()\n",
    "distances[distances==0] = 1\n",
    "distances = 1/distances\n",
    "for i in range(len(distances)):\n",
    "    distances[i, i] = 0\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dbscan = DBSCAN(eps=0.1, metric='precomputed', min_samples=2)\n",
    "dbscan.fit(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = group_by_cluster(dbscan, SENTENCES_DF['sentence'])\n",
    "len(clusters), len(clusters[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

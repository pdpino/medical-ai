{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEGBIO_PATH = os.environ['NEGBIO_PATH']\n",
    "if NEGBIO_PATH not in sys.path:\n",
    "    sys.path.append(NEGBIO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter, namedtuple\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n ../../../chexpert/chexpert-labeler/label.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IU_DIR = os.environ['DATASET_DIR_IU_XRAY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check sentences' nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = os.path.join(IU_DIR, 'reports', 'sentences_with_extra_info.csv')\n",
    "SENTENCES_DF = pd.read_csv(fpath)\n",
    "SENTENCES_DF['clean_sentence'] = [\n",
    "    ' '.join(s.lower().replace('xxxx', ' ').split())\n",
    "    for s in SENTENCES_DF['sentence']\n",
    "]\n",
    "SENTENCES_DF.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(SENTENCES_DF['clean_sentence'])\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCES_APPEARANCES = SENTENCES_DF.set_index('clean_sentence')['appearances'].to_dict()\n",
    "len(SENTENCES_APPEARANCES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load chexpert-stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgParser()\n",
    "args = parser.parse_args([\n",
    "    '--reports_path', '',\n",
    "    '--base-dir', '/home/pdpino/chexpert/chexpert-labeler',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Loader(args.reports_path, args.extract_impression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = Extractor(args.mention_phrases_dir,\n",
    "                      args.unmention_phrases_dir,\n",
    "                      verbose=args.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier(args.pre_negation_uncertainty_path,\n",
    "                        args.negation_path,\n",
    "                        args.post_negation_uncertainty_path,\n",
    "                        verbose=args.verbose, light=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator = Aggregator(CATEGORIES,\n",
    "                        verbose=args.verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run chexpert stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# sample_sentences = sentences[:100]\n",
    "sample_sentences = sentences\n",
    "loader.load(sample_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "extractor.extract(loader.collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "classifier.classify(loader.collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDetail(namedtuple('WordDetail', ['id', 'word', 'tag', 'deps'])):\n",
    "    def __repr__(self):\n",
    "        s = f'{self.word} ({self.tag})'\n",
    "        if len(self.deps) > 0:\n",
    "            s += '\\n'\n",
    "        for dep in self.deps:\n",
    "            s += f'\\t{dep}\\n'\n",
    "        return s\n",
    "Dependency = namedtuple('Dependency', ['word', 'tag', 'dep_type', 'role'])\n",
    "class NounDetail(namedtuple('NounDetail', ['words', 'deps'])):\n",
    "    def __repr__(self):\n",
    "        s = self.words.__repr__()\n",
    "        if len(self.deps) > 0:\n",
    "            s += '\\n'\n",
    "        for dep in self.deps:\n",
    "            s += f'\\t{dep}\\n'\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_node_relations(relations, current_id):\n",
    "    for relation in relations:\n",
    "        assert len(relation.nodes) == 2, f'len={len(relation.nodes)}'\n",
    "        first, second = relation.nodes\n",
    "\n",
    "        if first.refid == current_id:\n",
    "            other = second\n",
    "        elif second.refid == current_id:\n",
    "            other = first\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        yield relation, other.refid, other.role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_noun_FP = set([\n",
    "    'xxxx',\n",
    "    'no', # When like \"no pneumothorax, or ...\" --> set as NN\n",
    "    'streaky', 'patchy', 'bibasilar',\n",
    "    'or',\n",
    "    # 'top',\n",
    "])\n",
    "_noun_FN = set([\n",
    "    'cardiomegaly',\n",
    "    'mediastinal',\n",
    "    'fracture',\n",
    "    'dislocation',\n",
    "])\n",
    "\n",
    "def _is_noun(lemma, tag):\n",
    "    if lemma in _noun_FN:\n",
    "        return True\n",
    "    if lemma in _noun_FP:\n",
    "        return False\n",
    "\n",
    "    return tag in ('NN', 'NNS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_adjective_FN = set([\n",
    "    'streaky', 'patchy', 'hyperdense', 'retrocardiac', 'bandlike', 'number',\n",
    "])\n",
    "def _is_adjective(lemma, tag, dep_type):\n",
    "    if lemma in _adjective_FN:\n",
    "        return True\n",
    "    if tag == 'JJ':\n",
    "        return True\n",
    "    if dep_type in ('amod',): # 'dobj'\n",
    "        return True\n",
    "    if dep_type == 'nsubj' and tag == 'VBD':\n",
    "        return True\n",
    "    if dep_type == 'nsubjpass' and tag == 'VBN':\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_compounds_recursively_(id_to_details, relations, current_id, seen, found):\n",
    "    if current_id in seen:\n",
    "        return\n",
    "\n",
    "    seen.add(current_id)\n",
    "\n",
    "    _word_details = id_to_details[current_id]\n",
    "    found.append(_word_details)\n",
    "\n",
    "    for relation, other_id, _ in iter_node_relations(relations, current_id):\n",
    "        other_details = id_to_details[other_id]\n",
    "        dep_type = relation.infons['dependency']\n",
    "\n",
    "        if _is_noun(other_details.word, other_details.tag) and dep_type == 'compound':\n",
    "            _find_compounds_recursively_(\n",
    "                id_to_details, relations, other_id, seen, found,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "warnings = defaultdict(list)\n",
    "\n",
    "for document in loader.collection.documents:\n",
    "    for passage in document.passages:\n",
    "        for sentence in passage.sentences:\n",
    "            # 1. Collect all word-details and select nouns\n",
    "            id_to_details = {}\n",
    "            noun_ids = []\n",
    "\n",
    "            for annotation in sentence.annotations:\n",
    "                lemma = annotation.infons['lemma']\n",
    "                tag = annotation.infons['tag']\n",
    "                id_to_details[annotation.id] = WordDetail(\n",
    "                    id=annotation.id,\n",
    "                    word=lemma,\n",
    "                    tag=tag,\n",
    "                    deps=[],\n",
    "                )\n",
    "\n",
    "                # if True:\n",
    "                if _is_noun(lemma, tag):\n",
    "                    noun_ids.append(annotation.id)\n",
    "\n",
    "            # 2. Group compound nouns together\n",
    "            core_nouns = []\n",
    "            consumed_ids = set()\n",
    "            for noun_id in noun_ids:\n",
    "                if noun_id in consumed_ids:\n",
    "                    continue\n",
    "\n",
    "                words = []\n",
    "                _find_compounds_recursively_(\n",
    "                    id_to_details, sentence.relations, noun_id, consumed_ids, words,\n",
    "                )\n",
    "                core_nouns.append(NounDetail(\n",
    "                    words=words,\n",
    "                    deps=[],\n",
    "                ))\n",
    "\n",
    "            # 3. Get amod\n",
    "            full_nouns = []\n",
    "            for noun_details in core_nouns:\n",
    "                dependencies = []\n",
    "\n",
    "                for word_details in noun_details.words:\n",
    "                    word_id = word_details.id\n",
    "                    for relation, other_id, other_role in iter_node_relations(\n",
    "                        sentence.relations, word_id):\n",
    "                        other_details = id_to_details[other_id]\n",
    "                        dep_type = relation.infons['dependency']\n",
    "\n",
    "                        # if True:\n",
    "                        if _is_adjective(other_details.word, other_details.tag, dep_type):\n",
    "                            dependencies.append(Dependency(\n",
    "                                word=other_details.word,\n",
    "                                tag=other_details.tag,\n",
    "                                dep_type=dep_type,\n",
    "                                role=other_role,\n",
    "                            ))\n",
    "                    \n",
    "                full_nouns.append(noun_details._replace(\n",
    "                    deps=dependencies,\n",
    "                ))      \n",
    "                    \n",
    "            if len(full_nouns) == 0:\n",
    "                warnings['no-nouns'].append(sentence)\n",
    "            else:\n",
    "                results.append((sentence.text, full_nouns))\n",
    "len(warnings['no-nouns']), [s.text for s in warnings['no-nouns']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODOs:\n",
    "# Cross-reference nouns vs RG-procedure: intersection? which are left out?\n",
    "# Statistic: from all the sentences, what % use nouns from the procedure?\n",
    "# Statistic: from all the reports, what % use nouns from the procedure?\n",
    "\n",
    "\n",
    "### Some special cases:\n",
    "\n",
    "## Noun groups that should be adjectives\n",
    "# limit (+ normal, )\n",
    "# change\n",
    "# midline (when like \"trachea is midline\")\n",
    "# lung base (when like \"X ... in the lung base\") (location)\n",
    "# spine (when like \"X in the spine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(txt, details) for txt, details in results if re.search(r'juxtahilar', txt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s for s in results if 'juxtahilar' in s.text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and fix nouns\n",
    "\n",
    "* Remove dependency and details, keep only words\n",
    "* Manually fix noun issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceMeta(namedtuple('SentenceMeta', ['text', 'nouns'])):\n",
    "    def __repr__(self):\n",
    "        s = f'{self.text}'\n",
    "        if self.nouns:\n",
    "            n = ', '.join(str(x) for x in self.nouns)\n",
    "            s += f'\\n\\t{n}'\n",
    "        return s\n",
    "class NounMeta(namedtuple('NounMeta', ['words', 'adjectives'])):\n",
    "    @property\n",
    "    def noun(self):\n",
    "        return ' '.join(sorted(self.words))\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = ' '.join(self.words)\n",
    "        s = f'\"{s}\"'\n",
    "        if self.adjectives:\n",
    "            a = '|'.join(self.adjectives)\n",
    "            s += f' ({a})'\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_to_hash(words):\n",
    "    if isinstance(words, str):\n",
    "        words = words.split()\n",
    "    words = sorted(words)\n",
    "    return ','.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append adjectives to nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_FIX_ADJECTIVES = set([\n",
    "    'streaky', 'patchy', 'hyperdense', 'retrocardiac', 'bandlike', 'number',\n",
    "    'paratracheal', 'juxtahilar', 'bibasilar', 'basilar', 'perihilar', 'suprahilar',\n",
    "    'lobe', 'midlung', 'base',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_FIX_AMOD_UP_RAW = {\n",
    "    'airspace disease': 'focal',\n",
    "    'airspace opacity': 'focal',\n",
    "    'aortic': 'calcification',\n",
    "    'blunting': 'costophrenic',\n",
    "    'calcification': 'aortic',\n",
    "    'calcification': 'vascular',\n",
    "    'consolidation': 'focal',\n",
    "    'contour': 'mediastinal',\n",
    "    'edema': 'pulmonary',\n",
    "    'effusion': 'pleural',\n",
    "    'hernia': 'hiatal',\n",
    "    'joint': 'acromioclavicular',\n",
    "    'marking': 'interstitial',\n",
    "    'mediastinal': 'contour',\n",
    "    'silhouette': ('cardiomediastinal', 'cardiac'),\n",
    "    'space': 'pleural',\n",
    "    'spine': 'thoracic',\n",
    "    'structure': ('bony', 'osseous'),\n",
    "    'sulcus': 'costophrenic',\n",
    "    'tubing': 'shunt',\n",
    "    'vasculature': 'pulmonary',\n",
    "    'vascularity': 'pulmonary',\n",
    "    'view': 'lateral',\n",
    "}\n",
    "_FIX_AMOD_UP = {}\n",
    "for noun, amod in _FIX_AMOD_UP_RAW.items():\n",
    "    noun_hash = noun_to_hash(noun)\n",
    "    if isinstance(amod, str):\n",
    "        amod = (amod,)\n",
    "    amod = set(amod)\n",
    "    _FIX_AMOD_UP[noun_hash] = amod\n",
    "del _FIX_AMOD_UP_RAW\n",
    "len(_FIX_AMOD_UP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove adjectives from noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_FIX_AMOD_DOWN_RAW = {\n",
    "    'basilar opacity': 'basilar',\n",
    "    'catheter subclavian': 'subclavian',\n",
    "    'density number': 'number',\n",
    "    'granuloma midlung': 'midlung',\n",
    "    'midline sternotomy': 'midline',\n",
    "    'number opacity': 'number',\n",
    "    'opacity perihilar': 'perihilar',\n",
    "    'round opacity': 'round',\n",
    "\n",
    "    'atelectasis base': 'base',\n",
    "    'atelectasis basilar': 'basilar',\n",
    "    'atelectasis basilar subsegmental': ('basilar', 'subsegmental'),\n",
    "    'atelectasis bronchovascular crowding': ('bronchovascular', 'crowding'),\n",
    "    'atelectasis subsegmental': 'subsegmental',\n",
    "    'atelectasis lung': 'lung',\n",
    "    'atelectasis lobe': 'lobe',\n",
    "    'atelectasis base': 'base',\n",
    "    'atelectasis passive': 'passive',\n",
    "    'atelectasis base subsegmental': ('base', 'subsegmental'),\n",
    "    'atelectasis base lung': ('base', 'lung'),\n",
    "    'atelectasis base opacity': 'base',\n",
    "    'atelectasis perihilar': 'perihilar',\n",
    "    'atelectasis discoid': 'discoid',\n",
    "    'atelectasis basis lung': ('basis', 'lung'),\n",
    "    'atelectasis fissure subsegmental': ('subsegmental', 'fissure'),\n",
    "    'atelectasis lobe middle': ('lobe', 'middle'),\n",
    "    'airspace atelectasis opacity': '',\n",
    "    'atelectasis infiltrate lobe': ('infiltrate', 'lobe'),\n",
    "}\n",
    "_FIX_AMOD_DOWN = {}\n",
    "for noun, amod in _FIX_AMOD_DOWN_RAW.items():\n",
    "    noun_hash = noun_to_hash(noun)\n",
    "    if isinstance(amod, str):\n",
    "        amod = (amod,)\n",
    "    amod = set(amod)\n",
    "    _FIX_AMOD_DOWN[noun_hash] = amod\n",
    "del _FIX_AMOD_DOWN_RAW\n",
    "len(_FIX_AMOD_DOWN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finish preprocessing nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_meta = []\n",
    "for text, nouns in results:\n",
    "    nouns_meta = []\n",
    "    for compound_noun in nouns:\n",
    "        words = list(w.word for w in compound_noun.words)\n",
    "        adjectives = [d.word for d in compound_noun.deps]\n",
    "        noun_hash = noun_to_hash(words)\n",
    "        \n",
    "        # 1. Check for adjectives to carry up\n",
    "        amod_to_raise = _FIX_AMOD_UP.get(noun_hash, None)\n",
    "        if amod_to_raise:\n",
    "            adjectives = set(adjectives)\n",
    "            adjectives_to_raise = adjectives.intersection(amod_to_raise)\n",
    "            words = list(adjectives_to_raise) + words\n",
    "            \n",
    "            adjectives = list(adjectives - adjectives_to_raise)\n",
    "            \n",
    "        # 2. Check for adjectives to carry down\n",
    "        amod_to_move_down = _FIX_AMOD_DOWN.get(noun_hash, [])\n",
    "        for amod in amod_to_move_down:\n",
    "            words.remove(amod)\n",
    "            if amod not in adjectives:\n",
    "                adjectives.append(amod)\n",
    "            \n",
    "        nouns_meta.append(NounMeta(words, adjectives))\n",
    "\n",
    "    sentences_meta.append(SentenceMeta(text=text, nouns=nouns_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYNONYMS = {\n",
    "    'pneumothorace': 'pneumothorax',\n",
    "    'cardio silhouette': 'cardiac silhouette',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOUN_ADJECTIVES = defaultdict(list)\n",
    "NOUN_APPEARANCES = Counter()\n",
    "_warnings = defaultdict(list)\n",
    "\n",
    "for meta in sentences_meta:\n",
    "    if meta.text not in SENTENCES_APPEARANCES:\n",
    "        _warnings['no-match'].append(meta)\n",
    "        continue\n",
    "    sentence_appearances = SENTENCES_APPEARANCES[meta.text]\n",
    "    for noun in meta.nouns:\n",
    "        noun_str = SYNONYMS.get(noun.noun, noun.noun)\n",
    "        NOUN_APPEARANCES[noun_str] += sentence_appearances\n",
    "        NOUN_ADJECTIVES[noun_str].extend(noun.adjectives)\n",
    "        \n",
    "len(NOUN_ADJECTIVES), len(NOUN_APPEARANCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(_warnings['no-match']), _warnings['no-match']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s for s in SENTENCES_APPEARANCES if s.startswith('question edema')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(NOUN_APPEARANCES.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problematic nouns:\n",
    "# widening, redemonstration, \"a\", \"or\", collecting, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorize nouns\n",
    "\n",
    "(disease/organ/etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVED_CATEGORY = dict()\n",
    "with open(os.path.join(IU_DIR, 'reports', 'nouns', 'categories.json'), 'r') as f:\n",
    "    SAVED_CATEGORY = json.load(f)\n",
    "WRONG_NOUN = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_for_categories():\n",
    "    nouns_and_appearances = sorted(NOUN_APPEARANCES.items(), key=lambda x: x[1], reverse=True)\n",
    "    length = len(nouns_and_appearances)\n",
    "\n",
    "    for index, (noun, appearances) in enumerate(nouns_and_appearances):\n",
    "        if noun in SAVED_CATEGORY or noun in WRONG_NOUN:\n",
    "            continue\n",
    "\n",
    "        while True:\n",
    "            option = input(f'({index}/{length}) {noun} ({appearances})')\n",
    "\n",
    "            if option in ('quit', 'q'):\n",
    "                return\n",
    "            elif option in ('w',):\n",
    "                WRONG_NOUN.append(noun)\n",
    "                break\n",
    "            elif option in ('d','disease'):\n",
    "                category = 'disease'\n",
    "            elif option in ('gd','general-disease'):\n",
    "                category = 'general-disease'\n",
    "            elif option in ('dev','device'):\n",
    "                category = 'device'\n",
    "            elif option in ('o','organ'):\n",
    "                category = 'organ'\n",
    "            elif option in ('go','general-organ'):\n",
    "                category = 'general-organ'\n",
    "            elif option in ('n','normal'):\n",
    "                category = 'normal'\n",
    "            elif option in ('p', 'proj', 'projection'):\n",
    "                category = 'projection'\n",
    "            elif option in ('s', 'sus'):\n",
    "                category = 'sus'\n",
    "            elif option in ('surg', 'surgery',):\n",
    "                category = 'surgery'\n",
    "            else:\n",
    "                print('Option not recognized')\n",
    "                continue\n",
    "                \n",
    "            SAVED_CATEGORY[noun] = category\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_for_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(IU_DIR, 'reports', 'nouns', 'categories.json'), 'w') as f:\n",
    "    json.dump(SAVED_CATEGORY, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[k for k in NOUN_APPEARANCES if 'opacity' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Save to JSON\n",
    "[{\n",
    "    'text': m.text,\n",
    "    'nouns': [\n",
    "        {'noun': n.noun, 'adjectives': n.adjectives} for n in m.nouns\n",
    "    ]\n",
    "} for m in sentences_meta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3chexpert-label",
   "language": "python",
   "name": "py3chexpert-label"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug MIRQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n ../eval_report_generation_mirqi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MIRQI output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/software/MIRQI/testing2.csv')\n",
    "df.fillna('', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_gt = _attributes_to_list(df['attributes-gt'])\n",
    "attributes_gen = _attributes_to_list(df['attributes-gen'])\n",
    "len(attributes_gt), len(attributes_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['attributes-gt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n ../eval_report_generation_mirqi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = MIRQI_v2(attributes_gt, attributes_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['MIRQI-v2-attr-p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "attributes_gt[idx], attributes_gen[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIRQI Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n ../eval_report_generation_mirqi.py\n",
    "# %run -n ~/software/MIRQI/evaluate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### MIRQI original def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def MIRQI(gt_list, cand_list, pos_weight=0.8, attribute_weight=0.3, verbose=False):\n",
    "    \"\"\"Compute the score of matching keyword and associated attributes between gt list and candidate list.\n",
    "       It returns two scores:   MIRQI-r (recall: hits in gt)\n",
    "                                MIRQI-p (precision: correct ratio of all candidates)\n",
    "    \"\"\"\n",
    "\n",
    "    MIRQI_r = []\n",
    "    MIRQI_p = []\n",
    "    MIRQI_f = []\n",
    "\n",
    "    for gt_report_entry, cand_report_entry in zip(gt_list, cand_list):\n",
    "        attribute_cand_all = []\n",
    "\n",
    "        pos_count_in_gt = 0\n",
    "        pos_count_in_cand = 0\n",
    "        tp = 0.0\n",
    "        fp = 0.0\n",
    "        tn = 0.0\n",
    "        fn = 0.0\n",
    "\n",
    "        for gt_entity in gt_report_entry:\n",
    "            if gt_entity[2] == 'NEGATIVE':\n",
    "                continue\n",
    "            pos_count_in_gt = pos_count_in_gt + 1\n",
    "        neg_count_in_gt = len(gt_report_entry) - pos_count_in_gt\n",
    "\n",
    "        for entity_index, cand_entity in enumerate(cand_report_entry):\n",
    "            if cand_entity[2] == 'NEGATIVE':\n",
    "                for entity_index, gt_entity in enumerate(gt_report_entry):\n",
    "                    if  gt_entity[1] == cand_entity[1]:\n",
    "                        if gt_entity[2] == 'NEGATIVE':\n",
    "                            tn = tn + 1     # true negative hits\n",
    "                            break\n",
    "                        else:\n",
    "                            fn = fn + 1     # false negative hits\n",
    "                            break\n",
    "            else:\n",
    "                pos_count_in_cand = pos_count_in_cand + 1\n",
    "                for entity_index, gt_entity in enumerate(gt_report_entry):\n",
    "                    if gt_entity[1] == cand_entity[1]:\n",
    "                        if gt_entity[2] == 'NEGATIVE':\n",
    "                            fp = fp + 1     # false positive hits\n",
    "                            break\n",
    "                        else:\n",
    "                            tp = tp + 1.0 - attribute_weight    # true positive hits (key words part)\n",
    "                            # count attribute hits\n",
    "                            if gt_entity[3] == '':\n",
    "                                break\n",
    "                            attributes_all_gt = gt_entity[3].split('/')\n",
    "                            attribute_hit_count = 0\n",
    "                            for attribute in attributes_all_gt:\n",
    "                                if attribute in cand_entity[3]:\n",
    "                                    attribute_hit_count = attribute_hit_count + 1\n",
    "                            # true positive hits (attributes part)\n",
    "                            temp = attribute_hit_count/len(attributes_all_gt)*attribute_weight\n",
    "                            tp = tp + temp\n",
    "                            break\n",
    "        neg_count_in_cand = len(cand_report_entry) - pos_count_in_cand\n",
    "        #\n",
    "        # calculate score for positive/uncertain mentions\n",
    "        if pos_count_in_gt == 0 and pos_count_in_cand == 0:\n",
    "            score_r = 1.0\n",
    "            score_p = 1.0\n",
    "        elif pos_count_in_gt == 0 and pos_count_in_cand != 0:\n",
    "            score_r = 0.0\n",
    "            score_p = 0.0\n",
    "        elif pos_count_in_gt != 0 and pos_count_in_cand == 0:\n",
    "            score_r = 0.0\n",
    "            score_p = 0.0\n",
    "        else:\n",
    "            score_r = tp / (tp + fn + 0.000001)\n",
    "            score_p = tp / (tp + fp + 0.000001)\n",
    "\n",
    "        # calculate score for negative mentions\n",
    "        # if neg_count_in_cand != 0 and neg_count_in_gt != 0:\n",
    "        if tn != 0:\n",
    "            score_r = score_r * pos_weight + tn / (tn + fp + 0.000001) * (1.0 - pos_weight)\n",
    "            score_p = score_p * pos_weight + tn / (tn + fn + 0.000001) * (1.0 - pos_weight)\n",
    "\n",
    "        MIRQI_r.append(score_r)\n",
    "        MIRQI_p.append(score_p)\n",
    "        rec_prec = (score_r + score_p)\n",
    "        MIRQI_f.append(2*(score_r * score_p) / rec_prec if rec_prec != 0.0 else 0.0)\n",
    "\n",
    "    scores = {\n",
    "        'MIRQI-r': MIRQI_r,\n",
    "        'MIRQI-p': MIRQI_p,\n",
    "        'MIRQI-f': MIRQI_f,\n",
    "    }\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeated nodes with different attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_gt = \"right effusion with mild atelectasis. left effusion is also present.\"\n",
    "entities_gt = [\n",
    "    ['effusion', 'Pleural Effusion', 'POSITIVE', 'right/present'],\n",
    "    ['effusion', 'Pleural Effusion', 'POSITIVE', 'left/pleural'],\n",
    "]\n",
    "report_gen = report_gt\n",
    "entities_gen = [\n",
    "    ['effusion', 'Pleural Effusion', 'POSITIVE', 'left/pleural'],\n",
    "    ['effusion', 'Pleural Effusion', 'POSITIVE', 'right/present'],\n",
    "]\n",
    "{\n",
    "    **MIRQI([entities_gt], [entities_gen]),\n",
    "    **MIRQI_v2([entities_gt], [entities_gen]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GT nodes matched twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_gt = \"right pleural effusion.\"\n",
    "entities_gt = [\n",
    "    ['effusion', 'Pleural Effusion', 'POSITIVE', 'right'],\n",
    "]\n",
    "report_gen = \"right pleural effusion. left pleural effusion\"\n",
    "entities_gen = [\n",
    "    ['effusion', 'Pleural Effusion', 'POSITIVE', 'right'],\n",
    "    ['effusion', 'Pleural Effusion', 'POSITIVE', 'left'],\n",
    "]\n",
    "{\n",
    "    **MIRQI([entities_gt], [entities_gen]),\n",
    "    **MIRQI_v2([entities_gt], [entities_gen]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "import importlib\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.facecolor'] = 'white'\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../utils/__init__.py\n",
    "config_logging(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../datasets/common/constants.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medai.datasets import iu_xray, mimic_cxr\n",
    "IU_DIR = iu_xray.DATASET_DIR\n",
    "MIMIC_DIR = mimic_cxr.DATASET_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load sentences and reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = IU_DIR\n",
    "# dataset_dir = MIMIC_DIR\n",
    "\n",
    "fpath = os.path.join(dataset_dir, 'reports', 'sentences_with_chexpert_labels.csv')\n",
    "SENTENCES_DF = pd.read_csv(fpath)\n",
    "SENTENCES_DF.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = os.path.join(dataset_dir, 'reports', 'reports_with_chexpert_labels.csv')\n",
    "REPORTS_DF = pd.read_csv(fpath)\n",
    "REPORTS_DF.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "\n",
    "Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_to_str(name, value, fmt='%.3f'):\n",
    "    s = '%s=' + fmt\n",
    "    return s % (name, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cider IDF adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../metrics/report_generation/nlp/cider_idf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = list(REPORTS_DF['Reports'])\n",
    "DOC_FREQ = compute_doc_freq(reports)\n",
    "LOG_REF_LEN = np.log(len(reports))\n",
    "len(DOC_FREQ), len(reports), LOG_REF_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cider(gt, gen):\n",
    "    scorer = CiderScorerIDFModified()\n",
    "    scorer.document_frequency = DOC_FREQ\n",
    "    scorer.ref_len = LOG_REF_LEN\n",
    "\n",
    "    assert isinstance(gt, str)\n",
    "    assert isinstance(gen, str)\n",
    "    \n",
    "    scorer += (gt, [gen])\n",
    "    \n",
    "    cider, _ = scorer.compute_score()\n",
    "    return cider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.bleu.bleu_scorer import BleuScorer\n",
    "from pycocoevalcap.rouge.rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge(gt, gen):\n",
    "    assert isinstance(gt, str)\n",
    "    assert isinstance(gen, str)\n",
    "    scorer = Rouge()\n",
    "    return scorer.calc_score([gen], [gt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(gt, gen):\n",
    "    assert isinstance(gt, str)\n",
    "    assert isinstance(gen, str)\n",
    "    scorer = BleuScorer(4)\n",
    "    scorer += (gen, [gt])\n",
    "    bleus, _ = scorer.compute_score()\n",
    "    return bleus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nlp(gt, gen, show=True, **show_kwargs):\n",
    "    bleus = calculate_bleu(gt, gen)\n",
    "    cider = calculate_cider(gt, gen)\n",
    "    rouge = calculate_rouge(gt, gen)\n",
    "\n",
    "    if show:\n",
    "        names = ('B', 'B4', 'R', 'C')\n",
    "        values = (np.mean(bleus), bleus[-1], rouge, cider)\n",
    "        print('   '.join(metric_to_str(n, v, **show_kwargs) for n, v in zip(names, values)))\n",
    "    return bleus, rouge, cider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## CheXpert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run ../../metrics/report_generation/chexpert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as prf1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def calculate_chexpert(gt, gens, verbose=False, diseases=None):\n",
    "    assert isinstance(gt, str)\n",
    "    assert isinstance(gens, list)\n",
    "    \n",
    "    raw_labels = apply_labeler_to_column([gt] + gens)\n",
    "    # shape: 1 + n_gens, 14\n",
    "\n",
    "    labels = raw_labels.copy()\n",
    "    labels[labels == -2] = 0\n",
    "    labels[labels == -1] = 1\n",
    "    \n",
    "    if verbose:\n",
    "        print('Chexpert labels: \\n', labels)\n",
    "    \n",
    "    if diseases is not None:\n",
    "        diseases_idx = [CHEXPERT_DISEASES.index(d) for d in diseases]\n",
    "        labels = labels[:, diseases_idx]\n",
    "        # print('\\tFiltered: ', labels)\n",
    "    \n",
    "    gt = np.expand_dims(labels[0, :], 0)\n",
    "    \n",
    "    p, r, f = [], [], []\n",
    "    for i in range(len(gens)):\n",
    "        precision, recall, f1, _ = prf1s(\n",
    "            gt,\n",
    "            np.expand_dims(labels[i+1, :], 0),\n",
    "            zero_division=0,\n",
    "        )\n",
    "        p.append(precision)\n",
    "        r.append(recall)\n",
    "        f.append(f1)\n",
    "    p = np.array(p)\n",
    "    r = np.array(r)\n",
    "    f = np.array(f)\n",
    "    return p, r, f, raw_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_only_mentions_index(raw_labels):\n",
    "    # raw_labels shape: n_samples, n_diseases\n",
    "    index = (raw_labels != -2).any(axis=0)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## MIRQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run ../../metrics/report_generation/mirqi.py\n",
    "%run -n ../../eval_report_generation_mirqi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def calculate_mirqi(gt, gens, verbose=False):\n",
    "    attributes = _call_mirqi_for_reports([gt] + gens)\n",
    "    # shape: 1 + n_gens, 1\n",
    "    \n",
    "    attributes = _attributes_to_list(attributes.squeeze())\n",
    "    if verbose:\n",
    "        print('MIRQI attributes: \\n', attributes)\n",
    "    \n",
    "    gt = [attributes[0]]\n",
    "    scores = [MIRQI(gt, [attributes[i+1]]) for i in range(len(gens))]\n",
    "    \n",
    "    return scores, attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Toy samples 1\n",
    "\n",
    "NLP errors in negations, uncertains and synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Negations, synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gt = 'heart size is mildly enlarged . small right pneumothorax is seen .'\n",
    "gens = [\n",
    "    'heart size is normal . no pneumothorax is seen .',\n",
    "    'the cardiac silhouette is enlarged . no pneumothorax .',\n",
    "    'mild cardiomegaly . pneumothorax on right lung .',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for gen in gens:\n",
    "    bleus, rouge, cider = calculate_nlp(gt, gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores, attrs = calculate_mirqi(gt, [gt] + gens)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p, r, f, raw_labels = calculate_chexpert(gt, gens)\n",
    "p, r, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Out of reach info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gt = 'comparison to previous exam . heart size is enlarged . dr xxxx was contacted .'\n",
    "gens = [\n",
    "    'comparison to previous exam. heart size is enlarged . dr was contacted .',\n",
    "    'heart size is enlarged .',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for gen in gens:\n",
    "    bleus, rouge, cider = calculate_nlp(gt, gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p, r, f, raw_labels = calculate_chexpert(gt, gens)\n",
    "p, r, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores, attrs = calculate_mirqi(gt, [gt] + gens)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Toy samples 2\n",
    "\n",
    "Error gradation examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## First example: Pleural Effusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gt = 'there is a large right sided effusion .'\n",
    "gens = [\n",
    "    'there is a minimal right sided effusion .',\n",
    "    'there is a large left sided effusion .',\n",
    "    'there is a large right sided mass .',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for gen in gens:\n",
    "    bleus, rouge, cider = calculate_nlp(gt, gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p, r, f, raw_labels = calculate_chexpert(gt, gens)\n",
    "p, r, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores, attrs = calculate_mirqi(gt, [gt] + gens)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Second example: Atelectasis vs pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gt2 = 'opacities in the lung bases may represent atelectasis .'\n",
    "gens2 = [\n",
    "    'opacities in the left lung may represent atelectasis .',\n",
    "    'opacities in the lung bases may represent pneumonia .',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for gen2 in gens2:\n",
    "    bleus, rouge, cider = calculate_nlp(gt2, gen2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "prec, recall, f1, raw_labels = calculate_chexpert(gt2, gens2, verbose=True)\n",
    "prec, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores2, attrs2 = calculate_mirqi(gt2, [gt2] + gens2)\n",
    "scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "attrs2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# MIRQI extraction example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run ../../metrics/report_generation/mirqi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gt = 'heart size is normal . right effusion is present . there is a moderate hiatal hernia .'\n",
    "gens = [\n",
    "    'right effusion with mild atelectasis . left effusion is also present . cardiomegaly is present .',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores, attributes = calculate_mirqi(gt, gens)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Patch MIRQI\n",
    "\n",
    "By adding negative sentences (where there are unmentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run ../../metrics/report_generation/mirqi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gt2 = gt + ' no atelectasis .'\n",
    "gens2 = [\n",
    "    gens[0] + ' no hernia .',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores, attributes = calculate_mirqi(gt2, gens2)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run ../../metrics/report_generation/mirqi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[gt_attr, gen_attr] = attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "s = MIRQI([gt_attr], [gen_attr])\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = 'the fox then jumped over the puppy'\n",
    "gen = 'the fox jumped over the dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = calculate_nlp(gt, gen)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.facecolor'] = 'white'\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_logging(logging.WARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.bleu.bleu_scorer import BleuScorer\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from sklearn.metrics import precision_recall_fscore_support as prf1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge(gt, gen):\n",
    "    assert isinstance(gt, str)\n",
    "    assert isinstance(gen, str)\n",
    "    scorer = Rouge()\n",
    "    return scorer.calc_score([gen], [gt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(gt, gen):\n",
    "    assert isinstance(gt, str)\n",
    "    assert isinstance(gen, str)\n",
    "    scorer = BleuScorer(4)\n",
    "    scorer += (gen, [gt])\n",
    "    bleus, _ = scorer.compute_score()\n",
    "    return bleus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples vs metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run ../metrics/report_generation/chexpert.py\n",
    "%run ../metrics/report_generation/mirqi.py\n",
    "%run -n ../eval_report_generation_mirqi.py\n",
    "# %run ../metrics/report_generation/nlp/rouge.py\n",
    "# %run ../metrics/report_generation/nlp/bleu.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def calculate_chexpert(gt, gen, verbose=False, diseases=None):\n",
    "    raw_labels = apply_labeler_to_column([gen, gt])\n",
    "    # shape: 2, 14\n",
    "\n",
    "    labels = raw_labels.copy()\n",
    "    labels[labels == -2] = 0\n",
    "    labels[labels == -1] = 1\n",
    "    \n",
    "    if verbose:\n",
    "        print('Chexpert labels: \\n', labels)\n",
    "    \n",
    "    if diseases is not None:\n",
    "        diseases_idx = [CHEXPERT_DISEASES.index(d) for d in diseases]\n",
    "        labels = labels[:, diseases_idx]\n",
    "        # print('\\tFiltered: ', labels)\n",
    "    \n",
    "    precision, recall, f1, _ = prf1s(\n",
    "        np.expand_dims(labels[1, :], 0),\n",
    "        np.expand_dims(labels[0, :], 0),\n",
    "        zero_division=0,\n",
    "    )\n",
    "    return precision, recall, f1, raw_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def calculate_mirqi(gt, gen, verbose=False):\n",
    "    attributes = _call_mirqi_for_reports([gen, gt])\n",
    "    # shape: 2, 1\n",
    "    \n",
    "    attributes = _attributes_to_list(attributes.squeeze())\n",
    "    if verbose:\n",
    "        print('MIRQI attributes: \\n', attributes)\n",
    "    \n",
    "    scores = MIRQI([attributes[1]], [attributes[0]])\n",
    "    \n",
    "    return scores, attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(gt, gen, diseases=None, only_present=True, verbose=False):\n",
    "    results = {}\n",
    "    \n",
    "    bleu = calculate_bleu(gt, gen)\n",
    "    rouge = calculate_rouge(gt, gen)\n",
    "    \n",
    "    results.update({\n",
    "        'bleu': np.mean(bleu),\n",
    "        'rouge': rouge,\n",
    "    })\n",
    "    \n",
    "    precision, recall, f1, raw_labels = calculate_chexpert(gt, gen, verbose=verbose,\n",
    "                                                           diseases=diseases)\n",
    "    \n",
    "    if only_present:\n",
    "        present_labels = raw_labels.sum(axis=0) # shape: 14\n",
    "        f1 = f1[present_labels != -4]\n",
    "    f1 = f1.mean()\n",
    "    precision = precision[present_labels != -4].mean()\n",
    "    recall = recall[present_labels != -4].mean()\n",
    "    \n",
    "    results.update({\n",
    "        'f1': f1,\n",
    "        'prec': precision,\n",
    "        'recall': recall,\n",
    "    })\n",
    "    \n",
    "    mirqi_values, _ = calculate_mirqi(gt, gen, verbose=verbose)\n",
    "    for key in ('MIRQI-f', 'MIRQI-p', 'MIRQI-r'):\n",
    "        results[key] = mirqi_values[key][0]\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diseases = ['Cardiomegaly', 'Pneumothorax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = 'heart size is mildly enlarged . small right pneumothorax is seen .'\n",
    "gens = [\n",
    "    'heart size is normal . no pneumothorax is seen .',\n",
    "    'mild cardiomegaly . pneumothorax on right lung .',\n",
    "    'mild cardiomegaly . pneumothorax on right lung , bibasilar opacities and edema .',\n",
    "    'cardiac silhouette is moderately enlarged . left pneumothorax observed .',\n",
    "    'the cardiac silhouette is enlarged . no pneumothorax .',\n",
    "    'the cardiac silhouette is enlarged . no pneumothorax is seen.',\n",
    "#     'the cardiac silhouette is enlarged . pneumothorax observed .',\n",
    "#     'cardiac silhouette is mildly enlarged . small pneumothorax on right side .',\n",
    "    # 'cardiomediastinal silhouettes are within normal limits . lungs are clear without focal consolidation , pneumothorax , or pleural effusion . stable calcified granulomas . bony thorax is unremarkable .',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records([\n",
    "    calculate_metrics(gt, gen)\n",
    "    for gen in gens\n",
    "], index=gens)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, attributes = calculate_mirqi(gt, gens[1], verbose=True)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIRQI([attributes[1]], [attributes[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIRQI_v2([attributes[1]], [attributes[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = 'heart size is mildly enlarged . small right pneumothorax is seen . bibasilar opacities .'\n",
    "gens = [\n",
    "    'heart size is normal . no pneumothorax is seen . no opacities .',\n",
    "    'mild cardiomegaly . pneumothorax on right side .',\n",
    "    'cardiac silhouette is moderately enlarged . left pneumothorax observed . patchy opacities .',\n",
    "#     'the cardiac silhouette is enlarged . pneumothorax observed .',\n",
    "#     'cardiac silhouette is mildly enlarged . small pneumothorax on right side .',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records([\n",
    "    calculate_metrics(gt, gen)\n",
    "    for gen in gens\n",
    "], index=gens)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = 'heart size is mildly enlarged . bibasilar interstitial opacities . no pneumothorax is seen .'\n",
    "gens = [\n",
    "    # 'heart size is normal . no opacities . no pneumothorax is seen .',\n",
    "    'the cardiac silhouette is enlarged . pneumothorax is not observed . multiple opacities seen.',\n",
    "#     'heart size is moderately enlarged . no pneumothorax is seen .',\n",
    "#     'pneumothorax on right side . cardiac silhouette is mildly enlarged .',\n",
    "#     'cardiac silhouette is mildly enlarged . pneumothorax on right side .',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(gt, gens[0], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(gt, gen1) #, diseases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(gt, gen2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(gt, gen3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(gt, gen4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(gt, gen5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample generated reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../metrics/__init__.py\n",
    "%run ../utils/files.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_name = '0612_035549'\n",
    "# run_name = '0602_034645'\n",
    "# run_name = '0601_031606'\n",
    "run_name = '0612_233628'\n",
    "# run_name = '0617_143104'\n",
    "run_id = RunId(run_name, False, 'rg')\n",
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_rg_outputs(run_id, free=True, labeled=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check commonly generated reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_LUNG_RELATED_DISEASES = (\n",
    "    'Lung Lesion',\n",
    "    'Lung Opacity',\n",
    "    'Edema',\n",
    "    'Consolidation',\n",
    "    'Pneumonia',\n",
    "    'Atelectasis',\n",
    "    'Pneumothorax',\n",
    "    'Pleural Effusion',\n",
    "    'Pleural Other',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils/nlp.py\n",
    "%run ../datasets/common/constants.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTUAL_DISEASES = CHEXPERT_DISEASES[1:]\n",
    "actual_diseases_gen = [f'{d}-gen' for d in ACTUAL_DISEASES]\n",
    "actual_diseases_gt = [f'{d}-gt' for d in ACTUAL_DISEASES]\n",
    "lung_diseases_gen = [f'{d}-gen' for d in _LUNG_RELATED_DISEASES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df\n",
    "d = d.loc[d['dataset_type'] == 'train']\n",
    "d = d.loc[(d[actual_diseases_gen] == 0).all(axis=1)]\n",
    "# d = d.loc[(d[actual_diseases_gen] == 1).any(axis=1)]\n",
    "# d = d.loc[(d[lung_diseases_gen] == 0).all(axis=1)]\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = list(d['generated'])\n",
    "len(reports), len(set(reports))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_appearances = sorted(\n",
    "    Counter(reports).items(),\n",
    "    # key=lambda x: (1428 - len(x[0])) * 300000 + x[1],\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True,\n",
    ")\n",
    "reports_appearances[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_appearances[5:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [r for r in d['ground_truth'] if r.startswith('in comparison with the study of xxxx')]\n",
    "len(s), len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = reports_appearances[1][0]\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.loc[d['ground_truth'] == 'no pneumonia , vascular congestion , or pleural effusion .'].head(2)# [actual_diseases_gen].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use NLP metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.cider.cider_scorer import CiderScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df.copy()\n",
    "d = d.loc[d['dataset_type'] == 'test']\n",
    "d.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer_bleu = BleuScorer(4)\n",
    "scorer_rouge = Rouge()\n",
    "scorer_cider = CiderScorer(4)\n",
    "\n",
    "all_rouge_scores = []\n",
    "\n",
    "for index, row in d.iterrows():\n",
    "    gen = str(row['generated'])\n",
    "    gt = str(row['ground_truth'])\n",
    "    \n",
    "    scorer_bleu += (gen, [gt])\n",
    "    scorer_cider += (gen, [gt])\n",
    "    all_rouge_scores.append(scorer_rouge.calc_score([gen], [gt]))\n",
    "    \n",
    "bleus, all_bleu_scores = scorer_bleu.compute_score()\n",
    "cider, all_cider_scores = scorer_cider.compute_score()\n",
    "len(all_bleu_scores), len(all_cider_scores), len(all_rouge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bleu_scores = np.array(all_bleu_scores)\n",
    "all_bleu_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bleu_scores = all_bleu_scores.mean(axis=0)\n",
    "len(all_bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['bleu'] = all_bleu_scores\n",
    "d['rouge'] = all_rouge_scores\n",
    "d['cider'] = all_cider_scores\n",
    "d.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['ground_truth', 'generated', 'bleu', 'rouge', 'cider']\n",
    "d2 = d.sort_values(['bleu', 'rouge', 'cider'])[cols]\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(d2['ground_truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.loc[d2['ground_truth'].str.contains('no acute intrathoracic process')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(d2.loc[240745])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(d2['ground_truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(d2.loc[242324])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use chexpert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../metrics/report_generation/chexpert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rr = ['no pneumonia , vascular congestion , or pleural effusion']\n",
    "# rr = ['no acute cardiopulmonary process']\n",
    "rr = [\"\"\"in comparison with the study of xxxx ,\n",
    "        the monitoring and support devices are unchanged .\n",
    "        continued enlargement of the cardiac silhouette with\n",
    "        pulmonary vascular congestion and bilateral pleural effusions\n",
    "        with compressive atelectasis at the bases\"\"\"]\n",
    "labels = apply_labeler_to_column(rr)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(CHEXPERT_DISEASES, labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample real reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw reports\n",
    "\n",
    "Findings + impression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../datasets/preprocess/iu_xray.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reports = load_raw_reports()\n",
    "with open(os.path.join(REPORTS_DIR, 'reports.clean.v4.json')) as f:\n",
    "    reports = json.load(f)\n",
    "len(reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports['1.xml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_what_is_needed(report):\n",
    "    has_text = report['findings'] is not None and report['impression'] is not None\n",
    "    if not has_text:\n",
    "        return False\n",
    "    if 'xxxx' in report['findings'].lower() or 'xxxx' in report['impression'].lower():\n",
    "        return False\n",
    "    images = report['images']\n",
    "    frontal_image = any('frontal' in i['side'] and not i['broken'] for i in images)\n",
    "    lateral_image = any('frontal' not in i['side'] and not i['broken'] for i in images)\n",
    "    has_images = frontal_image and lateral_image\n",
    "    return has_text and has_images\n",
    "\n",
    "reports = {\n",
    "    k: report\n",
    "    for k, report in reports.items()\n",
    "    if has_what_is_needed(report)\n",
    "}\n",
    "len(reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studies = list(reports.keys())\n",
    "len(studies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample(study):\n",
    "    report_meta = reports[study]\n",
    "    \n",
    "    images = report_meta['images']\n",
    "    n_cols = len(images)\n",
    "    n_rows = 1\n",
    "    print(study)\n",
    "    \n",
    "    plt.figure(figsize=(n_cols * 5, n_rows*5))\n",
    "    for idx, image_meta in enumerate(images):\n",
    "        image_id = image_meta['id']\n",
    "        if image_meta['broken']:\n",
    "            print(f'WARNING: {image_id} is broken')\n",
    "        image_pos = image_meta['side']\n",
    "        title = f'{image_id} ({image_pos})'\n",
    "        \n",
    "        print(title)\n",
    "        \n",
    "        image_path = os.path.join(DATASET_DIR, 'images', f'{image_id}.png')\n",
    "        image = load_image(image_path, 'L')\n",
    "        plt.subplot(n_rows, n_cols, idx + 1)\n",
    "        plt.title(title)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "    \n",
    "    for key in ('indication', 'findings', 'impression'):\n",
    "        value = report_meta.get(key, None)\n",
    "        print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample('10.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample('922.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.loc[d['filename'].isin(list(reports))]\n",
    "print(len(d))\n",
    "d.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(d['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATES = ['3959.xml', '2532.xml', '1057.xml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = list([l[2], l[5], l[12], l[14], l[40]])\n",
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample('3095.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Clean reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run ../datasets/iu_xray.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fpath = os.path.join(DATASET_DIR, 'reports', 'reports_with_chexpert_labels.csv')\n",
    "chexpert_df = pd.read_csv(fpath)\n",
    "chexpert_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d = chexpert_df\n",
    "# d = d.loc[((d['Pneumothorax'] == 1) & (d['Cardiomegaly'] == 1))]\n",
    "d = d.loc[d['Consolidation'] == 1]\n",
    "d = d.sort_values('Reports', key=lambda x: x.str.len(), ascending=True)\n",
    "print(len(d))\n",
    "d.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list(d['Reports'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load rg-templates model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load CNN and rg-templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run ../models/checkpoint/__init__.py\n",
    "%run ../utils/files.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rg_run_id = RunId('0612_012741', debug=False, task='rg')\n",
    "cnn_name = re.match(r'.*cnn-(\\d{4}-\\d{6})', rg_run_id.name).group(1).replace('-', '_')\n",
    "run_id = RunId(cnn_name, debug=False, task='cls')\n",
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "compiled_model = load_compiled_model(run_id)\n",
    "compiled_model.model.eval()\n",
    "compiled_model.metadata['model_kwargs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../metrics/__init__.py\n",
    "%run ../metrics/report_generation/chexpert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = get_results_folder(rg_run_id)\n",
    "outputs_path = os.path.join(results_folder, f'outputs-labeled-free.csv')\n",
    "df = pd.read_csv(outputs_path)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Cardiomegaly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "others = labels_with_suffix('gen') + labels_with_suffix('gt')\n",
    "others.remove(f'{target}-gt')\n",
    "others.remove(f'{target}-gen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df\n",
    "d = d.loc[((d[f'{target}-gen'] == 1) & (d[f'{target}-gt'] == 1) & ((d[others] == 0).all(axis=1)))]\n",
    "print(len(d))\n",
    "d.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load image and Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../datasets/iu_xray.py\n",
    "%run -n ../eval_rg_template.py\n",
    "%run ../utils/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = get_default_image_transform(\n",
    "    (256, 256),\n",
    "    norm_by_sample=False,\n",
    "    mean=_DATASET_MEAN,\n",
    "    std=_DATASET_STD,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = 'CXR3993_IM-2044-1001' # Great example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_fpath = os.path.join(DATASET_DIR, 'images', f'{image_name}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(image_fpath, 'RGB')\n",
    "image = transform(image)\n",
    "image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotable_image = tensor_to_range01(image).permute(1, 2, 0).detach().cpu().numpy()\n",
    "plotable_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = image.unsqueeze(0).cuda()\n",
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, embedding = compiled_model.model(images)\n",
    "out = torch.sigmoid(out)\n",
    "out.size(), embedding.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = _get_threshold(run_id, 'pr', compiled_model.model.labels)\n",
    "thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(out >= thresh).type(torch.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../training/classification/grad_cam.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam = create_grad_cam(compiled_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions = calculate_attributions(grad_cam, images, 1, resize=False)\n",
    "attributions.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = images.size()[-2:]\n",
    "attributions = interpolate(attributions, image_size, mode='bilinear', align_corners=False)\n",
    "attributions.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = attributions.squeeze().detach().cpu().numpy()\n",
    "heatmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 1\n",
    "n_cols = 2\n",
    "plt.subplot(n_rows, n_cols, 1)\n",
    "plt.imshow(plotable_image)\n",
    "\n",
    "plt.subplot(n_rows, n_cols, 2)\n",
    "plt.imshow(heatmap)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axis = visualization.visualize_image_attr(\n",
    "    np.expand_dims(heatmap, 2),\n",
    "    plotable_image,\n",
    "    method='blended_heat_map',\n",
    "    # method='original_image',\n",
    "    cmap='jet',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure.savefig('/home/pdpino/downloads/iu-out-example-grad-cam.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = d.loc[d['image_fname'] == image_name]\n",
    "gt = str(row['ground_truth'].item())\n",
    "gen = str(row['generated'].item())\n",
    "gt, gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model.model.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check data amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../datasets/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_kwargs = {\n",
    "    # 'dataset_name': 'mimic-cxr',\n",
    "    'dataset_name': 'iu-x-ray',\n",
    "    'dataset_type': 'test',\n",
    "    'max_samples': None,\n",
    "    'frontal_only': True,\n",
    "    'reports_version': 'v4-1',\n",
    "    'image_size': (256, 256),\n",
    "}\n",
    "dataloader = prepare_data_report_generation(**dataset_kwargs)\n",
    "dataset = dataloader.dataset\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_train = 237964\n",
    "mimic_val = 1959\n",
    "mimic_test = 3403\n",
    "mimic_train + mimic_val + mimic_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iu_train = 2638\n",
    "iu_val = 336\n",
    "iu_test = 337\n",
    "iu_total = iu_train + iu_val + iu_test\n",
    "iu_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iu_train / iu_total, iu_val / iu_total, iu_test / iu_total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

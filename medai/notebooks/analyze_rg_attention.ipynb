{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['figure.facecolor'] = 'white'\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils/__init__.py\n",
    "config_logging(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../datasets/__init__.py\n",
    "%run ../utils/nlp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../models/checkpoint/__init__.py\n",
    "%run ../utils/files.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(metadata):\n",
    "    if 'vocab' in metadata:\n",
    "        return metadata['vocab']\n",
    "    if 'vocab' in metadata['dataset_kwargs']:\n",
    "        return metadata['dataset_kwargs']['vocab']\n",
    "    if 'vocab' in metadata['decoder_kwargs']:\n",
    "        return metadata['decoder_kwargs']['vocab']\n",
    "    raise Exception('Vocab not found in metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stuff_wrapper(run_name):\n",
    "    run_id = RunId(run_name, debug=False, task='rg')\n",
    "    compiled_model = load_compiled_model(run_id)\n",
    "    compiled_model.model.eval()\n",
    "    \n",
    "    vocab = get_vocab(compiled_model.metadata)\n",
    "    report_reader = ReportReader(vocab)\n",
    "\n",
    "    # HACK to wrap things\n",
    "    compiled_model.reader = report_reader\n",
    "    return compiled_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_hierarchical(metadata):\n",
    "    return 'h-' in metadata['decoder_kwargs']['decoder_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_atts_mean_and_std(att_dict, keys=None):\n",
    "    if keys is None:\n",
    "        keys = list(att_dict.keys())\n",
    "    else:\n",
    "        keys = [key for key in keys if key in att_dict]\n",
    "\n",
    "    n_rows = len(att_dict)\n",
    "    n_cols = 2\n",
    "    plt.figure(figsize=(n_cols*7, n_rows*5))\n",
    "\n",
    "    for i_key, key in enumerate(keys):\n",
    "        att = att_dict[key]\n",
    "\n",
    "        n_samples = att.size(0)\n",
    "\n",
    "        plt.subplot(n_rows, n_cols, i_key * 2 + 1)\n",
    "        plt.title(f'{key} (mean, samples={n_samples:,})')\n",
    "        plt.imshow(att.mean(dim=0).cpu().numpy())\n",
    "        plt.axis('off')\n",
    "        plt.colorbar()\n",
    "\n",
    "        plt.subplot(n_rows, n_cols, i_key * 2 + 2)\n",
    "        plt.title(f'{key} (STD, samples={n_samples:,})')\n",
    "        plt.imshow(att.std(dim=0).cpu().numpy())\n",
    "        plt.axis('off')\n",
    "        plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Analyze word-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Load stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run ../training/report_generation/flat.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "RUN_ID, COMPILED_MODEL = load_model_wrapper('0513_145846')\n",
    "METADATA = COMPILED_MODEL.metadata\n",
    "METADATA.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "HIERARCHICAL = is_hierarchical(METADATA)\n",
    "VOCAB, REPORT_READER = get_vocab_and_reader(METADATA)\n",
    "len(VOCAB), HIERARCHICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_kwargs = {\n",
    "    'hierarchical': HIERARCHICAL,\n",
    "    'dataset_name': 'iu-x-ray',\n",
    "    'image-size': (256, 256),\n",
    "    'max_samples': None,\n",
    "    'norm_by_sample': True,\n",
    "    'frontal_only': True,\n",
    "    'shuffle': True,\n",
    "    'vocab': VOCAB,\n",
    "}\n",
    "train_dataloader = prepare_data_report_generation(dataset_type='train', **dataset_kwargs)\n",
    "val_dataloader = prepare_data_report_generation(dataset_type='val', **dataset_kwargs)\n",
    "len(train_dataloader.dataset), len(val_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Show single examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_sample(dataloader, idx, free=True, colorbar=False):\n",
    "    item = dataloader.dataset[idx]\n",
    "\n",
    "    images = item.image.unsqueeze(0).to(DEVICE)\n",
    "    reports = torch.tensor(item.report).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = COMPILED_MODEL.model(images, reports, free=free, max_words=100)\n",
    "        gen_words, gen_att = output\n",
    "\n",
    "    report_gt = clean_gt_reports(reports)[0]\n",
    "    report_gen = _clean_gen_reports(gen_words)[0]\n",
    "    \n",
    "    print('GROUND TRUTH:')\n",
    "    print(REPORT_READER.idx_to_text(report_gt))\n",
    "    print('-'*50)\n",
    "    print('GENERATED:')\n",
    "    print(REPORT_READER.idx_to_text(report_gen))\n",
    "\n",
    "    gen_att = gen_att.squeeze(0).cpu().numpy() # shape: n_words+1, 16, 16\n",
    "    # assert len(report_gen) == gen_att.shape[0], f'{len(report_gen)} vs {gen_att.shape}'\n",
    "\n",
    "    plotable_image = tensor_to_range01(item.image).permute(1, 2, 0)\n",
    "    n_cols = 4\n",
    "    n_rows = math.ceil((len(report_gen) + 1) / n_cols)\n",
    "\n",
    "    plt.figure(figsize=(n_cols*7, n_rows*5))\n",
    "\n",
    "    plt.subplot(n_rows, n_cols, 1)\n",
    "    plt.imshow(plotable_image)\n",
    "    plt.axis('off')\n",
    "\n",
    "    for i in range(len(report_gen)):\n",
    "        word = report_gen[i]\n",
    "\n",
    "        plt.subplot(n_rows, n_cols, 2+i)\n",
    "        plt.title(REPORT_READER.idx_to_text([word]), fontsize=24)\n",
    "        plt.imshow(gen_att[i])\n",
    "        if colorbar:\n",
    "            plt.colorbar()\n",
    "\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_sample(val_dataloader, 100, colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Distribution for many samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def iterate_word_atts(dataloader, max_words=None):\n",
    "    counter = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        images = batch.images.to(DEVICE)\n",
    "        reports = batch.reports.to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = COMPILED_MODEL.model(images, reports, free=True, max_words=100)\n",
    "            gen_words, gen_att = output\n",
    "\n",
    "        reports_gt = clean_gt_reports(reports)\n",
    "        reports_gen = _clean_gen_reports(gen_words)\n",
    "\n",
    "        batch_size = gen_words.size(0)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            att = gen_att[i] # shape: n_words, 16, 16\n",
    "            report = reports_gen[i] # list\n",
    "\n",
    "            for word, att_map in zip(report, att):\n",
    "                word = REPORT_READER.idx_to_text([word])\n",
    "                yield word, att_map\n",
    "                \n",
    "                counter += 1\n",
    "                if max_words is not None and counter >= max_words:\n",
    "                    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Sample words from all the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_att = []\n",
    "\n",
    "for word, att in iterate_word_atts(val_dataloader, 10000):\n",
    "    all_att.append(att)\n",
    "    \n",
    "all_att = torch.stack(all_att, dim=0)\n",
    "all_att.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_att_mean = all_att.mean(dim=0).cpu().numpy()\n",
    "all_att_std = all_att.std(dim=0).cpu().numpy()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Mean')\n",
    "plt.imshow(all_att_mean)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('STD')\n",
    "plt.imshow(all_att_std)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Group by organs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run ../datasets/common/sentences2organs/compute.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "WORDS = list(VOCAB)\n",
    "len(WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_one_hot, warnings = find_organs_for_sentences(WORDS)\n",
    "MAIN_ORGAN_BY_WORD = {w:get_main_organ(o, w, warnings) for o, w in zip(_one_hot, WORDS)}\n",
    "len(MAIN_ORGAN_BY_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_att = defaultdict(list)\n",
    "\n",
    "for word, att in iterate_word_atts(val_dataloader): # 10000\n",
    "    organ = MAIN_ORGAN_BY_WORD[word]\n",
    "    all_att[organ].append(att)\n",
    "    \n",
    "all_att = {\n",
    "    k: torch.stack(a, dim=0)\n",
    "    for k, a in all_att.items()\n",
    "}\n",
    "all_att['heart'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_rows = len(all_att)\n",
    "n_cols = 2\n",
    "plt.figure(figsize=(n_cols*7, n_rows*5))\n",
    "\n",
    "for i_organ, organ in enumerate(MAIN_ORGANS):\n",
    "    att = all_att[organ]\n",
    "    \n",
    "    n_samples = att.size(0)\n",
    "\n",
    "    plt.subplot(n_rows, n_cols, i_organ * 2 + 1)\n",
    "    plt.title(f'{organ} (mean, samples={n_samples:,})')\n",
    "    plt.imshow(att.mean(dim=0).cpu().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot(n_rows, n_cols, i_organ * 2 + 2)\n",
    "    plt.title(f'{organ} (STD, samples={n_samples:,})')\n",
    "    plt.imshow(att.std(dim=0).cpu().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Group by relevant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "selected_words = [\n",
    "    'lungs', 'lung', 'heart', 'thorax', 'cardiomegaly', 'airspace',\n",
    "    'right', 'left', 'bilateral', 'bibasilar',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_att = defaultdict(list)\n",
    "\n",
    "for word, att in iterate_word_atts(val_dataloader):\n",
    "    if word in selected_words:\n",
    "        all_att[word].append(att)\n",
    "    \n",
    "all_att = {\n",
    "    k: torch.stack(a, dim=0)\n",
    "    for k, a in all_att.items()\n",
    "}\n",
    "all_att[selected_words[0]].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_words = 3\n",
    "words_to_plot = selected_words[:max_words]\n",
    "\n",
    "n_rows = len(words_to_plot)\n",
    "n_cols = 2\n",
    "plt.figure(figsize=(n_cols*7, n_rows*5))\n",
    "\n",
    "for i_word, word in enumerate(words_to_plot):\n",
    "    att = all_att[word]\n",
    "    \n",
    "    n_samples = att.size(0)\n",
    "    \n",
    "    plt.subplot(n_rows, n_cols, i_word * 2 + 1)\n",
    "    plt.title(f'{word} (mean, samples={n_samples:,})')\n",
    "    plt.imshow(att.mean(dim=0).cpu().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot(n_rows, n_cols, i_word * 2 + 2)\n",
    "    plt.title(f'{word} (STD, samples={n_samples:,})')\n",
    "    plt.imshow(att.std(dim=0).cpu().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.colorbar()\n",
    "    \n",
    "#     if word == 'thorax':\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word = 'lung'\n",
    "att = all_att[word]\n",
    "\n",
    "n_samples = att.size(0)\n",
    "n_samples = min(n_samples, 20)\n",
    "\n",
    "n_cols = min(3, n_samples)\n",
    "n_rows = math.ceil(n_samples / n_cols)\n",
    "plt.figure(figsize=(7*n_cols, n_rows*5))\n",
    "plt.suptitle(f'word={word} (samples={n_samples})', fontsize=18)\n",
    "\n",
    "for i_sample in range(n_samples):\n",
    "    att_sample = att[i_sample]\n",
    "    \n",
    "    plt.subplot(n_rows, n_cols, i_sample + 1)\n",
    "    plt.title(f'Sample {i_sample}', fontsize=16)\n",
    "    plt.imshow(att_sample.cpu().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze sentence attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../training/report_generation/hierarchical.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILED_MODEL1 = load_stuff_wrapper('0513_200618') # \n",
    "# COMPILED_MODEL2 = load_stuff_wrapper('0518_213120') # with supervise-attention\n",
    "# COMPILED_MODEL_BASE = load_stuff_wrapper('0523_031527')\n",
    "# COMPILED_MODEL_LRATT = load_stuff_wrapper('0525_232238')\n",
    "COMPILED_MODEL_LR = load_stuff_wrapper('0524_002837')\n",
    "COMPILED_MODEL_ISIZE = load_stuff_wrapper('0526_190114')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert COMPILED_MODEL_LR.reader.vocab == COMPILED_MODEL_ISIZE.reader.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPILED_MODEL_OLD = load_stuff_wrapper('0120_140940') # old with supervise-attention'\n",
    "{\n",
    "    k: {k2:(v2 if k2 != 'vocab' else len(v2)) for k2, v2 in v.items()} if isinstance(v, dict) else v\n",
    "    for k, v in COMPILED_MODEL_OLD.metadata.items()\n",
    "    if k != 'vocab'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPILED_MODEL_NO_SUPERV = load_stuff_wrapper('0518_225305')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __att-weights\n",
    "COMPILED_MODEL_BASE = load_stuff_wrapper('0519_215144')\n",
    "COMPILED_MODEL_LR = load_stuff_wrapper('0519_205343')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPILED_MODEL_LAMBDA = load_stuff_wrapper('0519_233237')\n",
    "COMPILED_MODEL_LRATT = load_stuff_wrapper('0520_005342')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPORT_READER = COMPILED_MODEL_LR.reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_kwargs = {\n",
    "    'hierarchical': True,\n",
    "    'dataset_name': 'iu-x-ray',\n",
    "    'image_size': (256, 256),\n",
    "    'max_samples': None,\n",
    "    'norm_by_sample': True,\n",
    "    'frontal_only': True,\n",
    "    'shuffle': False,\n",
    "    'sort_samples': True,\n",
    "    'vocab': REPORT_READER.vocab,\n",
    "}\n",
    "train_dataloader = prepare_data_report_generation(dataset_type='train', **dataset_kwargs)\n",
    "val_dataloader = prepare_data_report_generation(dataset_type='val', **dataset_kwargs)\n",
    "len(train_dataloader.dataset), len(val_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dataset_kwargs.copy()\n",
    "kwargs['image_size'] = (512, 512)\n",
    "train_dataloader_isize = prepare_data_report_generation(dataset_type='train', **kwargs)\n",
    "val_dataloader_isize = prepare_data_report_generation(dataset_type='val', **kwargs)\n",
    "len(train_dataloader_isize.dataset), len(val_dataloader_isize.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dataset_kwargs.copy()\n",
    "kwargs['vocab'] = COMPILED_MODEL_OLD.reader.vocab\n",
    "kwargs['image_size'] = COMPILED_MODEL_OLD.metadata['dataset_kwargs']['image_size']\n",
    "train_dataloader_old = prepare_data_report_generation(dataset_type='train', **kwargs)\n",
    "val_dataloader_old = prepare_data_report_generation(dataset_type='val', **kwargs)\n",
    "len(train_dataloader_old.dataset), len(val_dataloader_old.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../datasets/common/sentences2organs/compute.py\n",
    "%run ../datasets/common/constants.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(compiled_model, dataloader, idx, free=True, colorbar=False):\n",
    "    assert compiled_model.reader.vocab == dataloader.dataset.get_vocab()\n",
    "\n",
    "    def _print_report(report):\n",
    "        for i, sentence in enumerate(sentence_iterator(report)):\n",
    "            sentence = compiled_model.reader.idx_to_text(sentence)\n",
    "            organs_presence = _find_organs_for_sentence(sentence)\n",
    "            organs = '/'.join(\n",
    "                organ.replace('ground', '').replace(' lung', '')\n",
    "                for organ, presence in zip(JSRT_ORGANS, organs_presence)\n",
    "                if presence\n",
    "            )\n",
    "            print(f'{i} [{organs:>10}]: {sentence}')\n",
    "    \n",
    "    print(f'Testing run: {compiled_model.run_id}')\n",
    "\n",
    "    item = dataloader.dataset[idx]\n",
    "\n",
    "    images = item.image.unsqueeze(0).to(DEVICE)\n",
    "    reports = torch.tensor(item.report).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = compiled_model.model(images, None, free=free,\n",
    "                                      max_sentences=30, max_words=100)\n",
    "        words, stops, att_scores, topics = output\n",
    "        \n",
    "\n",
    "    report_gt = _flatten_gt_reports(reports)[0]\n",
    "    report_gen = _flatten_gen_reports(words, stops)[0]\n",
    "    \n",
    "    print('GROUND TRUTH:')\n",
    "    _print_report(report_gt)\n",
    "    print('-'*20)\n",
    "    print('GENERATED:')\n",
    "    _print_report(report_gen)\n",
    "\n",
    "    sentences = list(sentence_iterator(report_gen))\n",
    "    att_scores = att_scores.squeeze(0).cpu().numpy() # shape: n_sentences, 16, 16\n",
    "    # assert len(sentences) == att_scores.shape[0], f'{len(sentences)} vs {att_scores.shape}'\n",
    "\n",
    "    plotable_image = tensor_to_range01(item.image).permute(1, 2, 0)\n",
    "    n_cols = 4\n",
    "    n_rows = math.ceil((len(sentences) + 1) / n_cols)\n",
    "\n",
    "    plt.figure(figsize=(n_cols*7, n_rows*5))\n",
    "\n",
    "    plt.subplot(n_rows, n_cols, 1)\n",
    "    plt.imshow(plotable_image)\n",
    "    plt.axis('off')\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "\n",
    "        plt.subplot(n_rows, n_cols, 2+i)\n",
    "        # print(f'Sentence {i}: {compiled_model.reader.idx_to_text(sentence)}')\n",
    "        plt.title(f'Sentence {i}', fontsize=18)\n",
    "        plt.imshow(att_scores[i])\n",
    "        if colorbar:\n",
    "            plt.colorbar()\n",
    "\n",
    "        # plt.axis('off')\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = get_sample(COMPILED_MODEL_LR, val_dataloader, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = get_sample(COMPILED_MODEL_ISIZE, val_dataloader_isize, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = get_sample(COMPILED_MODEL_OLD, val_dataloader_old, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_attention_maps(compiled_model, dataloader, free=True, max_samples=None):\n",
    "    counter = 0\n",
    "    pbar = tqdm(total=len(dataloader.dataset) if max_samples is None else max_samples)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        images = batch.images.to(DEVICE)\n",
    "        reports = batch.reports.to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = compiled_model.model(images, reports, free=free,\n",
    "                                          max_sentences=30, max_words=100)\n",
    "            words, stops, att_scores, topics = output\n",
    "\n",
    "        reports_gt = _flatten_gt_reports(reports)\n",
    "        reports_gen = _flatten_gen_reports(words, stops)\n",
    "        \n",
    "        for gt_report, gen_report, atts in zip(reports_gt, reports_gen, att_scores):\n",
    "            gt_sentences = list(sentence_iterator(gt_report))\n",
    "            gen_sentences = list(sentence_iterator(gen_report))\n",
    "            \n",
    "            for i_sentence, (gt_sent, gen_sent, att) in enumerate(zip_longest(\n",
    "                gt_sentences,\n",
    "                gen_sentences,\n",
    "                atts,\n",
    "                fillvalue=[],\n",
    "                )):\n",
    "                gt_sent = REPORT_READER.idx_to_text(gt_sent)\n",
    "                gen_sent = REPORT_READER.idx_to_text(gen_sent)\n",
    "                yield gt_sent, gen_sent, att, i_sentence\n",
    "                \n",
    "                pbar.update(1)\n",
    "                counter += 1\n",
    "                if max_samples is not None and counter >= max_samples:\n",
    "                    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by organs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../datasets/common/sentences2organs/compute.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_att_by_organ(compiled_model, dataloader, correct_organ_only=False, **kwargs):\n",
    "    att_by_organ = defaultdict(list)\n",
    "    sentences_by_organ = defaultdict(list)\n",
    "\n",
    "    for gt_sent, _, att, _ in iter_attention_maps(compiled_model, dataloader, **kwargs):\n",
    "        if not isinstance(att, torch.Tensor):\n",
    "            continue\n",
    "\n",
    "        organs_onehot = _find_organs_for_sentence(gt_sent)\n",
    "        organ = get_main_organ(organs_onehot, gt_sent)\n",
    "        \n",
    "        if correct_organ_only:\n",
    "            gen_organ = get_main_organ(_find_organs_for_sentence(gen_sent), gen_sent)\n",
    "            if organ != gen_organ:\n",
    "                continue\n",
    "\n",
    "        att_by_organ[organ].append(att)\n",
    "        sentences_by_organ[organ].append(gt_sent)\n",
    "\n",
    "    att_by_organ = {\n",
    "        k: torch.stack(v, dim=0)\n",
    "        for k, v in att_by_organ.items()\n",
    "    }\n",
    "    return att_by_organ, sentences_by_organ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_by_organ_1, _ = get_att_by_organ(COMPILED_MODEL4, val_dataloader, max_samples=None)\n",
    "att_by_organ_2, _ = get_att_by_organ(COMPILED_MODEL3, val_dataloader, max_samples=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organs = [\n",
    "    organ\n",
    "    for organ in MAIN_ORGANS\n",
    "    if organ in att_by_organ_1 or organ in att_by_organ_2\n",
    "]\n",
    "\n",
    "n_rows = len(MAIN_ORGANS)\n",
    "n_cols = 4\n",
    "plt.figure(figsize=(n_cols*7, n_rows*5))\n",
    "\n",
    "for i_organ, organ in enumerate(organs):\n",
    "    def plot_heatmap(heatmap, index, title):\n",
    "        plt.subplot(n_rows, n_cols, i_organ * n_cols + index)\n",
    "        plt.title(title, fontsize=18)\n",
    "        plt.imshow(heatmap.cpu().numpy())\n",
    "        plt.axis('off')\n",
    "        plt.colorbar()\n",
    "\n",
    "    if organ in att_by_organ_1:\n",
    "        att = att_by_organ_1[organ]\n",
    "        n_samples = att.size(0)\n",
    "        plot_heatmap(att.mean(dim=0), 1, f'{organ} (mean, samples={n_samples:,})')\n",
    "        plot_heatmap(att.std(dim=0), 2, f'{organ} (STD, samples={n_samples:,})')\n",
    "    \n",
    "    if organ in att_by_organ_2:\n",
    "        att = att_by_organ_2[organ]\n",
    "        n_samples = att.size(0)\n",
    "        plot_heatmap(att.mean(dim=0), 3, f'supervised: {organ} (mean, samples={n_samples:,})')\n",
    "        plot_heatmap(att.std(dim=0), 4, f'supervised: {organ} (STD, samples={n_samples:,})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_by_position = defaultdict(list)\n",
    "\n",
    "for _, _, att, position in iter_attention_maps(COMPILED_MODEL4, val_dataloader,\n",
    "                                               max_samples=None):\n",
    "    if not isinstance(att, torch.Tensor):\n",
    "        continue\n",
    "        \n",
    "    if position >= 6:\n",
    "        position = '6+'\n",
    "        \n",
    "    att_by_position[f'pos {position}'].append(att)\n",
    "    \n",
    "att_by_position = {\n",
    "    k: torch.stack(v, dim=0)\n",
    "    for k, v in att_by_position.items()\n",
    "}\n",
    "att_by_position.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_atts_mean_and_std(att_by_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples_in_slider(i_position):\n",
    "    position = f'pos {i_position}'\n",
    "    atts = att_by_position[position]\n",
    "    def _plot_heatmap(idx):\n",
    "        heatmap = atts[idx].cpu().numpy()\n",
    "        \n",
    "        plt.suptitle(position)\n",
    "        plt.title(f'Sample {idx}')\n",
    "        plt.imshow(heatmap)\n",
    "\n",
    "    interact(_plot_heatmap, idx=widgets.IntSlider(min=0, max=len(atts)-1, step=1, value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples_in_slider(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

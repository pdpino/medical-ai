{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils/__init__.py\n",
    "config_logging(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils/files.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Debug light-chexpert cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Load Cache file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run ../metrics/report_generation/labeler_correctness/cache.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "FPATH = os.path.join(LABELER_CACHE_DIR, 'sentences_chexpert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(FPATH)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = list(df['sentences'])\n",
    "len(sentences), len(set(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Check empty sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sum(1 for s in sentences if len(s.split()) == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Remove trailing dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "repeated_sentences = defaultdict(lambda: 1)\n",
    "reduced_sentences = set()\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = sentence.split()\n",
    "    if sentence[-1] == '.':\n",
    "        sentence = sentence[:-1]\n",
    "    sentence = ' '.join(sentence)\n",
    "    if sentence in reduced_sentences:\n",
    "        repeated_sentences[sentence] += 1\n",
    "\n",
    "    reduced_sentences.add(sentence)\n",
    "    \n",
    "len(reduced_sentences), len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "repeated_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Remove repeated tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_duplicated_tokens(tokens):\n",
    "    return [\n",
    "        token\n",
    "        for i, token in enumerate(tokens)\n",
    "        if i == 0 or token != tokens[i-1]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "remove_duplicated_tokens(['there', 'there', 'is', 'stable', 'there'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "repeated_sentences = defaultdict(lambda: 1)\n",
    "reduced_sentences = set()\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = [\n",
    "        token\n",
    "        for token in sentence.split()\n",
    "        if token not in ('END', ',', '.', 'xxxx')\n",
    "    ]\n",
    "    \n",
    "    sentence = remove_duplicated_tokens(sentence)\n",
    "    \n",
    "    sentence = ' '.join(sentence)\n",
    "    if sentence in reduced_sentences:\n",
    "        repeated_sentences[sentence] += 1\n",
    "\n",
    "    reduced_sentences.add(sentence)\n",
    "    \n",
    "len(reduced_sentences), len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sorted(((k, v) for k, v in repeated_sentences.items()), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Clean sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Apply cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run ../metrics/report_generation/labeler_correctness/light_labeler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clean_sentence('there - there / &lt  asdf UNK'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clean_sentences = [\n",
    "    ' '.join(clean_sentence(sentence.split()))\n",
    "    for sentence in df['sentences']\n",
    "]\n",
    "len(set(clean_sentences)), len(clean_sentences), len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['clean_sentences'] = clean_sentences\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Remove duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols = [c for c in df.columns if 'sentence' not in c]\n",
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "unique_df = df.groupby('clean_sentences').first()\n",
    "unique_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(unique_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "unique_df = unique_df.reset_index(drop=False)\n",
    "del unique_df['sentences']\n",
    "unique_df = unique_df.rename(columns={'clean_sentences': 'sentences'})\n",
    "unique_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "unique_df.to_csv(FPATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Debug holistic chexpert-labeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run ../metrics/report_generation/chexpert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "FILL_EMPTY = -2\n",
    "FILL_UNCERTAIN = -1\n",
    "# dataset_name = 'iu-x-ray'\n",
    "dataset_name = 'mimic-cxr'\n",
    "\n",
    "gt_with_labels = _load_gt_df(dataset_name,\n",
    "                             fill_uncertain=FILL_UNCERTAIN, fill_empty=FILL_EMPTY)\n",
    "gt_with_labels.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run ../metrics/report_generation/chexpert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_gt_reports = list(gt_with_labels['Reports'])\n",
    "some_report = _gt_reports[3]\n",
    "\n",
    "reports = [\n",
    "    'Cardiomegaly .',\n",
    "    _gt_reports[10],\n",
    "    some_report,\n",
    "    some_report,\n",
    "    'no pneumothorax .',\n",
    "    'the cardiac silhouette and mediastinum size are within normal limits . there is no pulmonary edema . there is no focal consolidation . there are no xxxx of a pleural effusion . there is no evidence of pneumothorax .',\n",
    "    some_report,\n",
    "    'heart size is enlarged',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "labeler = ChexpertLabeler(fill_uncertain=FILL_UNCERTAIN, fill_empty=FILL_EMPTY,\n",
    "                          caller_id='debugging')\n",
    "labeler = CacheLookupLabeler(labeler, gt_with_labels)\n",
    "labeler = NBatchesLabeler(labeler)\n",
    "labeler = AvoidDuplicatedLabeler(labeler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "labels = labeler(reports)\n",
    "labels.shape, labels, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Check example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run ../utils/files.py\n",
    "%run ../metrics/report_generation/chexpert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "run1, run2 = '0604_165400', '0604_165401'\n",
    "# run1, run2 = '0609_194751', '0609_194752'\n",
    "\n",
    "run_original = RunId(run1, False, 'rg')\n",
    "run_copy = RunId(run2, False, 'rg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _load_file(run_id, fname):\n",
    "    fpath = os.path.join(get_results_folder(run_id), fname)\n",
    "    df = pd.read_csv(fpath)\n",
    "    sort_by = ['ground_truth']\n",
    "    if 'image_fname' in df.columns:\n",
    "        sort_by.append('image_fname')\n",
    "    df = df.sort_values(sort_by).reset_index(drop=True)\n",
    "    return df    \n",
    "\n",
    "def load_unlabeled(run_id):\n",
    "    return _load_file(run_id, 'outputs-free.csv')\n",
    "\n",
    "def load_labeled(run_id):\n",
    "    df = _load_file(run_id, 'outputs-labeled-free.csv')\n",
    "    \n",
    "    gt_labels = df[labels_with_suffix('gt')].to_numpy().astype(np.int8)\n",
    "    gen_labels = df[labels_with_suffix('gen')].to_numpy().astype(np.int8)\n",
    "    return df, gt_labels, gen_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df1 = load_unlabeled(run_original)\n",
    "df2 = load_unlabeled(run_copy)\n",
    "(df1 == df2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfl1, gt1, gen1 = load_labeled(run_original)\n",
    "dfl2, gt2, gen2 = load_labeled(run_copy)\n",
    "len(dfl1) == len(dfl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(dfl1['generated'] == df1['generated']).all(), (dfl2['generated'] == df2['generated']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(dfl1 == dfl2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(gt1 == gt2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(gen1 == gen2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "different_samples = (gen1 != gen2).any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d = dfl1[different_samples]['generated']\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Counter(Counter(d).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gen2[different_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gen1[(gen1 != gen2).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df1[(df1['generated'] != df2['generated'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug MIRQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = RunId('0121_210044', False, 'rg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'outputs-mirqi-free.csv'\n",
    "name = 'outputs-free.csv'\n",
    "\n",
    "fpath = os.path.join(get_results_folder(run_id), name)\n",
    "df = pd.read_csv(fpath)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df['attributes-gen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../metrics/report_generation/mirqi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(df['dataset_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "attributes = _call_mirqi_to_reports(df['ground_truth'])\n",
    "attributes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df), len(df['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df[:20]\n",
    "# df_small = df_small[['filename', 'epoch', 'dataset_type', ]]\n",
    "df_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = apply_mirqi_to_df(df_small, run_id.short_name, batches=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n ../eval_report_generation_mirqi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = RunId('0428_133018', True, 'rg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "evaluate_run(run_id,\n",
    "             override=False,\n",
    "             max_samples=20,\n",
    "             free=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt = _attributes_to_list(df2['attributes-gt'])\n",
    "gen = _attributes_to_list(df2['attributes-gen'])\n",
    "len(gt), len(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_v1 = MIRQI(gt, gen)\n",
    "scores_v2 = MIRQI_v2(gt, gen)\n",
    "scores_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.assign(**scores_v1)\n",
    "df3 = df3.assign(**scores_v2)\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = _calculate_metrics_dict(df3)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = [\n",
    "    'no pneumothorax .',\n",
    "    'cardiomegaly .',\n",
    "    'no active disease .',\n",
    "]\n",
    "labels = apply_mirqi_to_reports(reports)\n",
    "labels.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

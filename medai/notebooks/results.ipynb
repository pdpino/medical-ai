{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils/__init__.py\n",
    "%run ../utils/files.py\n",
    "%run ../metrics/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Choose task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK = 'seg'\n",
    "TASK = 'rg'\n",
    "# TASK = 'cls'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "KEY_COLS = ['run_name', 'dataset_type']\n",
    "if TASK == 'rg':\n",
    "    KEY_COLS.append('free')\n",
    "KEY_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TASK_FOLDER = _get_task_folder(TASK)\n",
    "BASE_FOLDER = os.path.join(WORKSPACE_DIR, TASK_FOLDER)\n",
    "RESULTS_FOLDER = os.path.join(BASE_FOLDER, 'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_suffix(filename):\n",
    "    match = re.search('.*metrics-(?P<suffix>\\w*)\\.json', filename)\n",
    "    if match is None:\n",
    "        suffix = ''\n",
    "    else:\n",
    "        suffix = match.group('suffix')\n",
    "    return suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "METRIC_TYPES = [\n",
    "    'chexpert',\n",
    "    'grad-cam',\n",
    "    'mirqi',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_results():\n",
    "    results_by_metric_type = {}\n",
    "\n",
    "    for run_name in os.listdir(RESULTS_FOLDER):\n",
    "        if run_name == 'debug':\n",
    "            continue\n",
    "\n",
    "        folder = os.path.join(RESULTS_FOLDER, run_name)\n",
    "        for filename in os.listdir(folder):\n",
    "            filepath = os.path.join(folder, filename)\n",
    "            if not os.path.isfile(filepath) or not filename.endswith('json'):\n",
    "                continue\n",
    "\n",
    "            metric_type = next(\n",
    "                (met for met in METRIC_TYPES if met in filename),\n",
    "                'base', # Default if no specific metric_type is found\n",
    "            )\n",
    "\n",
    "            with open(filepath, 'r') as f:\n",
    "                results_dict = json.load(f)\n",
    "   \n",
    "            results_df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "            results_df.reset_index(inplace=True)\n",
    "            results_df.rename(columns={'index': 'dataset_type'}, inplace=True)\n",
    "            results_df['run_name'] = run_name\n",
    "            if TASK == 'rg':\n",
    "                results_df['free'] = get_suffix(filename)           \n",
    "            \n",
    "            if metric_type not in results_by_metric_type:\n",
    "                results_by_metric_type[metric_type] = results_df\n",
    "            else:\n",
    "                prev = results_by_metric_type[metric_type]\n",
    "                results_by_metric_type[metric_type] = prev.append(results_df, ignore_index=True)\n",
    "\n",
    "    df = None\n",
    "    cols_in_order = list(KEY_COLS)\n",
    "    for results in results_by_metric_type.values():\n",
    "        cols_in_order += [col for col in results.columns if col not in cols_in_order]\n",
    "        \n",
    "        if df is None:\n",
    "            df = results\n",
    "        else:\n",
    "            df = df.merge(results, on=KEY_COLS, how='outer')\n",
    "                \n",
    "    return df[cols_in_order], results_by_metric_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def filter_results(dataset_type=None, metrics=None,\n",
    "                   metrics_contain=None, free=None,\n",
    "                   contains=None, doesnt_contain=None,\n",
    "                   drop=None, drop_na_rows=False, drop_key_cols=False):\n",
    "    df = RESULTS_DF\n",
    "    \n",
    "    if dataset_type:\n",
    "        if isinstance(dataset_type, str):\n",
    "            df = df[df['dataset_type'] == dataset_type]\n",
    "        elif isinstance(dataset_type, (list, tuple)):\n",
    "            dataset_type = set(dataset_type)\n",
    "            df = df[df['dataset_type'].isin(dataset_type)]\n",
    "    \n",
    "    if free is not None:\n",
    "        free_str = 'free' if free else 'notfree'\n",
    "        df = df.loc[df['free'] == free_str]\n",
    "    \n",
    "    if contains:\n",
    "        filter_contains = lambda d, s: d.loc[d['run_name'].str.contains(s)]\n",
    "        if isinstance(contains, (list, tuple)):\n",
    "            for c in contains:\n",
    "                df = filter_contains(df, c)\n",
    "        elif isinstance(contains, str):\n",
    "            df = filter_contains(df, contains)\n",
    "    \n",
    "    if doesnt_contain:\n",
    "        filter_doesnt_contain = lambda d, s: d.loc[~d['run_name'].str.contains(s)]\n",
    "        if isinstance(doesnt_contain, (list, tuple)):\n",
    "            for c in doesnt_contain:\n",
    "                df = filter_doesnt_contain(df, c)\n",
    "        elif isinstance(doesnt_contain, str):\n",
    "            df = filter_doesnt_contain(df, doesnt_contain)\n",
    "    \n",
    "    if drop:\n",
    "        df = df.loc[~df['run_name'].str.contains(drop)]\n",
    "        \n",
    "    if metrics_contain:\n",
    "        columns = KEY_COLS + [c for c in df.columns if metrics_contain in c]\n",
    "        df = df[columns]\n",
    "    elif metrics:\n",
    "        columns = KEY_COLS + metrics\n",
    "        df = df[columns]\n",
    "    \n",
    "    if drop_na_rows:\n",
    "        df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "    # Drop cols with all na\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    if drop_key_cols:\n",
    "        columns = [c for c in df.columns if c == 'run_name' or c not in KEY_COLS]\n",
    "        df = df[columns]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "RESULTS_DF, debug = load_results()\n",
    "print(len(RESULTS_DF))\n",
    "RESULTS_DF.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "set(\n",
    "    col.replace('-', '_').split('_')[0]\n",
    "    for col in RESULTS_DF.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def add_macro_avg_column(target_col):\n",
    "    matching_cols = [c for c in RESULTS_DF.columns if c.startswith(target_col)]\n",
    "    assert len(matching_cols) == 3, f'Matching cols not 3: {matching_cols}'\n",
    "    averages = RESULTS_DF[matching_cols].mean(axis=1)\n",
    "    RESULTS_DF[target_col] = averages\n",
    "    print(f'Calculated col {target_col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "add_macro_avg_column('n-shapes-gen')\n",
    "add_macro_avg_column('n-holes-gen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SEG_METRICS = []\n",
    "organs = ('heart', 'left lung', 'right lung')\n",
    "def _add_metric(metric_name, macro=True):\n",
    "    if macro: SEG_METRICS.append(metric_name)\n",
    "    SEG_METRICS.extend(f'{metric_name}-{organ}' for organ in organs)\n",
    "_add_metric('iou')\n",
    "# _add_metric('dice')\n",
    "_add_metric('n-shapes-gen')\n",
    "_add_metric('n-holes-gen')\n",
    "SEG_METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "replace_strs = [\n",
    "    # (r'^\\d{4}_\\d{6}_', ''),\n",
    "    (r'jsrt_scan_', ''),\n",
    "#     ('most-similar-image', '1nn'),\n",
    "#     ('_lr[\\d\\.]+', ''),\n",
    "#     ('_size256', ''),\n",
    "#     (r'_\\d{4}_\\d{6}_.*', ''),\n",
    "#     ('dummy-', ''),\n",
    "#     ('common', 'top'),\n",
    "#     ('-v2', ''),\n",
    "#     (r'top-(\\w)\\w+-(\\d+)', r'top-\\1-\\2'),\n",
    "#     ('_densenet-121', ''),\n",
    "]\n",
    "\n",
    "def rename_runs(run_name):\n",
    "    s = run_name\n",
    "    for target, replace_with in replace_strs:\n",
    "        s = re.sub(target, replace_with, s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filter_results(\n",
    "    metrics=SEG_METRICS,\n",
    "    dataset_type='test',\n",
    "    drop='1105_180035',\n",
    ").sort_values(\n",
    "    ['n-shapes-gen', 'n-holes-gen'],\n",
    "    ascending=True,\n",
    ").set_index('run_name').rename(index=rename_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP_METRICS = ['bleu1', 'bleu2', 'bleu3', 'bleu4', 'bleu', 'rougeL', 'ciderD']\n",
    "# CHEXPERT_METRICS = ['recall', 'prec', 'f1'] # 'acc', 'roc_auc', \n",
    "CHEXPERT_DISEASE_METRICS = [\n",
    "    c\n",
    "    for c in RESULTS_DF.columns\n",
    "    if any(\n",
    "        c.startswith(f'{ch}-')\n",
    "        for ch in ('f1', 'recall', 'prec')\n",
    "    ) and not c.endswith('-woNF')\n",
    "]\n",
    "# CHEXPERT_RUNTIME_METRICS = [col for col in RESULTS_DF.columns if col.startswith('chex')]\n",
    "# VAR_METRICS = [c for c in RESULTS_DF.columns if 'distinct' in c]\n",
    "# MIRQI_METRICS = [c for c in RESULTS_DF.columns if 'MIRQI' in c]\n",
    "MIRQI_METRICS_v1 = ['MIRQI-r', 'MIRQI-p', 'MIRQI-f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESSENTIAL_METRICS = [\n",
    "    'bleu', 'ciderD', 'rougeL',\n",
    "    # 'chex_f1', 'chex_acc', # 'chex_recall', 'chex_prec', # Runtime-chexpert\n",
    "    # 'MIRQI-v2-f',\n",
    "    # \n",
    "    # Holistic-chexpert:\n",
    "    'acc', 'f1', 'prec', 'recall', 'f1-woNF', 'prec-woNF', 'recall-woNF',\n",
    "    # 'pr_auc', 'pr_auc-woNF',\n",
    "    # 'acc',\n",
    "    # *CHEXPERT_DISEASE_METRICS,\n",
    "    *MIRQI_METRICS_v1,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_strs = [\n",
    "    (r'^\\d{4}_\\d{6}_', ''),\n",
    "    (r'_precnn-\\d{4}-\\d{6}', ''),\n",
    "    ('mimic-cxr_', ''),\n",
    "    # ('most-similar-image', '1nn'),\n",
    "    # (r'_lr[\\d\\.]+', ''),\n",
    "    # (r'_lr-emb[\\d\\.]+', ''),\n",
    "    ('_size256', ''),\n",
    "    ('-v2', ''),\n",
    "    ('_front', ''),\n",
    "    (r'__[\\w\\-]*', ''),\n",
    "]\n",
    "\n",
    "def rename_runs(run_name):\n",
    "    s = run_name\n",
    "    for target, replace_with in replace_strs:\n",
    "        s = re.sub(target, replace_with, s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = filter_results(\n",
    "    # contains='^01\\d{2}_\\d{6}_(h-|lstm)',\n",
    "    # contains=r'dummy|tpl|__bug-v4|0510_235335|paper', # __best-iu\n",
    "    # contains='__att',\n",
    "    contains='mimic-cxr',\n",
    "    # doesnt_contain='dummy-baseline|mimic',\n",
    "    dataset_type='test',\n",
    "    free=True,\n",
    "    metrics=ESSENTIAL_METRICS, # CHEXPERT_RUNTIME_METRICS,\n",
    "    drop_key_cols=True,\n",
    "    # drop_na_rows=True,\n",
    ")\n",
    "# res = res.replace(r'^\\d{4}_\\d{6}_(.*)', r'\\1', regex=True)\n",
    "# res = res.sort_values('run_name')\n",
    "res = res.set_index('run_name').rename(\n",
    "    index=rename_runs,\n",
    ")\n",
    "res.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Compare runtime chexpert vs holistic chexpert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def subtract_cols(df, cols_a, cols_b, drop_na_rows=True):\n",
    "    array_a = df[cols_a].to_numpy()\n",
    "    array_b = df[cols_b].to_numpy()\n",
    "    \n",
    "    df_2 = df[KEY_COLS].copy()\n",
    "    df_2 = pd.concat([df_2, pd.DataFrame(array_a - array_b, columns=cols_a)], axis=1)\n",
    "    \n",
    "    if drop_na_rows:\n",
    "        df_2.dropna(axis=0, inplace=True, how='any')\n",
    "    \n",
    "    return df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "metric = 'f1'\n",
    "\n",
    "runtime_chexpert = [c for c in RESULTS_DF.columns if c.startswith(f'chex_{metric}')]\n",
    "holistic_chexpert = [c for c in RESULTS_DF.columns if c.startswith(metric)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = RESULTS_DF\n",
    "df = df.loc[~df['run_name'].str.contains('dummy')]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "set(df['run_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = subtract_cols(df, runtime_chexpert, holistic_chexpert)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "run_name = '0112_154506_lstm-v2_lr0.001_densenet-121-v2_noes'\n",
    "debug = False\n",
    "d1 = load_rg_outputs(run_name, debug=debug, free=True)\n",
    "d2 = load_rg_outputs(run_name, debug=debug, free=False)\n",
    "len(d1), len(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c1 = Counter(d1['filename'])\n",
    "c2 = Counter(d2['filename'])\n",
    "len(c1), len(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for fname in c1.keys():\n",
    "    v1 = c1[fname]\n",
    "    v2 = c2[fname]\n",
    "    if v1 != v2:\n",
    "        print('Wrong: ', fname, v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "set(d2['dataset_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretty-print (latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_strs = [\n",
    "    (r'^\\d{4}_\\d{6}_', ''),\n",
    "    ('most-similar-image', '1nn'),\n",
    "    ('_lr[\\d\\.]+', ''),\n",
    "    ('_size256', ''),\n",
    "    (r'_\\d{4}_\\d{6}_.*', ''),\n",
    "    ('dummy-', ''),\n",
    "    ('common', 'top'),\n",
    "    ('-v2', ''),\n",
    "    (r'top-(\\w)\\w+-(\\d+)', r'top-\\1-\\2'),\n",
    "    ('_densenet-121', ''),\n",
    "]\n",
    "\n",
    "def rename_runs(run_name):\n",
    "    s = run_name\n",
    "    for target, replace_with in replace_strs:\n",
    "        s = re.sub(target, replace_with, s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['bleu', 'rougeL', 'ciderD'] + CHEXPERT_METRICS + MIRQI_METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filter_results(dataset_type='test',\n",
    "                    free=True,\n",
    "                    metrics=columns,\n",
    "                    contains='(?=_lstm-att-v2.*densenet|_lstm-v2.*densenet|dummy)',\n",
    "                    drop='0915_173951|0915_174222|0916_104739',\n",
    "                    drop_na_rows=True,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorten_cols = lambda s: s.replace('MIRQI-v2', 'v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.set_index('run_name').rename(\n",
    "    index=rename_runs,\n",
    "    columns=shorten_cols,\n",
    ").sort_index().to_latex(\n",
    "    columns=[shorten_cols(c) for c in columns],\n",
    "    float_format='%.3f',\n",
    "    column_format='l' + 'c' * len(columns),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# contains = 'covid-x'\n",
    "# contains = 'cxr14'\n",
    "# contains = 'e0'\n",
    "# contains = '0717_120222_covid-x_densenet-121_lr1e-06_os_aug-covid'\n",
    "# contains = '0717_101812_covid-x_densenet-121_lr1e-06_os-max2_aug-covid'\n",
    "run_name = '0717_120222_covid-x_densenet-121_lr1e-06_os_aug-covid' # WINNER\n",
    "\n",
    "contains = '0717_101812_covid-x_densenet-121_lr1e-06_os-max2_aug-covid'\n",
    "contains = 'covid-uc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ESSENTIAL_METRICS = [\n",
    "    'acc', 'roc_auc', 'hamming', # 'prec', 'recall',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    'acc', 'roc_auc', 'prec', 'recall', 'roc_auc_Cardiomegaly', 'roc_auc_Pneumonia',\n",
    "    'recall_Cardiomegaly', 'recall_Pneumonia',\n",
    "    'iobb-masks', 'iobb-masks-Cardiomegaly', 'iobb-masks-Pneumonia',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def simplify_names(s):\n",
    "    model_name = re.sub(r'^\\d{4}_\\d{6}_cxr14_(.*)_lr.*', r'\\1', s)\n",
    "    if 'hint' in s:\n",
    "        return f'{model_name}_hint'\n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d = filter_results(\n",
    "    contains=r'^(01|12|02)\\w+_cxr14_',\n",
    "    doesnt_contain=[\n",
    "        r'_Card',\n",
    "        r'_Pneu',\n",
    "        '1027_144914', # really bad results\n",
    "    ],\n",
    "    dataset_type=('val'), # -bbox\n",
    "    # metrics_contain='iou',\n",
    "    metrics=metrics, # ESSENTIAL_METRICS,\n",
    "    drop_key_cols=True,\n",
    "    # drop_na_rows=True,\n",
    ").sort_values('roc_auc', ascending=False)\n",
    "# d['run_name'] = d['run_name'].apply(simplify_names)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Report-generation: results at different report lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vals_words = [20, 25, 27, 33, 44, None]\n",
    "vals_sents = [3, 4, 5, 6, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_words = vals_words[0]\n",
    "suffix = f'max-words-{max_words}' if max_words else ''\n",
    "all_results = load_results(suffix)\n",
    "results_df_test = create_results_df(all_results, 'test')\n",
    "results_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save baseline results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../datasets/common/constants.py\n",
    "%run ../utils/__init__.py\n",
    "%run ../utils/files.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_metrics(folder, filename, results_dict):\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(results_dict, f, indent=2)\n",
    "    print(f'Saved dict to {filepath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mirqi_metrics(folder, results):\n",
    "    _save_metrics(folder, 'mirqi-metrics-free.json', results)\n",
    "\n",
    "def save_chexpert_metrics(folder, results):\n",
    "    _save_metrics(folder, 'chexpert-metrics-free.json', results)\n",
    "\n",
    "def save_runtime_metrics(folder, results):\n",
    "    _save_metrics(folder, 'metrics-free.json', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_woNF(metrics, prefixes, diseases=CHEXPERT_DISEASES, verbose=False):\n",
    "    if isinstance(prefixes, str):\n",
    "        prefixes = (prefixes,)\n",
    "\n",
    "    macro_avg_woNF = {}\n",
    "\n",
    "    for prefix in prefixes:\n",
    "        keys = [\n",
    "            f'{prefix}-{disease}'\n",
    "            for disease in diseases\n",
    "            if disease.lower() != 'no finding'\n",
    "        ]\n",
    "        macro_avg = np.mean([metrics[k] for k in keys])\n",
    "            \n",
    "        if verbose:\n",
    "            print(f'Prefix={prefix}, averaging: {keys}')\n",
    "        macro_avg_woNF[f'{prefix}-woNF'] = macro_avg\n",
    "    return macro_avg_woNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper MIRQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = get_results_folder(RunId('paper_iu-x-ray_zhang-et-al-mirqi', False, 'rg'), save_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu1, bleu2, bleu3, bleu4 = 0.441, 0.291, 0.203, 0.147\n",
    "runtime_results = {\n",
    "    'test': {\n",
    "        'bleu1': bleu1, 'bleu2': bleu2, 'bleu3': bleu3, 'bleu4': bleu4,\n",
    "        'bleu': np.mean([bleu1, bleu2, bleu3, bleu4]),\n",
    "        'ciderD': 0.304,\n",
    "        'rougeL': 0.367,\n",
    "    }\n",
    "}\n",
    "mirqi_results = {\n",
    "    'test': {\n",
    "        'MIRQI-r': 0.483,\n",
    "        'MIRQI-p': 0.490,\n",
    "        'MIRQI-f': 0.478,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_mirqi_metrics(folder, mirqi_results)\n",
    "save_runtime_metrics(folder, runtime_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lovelace et al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = get_results_folder(RunId('paper_mimic-cxr_lovelace-et-al', False, 'rg'), save_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using their transformer w/fine-tuning ablation\n",
    "bleu1, bleu2, bleu3, bleu4 = 0.415, 0.272, 0.193, 0.146\n",
    "runtime_results = {\n",
    "    'test': {\n",
    "        'bleu1': bleu1, 'bleu2': bleu2, 'bleu3': bleu3, 'bleu4': bleu4,\n",
    "        'bleu': np.mean([bleu1, bleu2, bleu3, bleu4]),\n",
    "        'ciderD': 0.316, # not sure if Cider-D or Cider\n",
    "        'rougeL': 0.318,\n",
    "    }\n",
    "}\n",
    "_values = {\n",
    "    'f1': 22.8,\n",
    "    'prec': 33.3,\n",
    "    'recall': 21.7,\n",
    "\n",
    "    'f1-Atelectasis': 32.2,\n",
    "    'f1-Cardiomegaly': 43.3,\n",
    "    'f1-Consolidation': 7.3,\n",
    "    'f1-Edema': 29.8,\n",
    "    'f1-Enlarged Cardiomediastinum': 5.9,\n",
    "    'f1-Fracture': 0,\n",
    "    'f1-Lung Lesion': 1.4,\n",
    "    'f1-Lung Opacity': 17.1,\n",
    "    'f1-No Finding': 54.1,\n",
    "    'f1-Pleural Effusion': 48.0,\n",
    "    'f1-Pleural Other': 0.9,\n",
    "    'f1-Pneumonia': 3.9,\n",
    "    'f1-Pneumothorax': 9.8,\n",
    "    'f1-Support Devices': 66.0,\n",
    "\n",
    "    'prec-Atelectasis': 43.0,\n",
    "    'prec-Cardiomegaly': 46.9,\n",
    "    'prec-Consolidation': 15.7,\n",
    "    'prec-Edema': 37.6,\n",
    "    'prec-Enlarged Cardiomediastinum': 12.3,\n",
    "    'prec-Fracture': 0,\n",
    "    'prec-Lung Lesion': 23.8,\n",
    "    'prec-Lung Opacity': 64.0,\n",
    "    'prec-No Finding': 39.0,\n",
    "    'prec-Pleural Effusion': 71.2,\n",
    "    'prec-Pleural Other': 16.1,\n",
    "    'prec-Pneumonia': 7,\n",
    "    'prec-Pneumothorax': 12.9,\n",
    "    'prec-Support Devices': 77.0,\n",
    "\n",
    "    'recall-Atelectasis': 25.8,\n",
    "    'recall-Cardiomegaly': 40.2,\n",
    "    'recall-Consolidation': 4.8,\n",
    "    'recall-Edema': 24.6,\n",
    "    'recall-Enlarged Cardiomediastinum': 3.9,\n",
    "    'recall-Fracture': 0,\n",
    "    'recall-Lung Lesion': 0.7,\n",
    "    'recall-Lung Opacity': 9.9,\n",
    "    'recall-No Finding': 88.2,\n",
    "    'recall-Pleural Effusion': 36.2,\n",
    "    'recall-Pleural Other': 0.5,\n",
    "    'recall-Pneumonia': 2.7,\n",
    "    'recall-Pneumothorax': 7.8,\n",
    "    'recall-Support Devices': 57.8,\n",
    "}\n",
    "woNF = calculate_avg_woNF(_values, ['f1', 'recall', 'prec'])\n",
    "_values.update(woNF)\n",
    "chexpert_results = {\n",
    "    'test': {\n",
    "        k: value / 100\n",
    "        for k, value in _values.items()\n",
    "    },\n",
    "}\n",
    "woNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chexpert_metrics(folder, chexpert_results)\n",
    "save_runtime_metrics(folder, runtime_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boag et al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = get_results_folder(RunId('paper_mimic-cxr_boag-et-al-1nn', False, 'rg'), save_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using their 1-NN model\n",
    "bleu1, bleu2, bleu3, bleu4 = 0.305, 0.171, 0.098, 0.057\n",
    "runtime_results = {\n",
    "    'test': {\n",
    "        'bleu1': bleu1, 'bleu2': bleu2, 'bleu3': bleu3, 'bleu4': bleu4,\n",
    "        'bleu': np.mean([bleu1, bleu2, bleu3, bleu4]),\n",
    "        'ciderD': 0.755, # not sure if Cider-D or Cider\n",
    "    }\n",
    "}\n",
    "_values = {\n",
    "    'acc': 0.818,\n",
    "    'prec': 0.253,\n",
    "    'f1': 0.258,\n",
    "\n",
    "    'f1-Support Devices': 0.527,\n",
    "    'f1-Lung Opacity': 0.417,\n",
    "    'f1-Cardiomegaly': 0.445,\n",
    "    'f1-Atelectasis': 0.375,\n",
    "    'f1-No Finding': 0.455,\n",
    "    'f1-Pleural Effusion': 0.532,\n",
    "    'f1-Edema': 0.286,\n",
    "    'f1-Enlarged Cardiomediastinum': 0.142,\n",
    "    'f1-Pneumonia': 0.08,\n",
    "    'f1-Pneumothorax': 0.111,\n",
    "    'f1-Fracture': 0.060,\n",
    "    'f1-Lung Lesion': 0.062,\n",
    "    'f1-Consolidation': 0.085,\n",
    "    'f1-Pleural Other': 0.039,\n",
    "}\n",
    "woNF = calculate_avg_woNF(_values, 'f1')\n",
    "_values.update(woNF)\n",
    "chexpert_results = {\n",
    "    'test': _values,\n",
    "}\n",
    "woNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chexpert_metrics(folder, chexpert_results)\n",
    "save_runtime_metrics(folder, runtime_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cnn-rnn-beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = get_results_folder(RunId('paper_mimic-cxr_boag-et-al-cnn-rnn-beam', False, 'rg'), save_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using their CNN-RNN-beam\n",
    "bleu1, bleu2, bleu3, bleu4 = 0.305, 0.201, 0.137, 0.092\n",
    "runtime_results = {\n",
    "    'test': {\n",
    "        'bleu1': bleu1, 'bleu2': bleu2, 'bleu3': bleu3, 'bleu4': bleu4,\n",
    "        'bleu': np.mean([bleu1, bleu2, bleu3, bleu4]),\n",
    "        'ciderD': 0.850, # not sure if Cider-D or Cider\n",
    "    }\n",
    "}\n",
    "_values = {\n",
    "    'acc': 0.837, \n",
    "    'prec': 0.304,\n",
    "    'f1': 0.186,\n",
    "\n",
    "    'f1-Support Devices': 0.613,\n",
    "    'f1-Lung Opacity': 0.077,\n",
    "    'f1-Cardiomegaly': 0.390,\n",
    "    'f1-Atelectasis': 0.146,\n",
    "    'f1-No Finding': 0.407,\n",
    "    'f1-Pleural Effusion': 0.473,\n",
    "    'f1-Edema': 0.271,\n",
    "    'f1-Enlarged Cardiomediastinum': 0.134,\n",
    "    'f1-Pneumonia': 0.03,\n",
    "    'f1-Pneumothorax': 0.043,\n",
    "    'f1-Fracture': 0.001,\n",
    "    'f1-Lung Lesion': 0.001, # less than that\n",
    "    'f1-Consolidation': 0.014,\n",
    "    'f1-Pleural Other': 0.001, # less than that\n",
    "}\n",
    "woNF = calculate_avg_woNF(_values, ['f1'], CHEXPERT_DISEASES)\n",
    "_values.update(woNF)\n",
    "chexpert_results = {\n",
    "    'test': _values,\n",
    "}\n",
    "woNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chexpert_metrics(folder, chexpert_results)\n",
    "save_runtime_metrics(folder, runtime_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liu et al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = get_results_folder(RunId('paper_mimic-cxr_liu-et-al-ccr', False, 'rg'), save_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using their CCR ablation\n",
    "bleu1, bleu2, bleu3, bleu4 = 0.294, 0.190, 0.134, 0.094\n",
    "runtime_results = {\n",
    "    'test': {\n",
    "        'bleu1': bleu1, 'bleu2': bleu2, 'bleu3': bleu3, 'bleu4': bleu4,\n",
    "        'bleu': np.mean([bleu1, bleu2, bleu3, bleu4]),\n",
    "        'ciderD': 0.956,\n",
    "        'rougeL': 0.284,\n",
    "    }\n",
    "}\n",
    "_values = {\n",
    "    'acc': 0.868,\n",
    "    'prec': 0.313,\n",
    "    'recall': 0.126,\n",
    "\n",
    "    'prec-No Finding': 0.491,\n",
    "    'prec-Enlarged Cardiomediastinum': 0.202,\n",
    "    'prec-Cardiomegaly': 0.678,\n",
    "    'prec-Lung Lesion': 0,\n",
    "    'prec-Lung Opacity': 0.640,\n",
    "    'prec-Edema': 0.280,\n",
    "    'prec-Consolidation': 0.037,\n",
    "    'prec-Pneumonia': 0,\n",
    "    'prec-Atelectasis': 0.476,\n",
    "    'prec-Pneumothorax': 0.039,\n",
    "    'prec-Pleural Effusion': 0.683,\n",
    "    'prec-Pleural Other': 0,\n",
    "    'prec-Fracture': 0,\n",
    "    'prec-Support Devices': 0.849,\n",
    "}\n",
    "woNF = calculate_avg_woNF(_values, ['prec'], CHEXPERT_DISEASES, verbose=False)\n",
    "_values.update(woNF)\n",
    "chexpert_results = {\n",
    "    'test': _values,\n",
    "}\n",
    "woNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chexpert_metrics(folder, chexpert_results)\n",
    "save_runtime_metrics(folder, runtime_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = get_results_folder(RunId('paper_mimic-cxr_liu-et-al-full', False, 'rg'), save_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using their CCR ablation\n",
    "bleu1, bleu2, bleu3, bleu4 = 0.313, 0.206, 0.146, 0.103\n",
    "runtime_results = {\n",
    "    'test': {\n",
    "        'bleu1': bleu1, 'bleu2': bleu2, 'bleu3': bleu3, 'bleu4': bleu4,\n",
    "        'bleu': np.mean([bleu1, bleu2, bleu3, bleu4]),\n",
    "        'ciderD': 1.046,\n",
    "        'rougeL': 0.306,\n",
    "    }\n",
    "}\n",
    "_values = {\n",
    "    'acc': 0.867,\n",
    "    'prec': 0.309,\n",
    "    'recall': 0.134,\n",
    "\n",
    "    'prec-No Finding': 0.405,\n",
    "    'prec-Enlarged Cardiomediastinum': 0.167,\n",
    "    'prec-Cardiomegaly': 0.704,\n",
    "    'prec-Lung Lesion': 0,\n",
    "    'prec-Lung Opacity': 0.460,\n",
    "    'prec-Edema': 0,\n",
    "    'prec-Consolidation': 0,\n",
    "    'prec-Pneumonia': 0.4,\n",
    "    'prec-Atelectasis': 0.521,\n",
    "    'prec-Pneumothorax': 0.098,\n",
    "    'prec-Pleural Effusion': 0.689,\n",
    "    'prec-Pleural Other': 0,\n",
    "    'prec-Fracture': 0,\n",
    "    'prec-Support Devices': 0.880,\n",
    "}\n",
    "woNF = calculate_avg_woNF(_values, ['prec'], CHEXPERT_DISEASES, verbose=False)\n",
    "woNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_values.update(woNF)\n",
    "\n",
    "chexpert_results = {\n",
    "    'test': _values,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chexpert_metrics(folder, chexpert_results)\n",
    "save_runtime_metrics(folder, runtime_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

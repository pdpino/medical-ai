{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils/__init__.py\n",
    "%run ../utils/files.py\n",
    "%run ../metrics/__init__.py\n",
    "%run ../models/checkpoint/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK = 'seg'\n",
    "# TASK = 'rg'\n",
    "# TASK = 'cls-seg'\n",
    "TASK = 'cls' # ('cls', 'cls-seg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_COLS = ['run_name', 'timestamp', 'dataset_type']\n",
    "if TASK == 'rg':\n",
    "    KEY_COLS.extend(['free', 'best', 'beam'])\n",
    "KEY_COLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_run_folders(tasks, target_folder):\n",
    "    if isinstance(tasks, str):\n",
    "        tasks = (tasks,)\n",
    "    results = []\n",
    "    for task in tasks:\n",
    "        target_glob = os.path.join(WORKSPACE_DIR, _get_task_folder(task), target_folder, '*')\n",
    "        results.extend(glob.glob(target_glob))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_free_suffix_and_beam(filename):\n",
    "    match = re.search(\n",
    "        '.*metrics-(?P<free>[notfree]+)(-(?P<suffix>[\\w\\-]+))?(\\.bs(?P<beam>\\d+))?\\.json',\n",
    "        filename,\n",
    "    )\n",
    "    if match is None:\n",
    "        return 'free', None, 0\n",
    "    return match.group('free'), match.group('suffix'), match.group('beam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_free_suffix_and_beam('chexpert-metrics-notfree.bs100.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_TYPES = [\n",
    "    'chexpert',\n",
    "    'grad-cam',\n",
    "    'mirqi',\n",
    "    'bertscore',\n",
    "    'bleurt',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_timestamp(run_name):\n",
    "    if re.search('^\\d{4}_\\d{6}', run_name):\n",
    "        return run_name[:11]\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results():\n",
    "    results_by_metric_type = {}\n",
    "\n",
    "    for run_folder in _get_run_folders(TASK, 'results'):\n",
    "        run_name = os.path.basename(run_folder)\n",
    "        \n",
    "        if run_name == 'debug':\n",
    "            continue\n",
    "\n",
    "        for filename in os.listdir(run_folder):\n",
    "            filepath = os.path.join(run_folder, filename)\n",
    "            if not os.path.isfile(filepath) or not filename.endswith('json'):\n",
    "                continue\n",
    "\n",
    "            if any(\n",
    "                s in filename\n",
    "                for s in ('thresholds-', 'training-stats')\n",
    "                ):\n",
    "                continue\n",
    "                \n",
    "            metric_type = next(\n",
    "                (met for met in METRIC_TYPES if met in filename),\n",
    "                'base', # Default if no specific metric_type is found\n",
    "            )\n",
    "\n",
    "            with open(filepath, 'r') as f:\n",
    "                results_dict = json.load(f)\n",
    "   \n",
    "            results_df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "            results_df.reset_index(inplace=True)\n",
    "            results_df.rename(columns={'index': 'dataset_type'}, inplace=True)\n",
    "            results_df['run_name'] = run_name\n",
    "            results_df['timestamp'] = _extract_timestamp(run_name)\n",
    "            if TASK == 'rg':\n",
    "                free, best_metric, beam_size = get_free_suffix_and_beam(filename)\n",
    "                results_df['free'] = free\n",
    "                results_df['best'] = best_metric\n",
    "                results_df['beam'] = int(beam_size or 0)\n",
    "            \n",
    "            if metric_type not in results_by_metric_type:\n",
    "                results_by_metric_type[metric_type] = results_df\n",
    "            else:\n",
    "                # Append to the previous DF\n",
    "                prev = results_by_metric_type[metric_type]\n",
    "                results_by_metric_type[metric_type] = prev.append(results_df, ignore_index=True)\n",
    "\n",
    "    df = None\n",
    "    cols_in_order = list(KEY_COLS)\n",
    "    for results in results_by_metric_type.values():\n",
    "        cols_in_order += [col for col in results.columns if col not in cols_in_order]\n",
    "        \n",
    "        if df is None:\n",
    "            df = results\n",
    "        else:\n",
    "            df = df.merge(results, on=KEY_COLS, how='outer')\n",
    "                \n",
    "    return df[cols_in_order], results_by_metric_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_stats():\n",
    "    re_filename = re.compile(r'training-stats.*.json')\n",
    "\n",
    "    all_training_stats = []\n",
    "\n",
    "    for run_folder in _get_run_folders(TASK, 'models'):\n",
    "        run_name = os.path.basename(run_folder)\n",
    "\n",
    "        if run_name == 'debug':\n",
    "            continue\n",
    "\n",
    "        for filename in os.listdir(run_folder):\n",
    "            if not re_filename.match(filename):\n",
    "                continue\n",
    "            \n",
    "            filepath = os.path.join(run_folder, filename)\n",
    "            if not os.path.isfile(filepath):\n",
    "                continue\n",
    "\n",
    "            with open(filepath, 'r') as f:\n",
    "                training_stats_original_dict = json.load(f)\n",
    "\n",
    "            # Unwrap dicts and fix old key-values\n",
    "            training_stats = dict()\n",
    "            for key, value in training_stats_original_dict.items():\n",
    "                if isinstance(value, dict):\n",
    "                    for k, v in value.items():\n",
    "                        training_stats[k] = v\n",
    "                elif key == 'n_epochs' or key == 'epochs':\n",
    "                    training_stats['final_epoch'] = value\n",
    "                    training_stats['current_epoch'] = value\n",
    "                else:\n",
    "                    training_stats[key] = value\n",
    "                    \n",
    "            # Add total-time column\n",
    "            secs_per_epoch = training_stats['secs_per_epoch']\n",
    "            n_epochs = training_stats['current_epoch'] - training_stats['initial_epoch']\n",
    "            total_time = secs_per_epoch * n_epochs\n",
    "            training_stats['total_time'] = duration_to_str(total_time)\n",
    "    \n",
    "            # Add pretty-time columns\n",
    "            training_stats['time_per_epoch'] = duration_to_str(secs_per_epoch)\n",
    "    \n",
    "            # Add run_name column\n",
    "            training_stats['run_name'] = run_name\n",
    "\n",
    "            all_training_stats.append(training_stats)\n",
    "        \n",
    "    df = pd.DataFrame(all_training_stats)\n",
    "    cols = ['run_name'] + [c for c in df.columns if c != 'run_name']\n",
    "    df = df[cols]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _filter_df_run_name_contains(df, contains):\n",
    "    if contains:\n",
    "        filter_contains = lambda d, s: d.loc[d['run_name'].str.contains(s)]\n",
    "        if isinstance(contains, (list, tuple)):\n",
    "            for c in contains:\n",
    "                df = filter_contains(df, c)\n",
    "        elif isinstance(contains, str):\n",
    "            df = filter_contains(df, contains)\n",
    "    return df\n",
    "\n",
    "def __rename_run_name(run_name, replace_strs):\n",
    "    s = run_name\n",
    "    for target, replace_with in replace_strs:\n",
    "        s = re.sub(target, replace_with, s)\n",
    "    return s\n",
    "\n",
    "def _df_rename_runs(df, rename_runs):\n",
    "    if rename_runs and 'run_name' in df:\n",
    "        df['run_name'] = [\n",
    "            __rename_run_name(r, rename_runs)\n",
    "            for r in df['run_name']\n",
    "        ]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_renamer(replace_strs):\n",
    "    def _rename_run(run_name):\n",
    "        s = run_name\n",
    "        for target, replace_with in replace_strs:\n",
    "            s = re.sub(target, replace_with, s)\n",
    "        return s\n",
    "    return _rename_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_results(dataset_type=None, metrics=None,\n",
    "                   metrics_contain=None,\n",
    "                   contains=None, doesnt_contain=None,\n",
    "                   drop=None, drop_na_rows=False, drop_key_cols=False,\n",
    "                   timestamp_col=False, best_col=False, beam_col=False,\n",
    "                   rename_runs=None, remove_timestamp=False,\n",
    "                   free=None,\n",
    "                   beam_size=None,\n",
    "                   best=None,\n",
    "                  ):\n",
    "    df = RESULTS_DF\n",
    "    \n",
    "    if dataset_type:\n",
    "        if isinstance(dataset_type, str):\n",
    "            df = df[df['dataset_type'] == dataset_type]\n",
    "        elif isinstance(dataset_type, (list, tuple)):\n",
    "            dataset_type = set(dataset_type)\n",
    "            df = df[df['dataset_type'].isin(dataset_type)]\n",
    "    \n",
    "    if free is not None:\n",
    "        free_str = 'free' if free else 'notfree'\n",
    "        df = df.loc[df['free'] == free_str]\n",
    "    \n",
    "    if best is not None:\n",
    "        # Keep null to keep paper ones\n",
    "        df = df.loc[((df['best'] == best) | (df['best'].isnull()))]\n",
    "    if beam_size is not None:\n",
    "        if 'beam' not in df.columns:\n",
    "            print('ERROR: cannot filter by beam_size, beam column not found')\n",
    "        else:\n",
    "            df = df.loc[(\n",
    "                (df['beam'] == beam_size) |\n",
    "                ((df['best'].isnull()) & (df['beam'] == 0)) ## Other cases\n",
    "            )]\n",
    "    \n",
    "    \n",
    "    df = _filter_df_run_name_contains(df, contains)\n",
    "    \n",
    "    if doesnt_contain:\n",
    "        filter_doesnt_contain = lambda d, s: d.loc[~d['run_name'].str.contains(s)]\n",
    "        if isinstance(doesnt_contain, (list, tuple)):\n",
    "            for c in doesnt_contain:\n",
    "                df = filter_doesnt_contain(df, c)\n",
    "        elif isinstance(doesnt_contain, str):\n",
    "            df = filter_doesnt_contain(df, doesnt_contain)\n",
    "    \n",
    "    if drop:\n",
    "        df = df.loc[~df['run_name'].str.contains(drop)]\n",
    "        \n",
    "    if metrics_contain:\n",
    "        columns = KEY_COLS + [c for c in df.columns if metrics_contain in c]\n",
    "        df = df[columns]\n",
    "    elif metrics:\n",
    "        columns = KEY_COLS + metrics\n",
    "        df = df[columns]\n",
    "    \n",
    "    if drop_na_rows:\n",
    "        df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "    # Drop cols with all na\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    if drop_key_cols:\n",
    "        columns = [\n",
    "            c for c in df.columns\n",
    "            if c == 'run_name' or \\\n",
    "                (timestamp_col and c == 'timestamp') or \\\n",
    "                (best_col and c == 'best') or \\\n",
    "                (beam_col and c == 'beam') or \\\n",
    "                c not in KEY_COLS\n",
    "        ]\n",
    "        df = df[columns]\n",
    "\n",
    "    _df_rename_runs(df, rename_runs)\n",
    "\n",
    "    if remove_timestamp:\n",
    "        df = df.replace(r'^\\d{4}_\\d{6}_', '', regex=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_training_stats(contains=None, columns=None,\n",
    "                          rename_runs=None, remove_timestamp=False,\n",
    "                         ):\n",
    "    df = TRAINING_STATS_DF\n",
    "    \n",
    "    df = _filter_df_run_name_contains(df, contains)\n",
    "\n",
    "    _df_rename_runs(df, rename_runs)\n",
    "    \n",
    "    if remove_timestamp:\n",
    "        df = df.replace(r'^\\d{4}_\\d{6}_', '', regex=True)\n",
    "        \n",
    "    if columns is not None:\n",
    "        df = df[columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"The frame.append method is deprecated and will be removed from pandas in a future version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DF, debug = load_results()\n",
    "print(len(RESULTS_DF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_STATS_DF = load_training_stats()\n",
    "print(len(TRAINING_STATS_DF))\n",
    "# TRAINING_STATS_DF.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# set(\n",
    "#     col.replace('-', '_').split('_')[0]\n",
    "#     for col in RESULTS_DF.columns\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def add_macro_avg_column(target_col):\n",
    "    matching_cols = [c for c in RESULTS_DF.columns if c.startswith(target_col)]\n",
    "    assert len(matching_cols) == 3, f'Matching cols not 3: {matching_cols}'\n",
    "    averages = RESULTS_DF[matching_cols].mean(axis=1)\n",
    "    RESULTS_DF[target_col] = averages\n",
    "    print(f'Calculated col {target_col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "add_macro_avg_column('n-shapes-gen')\n",
    "add_macro_avg_column('n-holes-gen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SEG_METRICS = []\n",
    "organs = ('heart', 'left lung', 'right lung')\n",
    "def _add_metric(metric_name, macro=True):\n",
    "    if macro: SEG_METRICS.append(metric_name)\n",
    "    SEG_METRICS.extend(f'{metric_name}-{organ}' for organ in organs)\n",
    "_add_metric('iou')\n",
    "# _add_metric('dice')\n",
    "_add_metric('n-shapes-gen')\n",
    "_add_metric('n-holes-gen')\n",
    "SEG_METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "replace_strs = [\n",
    "    # (r'^\\d{4}_\\d{6}_', ''),\n",
    "    (r'jsrt_scan_', ''),\n",
    "#     ('most-similar-image', '1nn'),\n",
    "#     ('_lr[\\d\\.]+', ''),\n",
    "#     ('_size256', ''),\n",
    "#     (r'_\\d{4}_\\d{6}_.*', ''),\n",
    "#     ('dummy-', ''),\n",
    "#     ('common', 'top'),\n",
    "#     ('-v2', ''),\n",
    "#     (r'top-(\\w)\\w+-(\\d+)', r'top-\\1-\\2'),\n",
    "#     ('_densenet-121', ''),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filter_results(\n",
    "    metrics=SEG_METRICS,\n",
    "    dataset_type='test',\n",
    "    drop='1105_180035',\n",
    "    rename_runs=replace_strs,\n",
    ").sort_values(\n",
    "    ['n-shapes-gen', 'n-holes-gen'],\n",
    "    ascending=True,\n",
    ").set_index('run_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHEXPERT_METRICS = ['recall', 'prec', 'f1'] # 'acc', 'roc_auc', \n",
    "CHEXPERT_DISEASE_METRICS = [\n",
    "    c\n",
    "    for c in RESULTS_DF.columns\n",
    "    if any(\n",
    "        c.startswith(f'{ch}-')\n",
    "        for ch in ('f1', 'recall', 'prec')\n",
    "    ) and not c.endswith('-woNF')\n",
    "]\n",
    "# CHEXPERT_RUNTIME_METRICS = [col for col in RESULTS_DF.columns if col.startswith('chex')]\n",
    "# VAR_METRICS = [c for c in RESULTS_DF.columns if 'distinct' in c]\n",
    "# MIRQI_METRICS = [c for c in RESULTS_DF.columns if 'MIRQI' in c]\n",
    "MIRQI_METRICS_v1 = ['MIRQI-f', 'MIRQI-p', 'MIRQI-r']\n",
    "# MIRQI_METRICS_v2 = [f'MIRQI-v2-{s}' for s in ('f', 'p', 'r', 'np', 'sp', 'attr-p', 'attr-r')]\n",
    "# MIRQI_METRICS_v2 = [c for c in RESULTS_DF.columns if 'MIRQI-v2' in c]\n",
    "# MIRQI_METRICS_v3 = ['MIRQI-v3-clean-f', 'MIRQI-v3-clean-p', 'MIRQI-v3-clean-r']\n",
    "# MIRQI_METRICS_v4 = ['MIRQI-v4-pos-f', 'MIRQI-v4-pos-p', 'MIRQI-v4-pos-r']\n",
    "# MIRQI_METRICS_v5 = ['MIRQI-v5-game-f', 'MIRQI-v5-game-p', 'MIRQI-v5-game-r']\n",
    "# MIRQI_METRICS_v6 = ['MIRQI-v6-game-f', 'MIRQI-v6-game-p', 'MIRQI-v6-game-r']\n",
    "# MIRQI_METRICS_v7 = [f'MIRQI-v7-attr-only-{s}' for s in ('f', 'p', 'r')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP_METRICS = [\n",
    "    'bleu', 'bleu1', 'bleu2', 'bleu3', 'bleu4',\n",
    "    'rougeL', 'ciderD',\n",
    "]\n",
    "ESSENTIAL_METRICS = [\n",
    "    ## (holistic) CHEXPERT:\n",
    "    # 'acc',\n",
    "    'f1', 'prec', 'recall',\n",
    "\n",
    "    # 'f1-woNF', 'prec-woNF', 'recall-woNF', # *CHEXPERT_DISEASE_METRICS,\n",
    "    \n",
    "    ## NLP\n",
    "    # 'bleu1', 'bleu2', 'bleu3', 'bleu4',\n",
    "    'bleu1', 'bleu4',\n",
    "    'rougeL', 'ciderD',\n",
    "    'bleurt', 'bertscore-f1',\n",
    "    # 'meteor',\n",
    "\n",
    "    # 'chex_f1', 'chex_acc', # 'chex_recall', 'chex_prec', # Runtime-chexpert\n",
    "\n",
    "    # *MIRQI_METRICS_v1,\n",
    "#     *MIRQI_METRICS_v2,\n",
    "    # *MIRQI_METRICS_v5,\n",
    "    # *MIRQI_METRICS_v6,\n",
    "    # *MIRQI_METRICS_v3,\n",
    "    # *MIRQI_METRICS_v7,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_runs = [\n",
    "    # (r'_precnn-\\d{4}-\\d{6}', ''),\n",
    "    (r'(mimic-cxr|iu-x-ray)_', ''),\n",
    "    # ('most-similar-image', '1nn'),\n",
    "    # (r'_lr(-\\w+)?[\\d\\.e\\-]+', ''),\n",
    "    # (r'_lr[\\d\\.]+', ''),\n",
    "    ('_size256', ''),\n",
    "    # ('-v2', ''),\n",
    "    ('_front', ''),\n",
    "    (r'__[\\w\\-]*', ''),\n",
    "    (r'_(pre)?cnn\\-\\d{4}\\-\\d{6}', ''),\n",
    "    ('_densenet-121-v2', ''),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IU = True\n",
    "# MICCAI experiments:\n",
    "# iu lstm 0612_035549, best-bleu: 0621_134437\n",
    "# mimic lstm best-bleu: 0621_231122\n",
    "# iu h-coatt: 0623_120544|0623_110053\n",
    "# mimic h-coatt: 0623_192208\n",
    "CONTAINS = \\\n",
    "    ('iu-x-ray', # paper\n",
    "     # 0623_142422|0623_142452|\n",
    "     # lstm-att-v2 = 0612_035549\n",
    "     # ST | SAT: 1123_001440|1119_183609\n",
    "     # 1-nns (euclidean|cosine) 0612_160902|1210_212248\n",
    "     r'((tpl|most-similar-image).*cnn-1118-203841.*v4-1)|1123_001440|1119_183609|0623_202003|1103_133310|1103_133405|0612_160842|0612_160823|paper(?!_show|_coatt_re-impl)') \\\n",
    "    if IU else \\\n",
    "    ('mimic-cxr',\n",
    "     # Old tpl-chex-v1|tpl-m-chex-grouped: 0702_140740|0702_143050 | 1102_100501\n",
    "     # 1-nn: euclidean|cosine: 1103_111912|1210_212245\n",
    "     # v4-2 experiments: constant-mimic|words|sentences|1nn|rand|lstm-att|h-coatt|tpl-chex-v1|grouped\n",
    "     # lstm-att-v2 = 0702_183533\n",
    "     # SAT | ST = 1113_185718|1119_183153\n",
    "     # SAT | ST = 1201_150847|1202_161321\n",
    "     r'paper(?!_show|_coatt_re-impl)|1102_115221|1201_150847|1202_161321|0702_145200|1112_125550|1112_131626|1103_111912|1210_212245|0702_150811|0703_144847|1102_190559|1129_212630',\n",
    "    )\n",
    "# OLD MIMIC:\n",
    "# r'dummy-m|0617_144209|0623_103308|0625_184437|0612_233628|tpl-(chex-v1|m-chex-grouped-v6)-ordbest-v2.*cnn-0612-082139'\n",
    "# r'dummy|((tpl-(chex-v1-ord|m-chex-grouped-v6)|h-coatt|lstm-att-v2).*cnn-0612-082139)',\n",
    "\n",
    "res = filter_results(\n",
    "    # H-coatt models\n",
    "    # contains=('iu-x-ray', 'h-coatt.*v4-1|paper_coatt'), # 0623_202003 # .*mti\n",
    "    # 0623_120544 vs paper_coatt_re-impl-hrgr\n",
    "    # contains=('mimic-cxr', '_h-coatt.*v4-2|paper_coatt'),\n",
    "    \n",
    "    ### LSTM models (show and tell, show attend, etc)\n",
    "    # contains=('iu-x-ray', '_lstm-v2.*v4-1.*front|show-tell|boag-et-al-cnn'),\n",
    "    # contains=('iu-x-ray', '_lstm-att-v2.*v4-1.*front|s-att|show-attend-tell'),\n",
    "    # contains=('mimic-cxr', '_lstm-v2.*v4-2|show-tell|boag-et-al-cnn'),\n",
    "    # contains=('mimic-cxr', '_lstm-att-v2.*1101-115743.*v4-2|s-att|show-attend-tell'), # re-impl-liu-2021-et-al-CA\n",
    "    # contains=('iu-x-ray', 's-tell|show-tell|rtex'),\n",
    "    # contains=('iu-x-ray', 's-att-tell|show-attend-tell'),\n",
    "    # contains=('mimic-cxr', 's-tell|show-tell|liu-et-al|ratchet'),\n",
    "    # contains=('mimic-cxr', 's-att-tell|show-attend-tell|liu-et-al'),\n",
    "    \n",
    "    ### Beam experiments\n",
    "    # contains=('iu-x-ray', 's-att|s-tell'), # , 'ema'\n",
    "    # contains=('mimic-cxr_s-', 'lr0\\.0001'), # , '(?!ema)'\n",
    "    \n",
    "    ### Template sets (stress tests, ablations, etc) # new cnn: 1118_203841\n",
    "    # contains=('iu-x-ray', 'tpl.*cnn-1118-203841', 'ordbest-v2'),\n",
    "    # OLD TPL models: 0623_142422|0623_142452|gaming\n",
    "    # IU with new CNN: 1118_210509|1118_210821\n",
    "\n",
    "    # contains=('mimic-cxr', '1102_190559|1129_212630|gaming', 'ordbest-v2'),\n",
    "    # MIMIC old: 0702_140740|0702_143050\n",
    "    \n",
    "    ### Checkpointing by metric\n",
    "    # contains=('iu-x-ray', '0612_035549|0621_134437|0621_131422|0621_132927'),\n",
    "    \n",
    "    ### Stress test 3\n",
    "    # contains=('iu-x-ray.*v4-1', 'constant'),\n",
    "    #contains=('mimic-cxr.*v4-2', 'constant|chex-v6'),\n",
    "    contains='1102_190559|1102_115221|0702_160242|1104_134722|1102_205924',\n",
    "    # contains=('iu-x-ray.*v4-1', 'tpl'),\n",
    "    \n",
    "    \n",
    "    ## New papers 2022\n",
    "    # contains=('mimic-cxr', 'know|prog|kgae-supv'),\n",
    "    # contains=('iu-x-ray', 'kgae|know|prog'),\n",
    "    # contains=('iu-x-ray', 'paper'),\n",
    "    # contains=('mimic-cxr', 'paper'),\n",
    "    \n",
    "    # contains=('iu-x-ray', 'dummy-common-sentences', 'v4-1'),\n",
    "    # contains=('mimic-cxr', 'dummy-common-words', 'v4-2'),\n",
    "    \n",
    "    # contains=CONTAINS,\n",
    "    doesnt_contain=(\n",
    "        'paper',\n",
    "        # 'dummy-common', \n",
    "        'dummy-baseline', '_satt', '_ssent', '_COPY', 'tiny',\n",
    "        'boag-et-al-1nn', 'liu-et-al-ccr', 'tienet', 'rtmic',\n",
    "        'most-similar-image_0519-181554', 'cls-seg', 'noisy',\n",
    "        'vgg-19',\n",
    "        # 'constant-mimic',\n",
    "        're-impl',\n",
    "        # 'nguyen-et-al', # for now\n",
    "        'show-tell_re-impl-boag-et-al', # repeated from boag-et-al-cnn-rnn...\n",
    "        'miura-et-al-fcen', # This ablation is ignored\n",
    "        r'paper_boag-et-al-cnn-rnn(?!-beam)',\n",
    "    ),\n",
    "    dataset_type='test',\n",
    "    free=True,\n",
    "    metrics=ESSENTIAL_METRICS,\n",
    "    drop_key_cols=True,\n",
    "    timestamp_col=True,\n",
    "    # drop_na_rows=True,\n",
    "    rename_runs=rename_runs,\n",
    "    remove_timestamp=True,\n",
    "    best='lighter-chex_f1',\n",
    "    # best='ciderD',\n",
    "    beam_size=0,\n",
    "    # best_col=True, # beam_col=True,\n",
    "    # best='bleu4',\n",
    ").set_index('run_name')\n",
    "# res = res.sort_values(['run_name', 'best'], ascending=True) # 'beam'\n",
    "# res = res.sort_values('bleu4')\n",
    "res = res.sort_index()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main-table to latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold(s):\n",
    "    return '\\textbf{' + s + '}'\n",
    "\n",
    "shorten_cols = get_renamer([\n",
    "    ('-woNF', '-d'),\n",
    "    ('ciderD', 'C-D'),\n",
    "    (r'bleu(\\d)', r'B-\\1'),\n",
    "    ('bleu', 'B'),\n",
    "    ('rougeL', 'R-L'),\n",
    "    ('acc', 'Acc'),\n",
    "    ('prec', 'P'),\n",
    "    ('recall', 'R'),\n",
    "    ('f1', 'F-1'),\n",
    "    ('MIRQI-f', 'M-F-1'),\n",
    "    ('MIRQI-r', 'M-R'),\n",
    "    ('MIRQI-p', 'M-P'),\n",
    "])\n",
    "def latexify_cols(col):\n",
    "    return bold(shorten_cols(col))\n",
    "\n",
    "get_official_run_name = get_renamer([\n",
    "    # All trained models\n",
    "    ('_reports-v4-1', ''),\n",
    "    ('_reports-v4-2', ''),\n",
    "    (r'_(cnn-)?\\d{4}-\\d{6}', ''),\n",
    "    ('_densenet-121', ''),\n",
    "    # Dummy models\n",
    "    (r'most-similar-image.*dist-cos', '1-NN (cosine)'),\n",
    "    (r'most-similar-image', '1-NN (euclidean)'),\n",
    "    ('dummy-', ''),\n",
    "    ('common-', 'Top-'),\n",
    "    ('constant', 'Constant'),\n",
    "    ('random', 'Random retrieval'),\n",
    "    # DL models\n",
    "    ('lstm-att.*', 'CNN-LSTM-att'),\n",
    "    ('s-tell_.*', 'ST\\reimplemented{}, \\shortciteauthor{vinyals2015showtell}\\categoryLSTM{}'),\n",
    "    ('s-att-tell_.*', 'SAT\\reimplemented{}, \\shortciteauthor{xu2015showattendtell}\\categoryLSTM{}'),\n",
    "    # Template models\n",
    "    # ('ordbest-v2', ''),\n",
    "    ('tpl', 'Templ.'),\n",
    "    (r'-chex-v1-ordbest-v2.*', ' single'),\n",
    "    # (r'-chex-v1-noisy.*', ' top-char.'),\n",
    "    (r'-chex-v2-grouped-ordbest-v2', ' grouped'),\n",
    "    # (r'-m-chex-grouped-v6-ordbest-v2', ' grouped'), # DEPRECATED\n",
    "    (r'-m-chex-grouped-v8-ordbest-v2', ' grouped', ),\n",
    "    (r'-chex-v1-gaming-rm-neg-ordbest-v2', ' abn-only'),\n",
    "    (r'-chex-v1-gaming-dup-ordbest-v2', ' repeated'),\n",
    "    ('-ord\\w+', ''),\n",
    "    (r'h-coatt.*(__)?', r'CoAtt\\1\\reimplemented{}, \\shortciteauthor{jing2017automatic}\\categoryLSTM{}'),\n",
    "    # Papers\n",
    "    ('paper_', ''),\n",
    "    ('arl', 'ARL, \\shortciteauthor{hou2021arl}\\textsuperscript{L,RL}'),\n",
    "    ('rtex', 'RTEX, \\shortciteauthor{kougia2021rtex}\\categoryRetrieval{}'),\n",
    "    ('zhang-et-al-mirqi', '\\shortciteauthor{zhang2020graph}\\textsuperscript{L,f+i}'),\n",
    "    ('lovelace-et-al', '\\shortciteauthor{lovelace2020learning}\\categoryTransformer{}'),\n",
    "    ('liu-et-al-full', '\\shortciteauthor{liu2019clinically}\\textsuperscript{L,RL}'),\n",
    "    ('liu-2021-et-al-CA', '\\shortciteauthor{liu2021contrastive}\\textsuperscript{L,CA}'),\n",
    "    # ('boag-et-al-1nn', '1-nn \\shortciteauthor{boag2020baselines}'),\n",
    "    ('boag-et-al-cnn-rnn-beam', '\\shortciteauthor{boag2020baselines}\\categoryLSTM{}'),\n",
    "    ('chen-et-al', '\\shortciteauthor{chen2020memory}\\categoryTransformer{}'),\n",
    "    ('clara', 'CLARA, \\shortciteauthor{biswal2020clara}\\categoryRetrieval{}'),\n",
    "    ('coatt', 'CoAtt, \\shortciteauthor{jing2017automatic}\\textsuperscript{L,f+i}'),\n",
    "    ('ni-et-al', 'CVSE, \\shortciteauthor{ni2020cvse}\\textsuperscript{R,Ab}'),\n",
    "    ('hrgr', 'HRGR, \\shortciteauthor{li2018hybrid}\\categoryRetrieval{}'),\n",
    "    ('kerp', 'KERP, \\shortciteauthor{li2019knowledge}\\categoryRetrieval{}'),\n",
    "    ('syeda-et-al', '\\shortciteauthor{syeda2020chest}\\textsuperscript{R,f+i}'),\n",
    "    ('nguyen-et-al', '\\shortciteauthor{nguyen2021automated}\\categoryTransformer{}'),\n",
    "    ('nishino-et-al', '\\shortciteauthor{nishino2020reinforcement}\\textsuperscript{L,RL}'),\n",
    "    ('ratchet', 'RATCHET, \\shortciteauthor{hou2021ratchet}\\categoryTransformer{}'),\n",
    "    ('vti', '\\shortciteauthor{najdenkoska2021variational}\\textsuperscript{T,f+i}'),\n",
    "    ('kgae-supv', '\\cite{liu2021kgae}\\categoryTransformer{}'),\n",
    "    ('knowledge', '\\cite{yang2021knowledge}\\categoryTransformer{}'),\n",
    "    ('progressive', '\\cite{nooralahzadeh2021progressive}\\categoryTransformer{}'),\n",
    "    ('miura-et-al-fcen', 'DELETEME'),\n",
    "    ('miura-et-al-fce', '\\shortciteauthor{miura-etal-2021-improving}\\textsuperscript{T,RL}'),\n",
    "    # ('-mirqi', ''),\n",
    "    # (r'(\\w+)-et-al', r'\\1 et al.'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold_best_value_in_values(values):\n",
    "    formatter = lambda x: f'{x:.3f}'\n",
    "\n",
    "    values = np.nan_to_num(values, nan=-1)\n",
    "\n",
    "    # Get max_value\n",
    "    max_value = np.max(values)\n",
    "    max_value = formatter(max_value)\n",
    "\n",
    "    values_str = []\n",
    "    for value in values:\n",
    "        if value == -1:\n",
    "            value_s = '-'\n",
    "        else:\n",
    "            value_s = formatter(value)\n",
    "        if value_s == max_value:\n",
    "            value_s = bold(value_s)\n",
    "        values_str.append(value_s)\n",
    "        \n",
    "    return values_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold_best_value_by_column(df, apply=True):\n",
    "    # METRICS_RANGE_100 = set() # ('bleu', 'rougeL')\n",
    "    if not apply:\n",
    "        return df\n",
    "    \n",
    "    df2 = df.copy()\n",
    "    for col in df.columns:\n",
    "        column = df[col]\n",
    "        if column.dtypes == 'O':\n",
    "            # Skip \"object\" like columns (e.g. with strings)\n",
    "            continue\n",
    "\n",
    "        df2[col] = bold_best_value_in_values(column.values)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rotated_multirow_args():\n",
    "    dataset = 'IU X-ray' if IU else 'MIMIC-CXR'\n",
    "    return '{' + str(len(res)) + '}{' + dataset + '}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = res.drop(columns='timestamp') if 'timestamp' in res.columns else res\n",
    "table = bold_best_value_by_column(table, False).rename(\n",
    "    index=get_official_run_name,\n",
    "    columns=latexify_cols,\n",
    ")\n",
    "n_metrics = len(table.columns)\n",
    "table = table.reset_index().rename(columns={'run_name': bold('Model')}).to_latex(\n",
    "    float_format='%.3f',\n",
    "    column_format='l' + 'c' * n_metrics,\n",
    "    na_rep='-',\n",
    "    index=False,\n",
    "    escape=False,\n",
    "    # bold_rows=True,\n",
    ")\n",
    "table = re.sub(r' +', ' ', table, flags=re.M)\n",
    "# Add this additional column for the dataset (IU or MIMIC)\n",
    "# table = re.sub(r'^ +', '& ', table, flags=re.M)\n",
    "# table = re.sub(\n",
    "#     r'^\\& (CLARA|Templ\\. simple|Boag)',\n",
    "#     r'\\cline{2-11}\\n& \\1', table, flags=re.M,\n",
    "# )\n",
    "# table = re.sub(\n",
    "#     r'^\\\\midrule',\n",
    "#     r'\\\\midrule\\n\\\\rotatedMultirow' + _rotated_multirow_args(),\n",
    "#     table, flags=re.M,\n",
    "# )\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chexpert by disease table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold_best_value_by_row(df):\n",
    "    df2 = df.copy()\n",
    "    for row in df.index:\n",
    "        values = df.loc[row].values\n",
    "        \n",
    "        df2.loc[row] = bold_best_value_in_values(values)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'f1'\n",
    "metrics = [c for c in CHEXPERT_DISEASE_METRICS if base in c] + [base]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_runs_2 = [\n",
    "    ('iu-x-ray_', ''),\n",
    "    ('mimic-cxr_', ''),\n",
    "    ('_front', ''),\n",
    "    ('tpl-chex-v1-grouped-ordbest_cnn-0611-155356_densenet-121-v2', 'densenet-121 + templates'),\n",
    "    ('dummy-', ''),\n",
    "    (r'_(precnn-)?\\d{4}-\\d{6}', ''),\n",
    "    (r'_lr(-emb)?[\\d\\.]+', ''),\n",
    "    (r'__\\w+', ''),\n",
    "    ('-v2', ''),\n",
    "    ('_cnn-freeze', ''),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSPOSE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ORDER = [\n",
    "    'most-similar',\n",
    "    's-att-tell',\n",
    "    'paper_boag',\n",
    "    'paper_lovelace',\n",
    "    'paper_ratchet',\n",
    "    'paper_miura',\n",
    "    'paper_ni',\n",
    "    'tpl',\n",
    "]\n",
    "def _order_runs(index):\n",
    "    def _get_order(run):\n",
    "        for i, o in enumerate(_ORDER):\n",
    "            if run.startswith(o):\n",
    "                return i\n",
    "        raise Exception(f'{run} not considered in order!')\n",
    "    return pd.Index([_get_order(run) for run in index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filter_results(\n",
    "    # lstm-att: 0612_233628\n",
    "    # 1-nn old-cnn: 0612_215504 , euclidean: 1103_111912, cosine: 1210_212245\n",
    "    contains=('mimic-cxr', r'1210_212245|1102_190559|1113_185718|paper_(boag-et-al-cnn-rnn-beam|lovelace|ni\\-et|ratchet|miura)'),\n",
    "    # contains=('1102_190559|1129_191853'),\n",
    "    # contains=('chex-v5'),\n",
    "    # old: 0612_215709 \n",
    "    # contains=('iu-x-ray', r'dummy|tpl|__base|paper'),\n",
    "    # contains=('iu-x-ray', r'0611_182321|0612_012900'), # 0612_012741\n",
    "    # dummy-most|dummy-random|__freeze\n",
    "    doesnt_contain=('dummy-baseline', '_satt', '_ssent', '_COPY', 'tiny', 'miura-et-al-fcen'),\n",
    "    dataset_type='test',\n",
    "    free=True,\n",
    "    metrics=metrics,\n",
    "    rename_runs=rename_runs_2,\n",
    "    drop_key_cols=True,\n",
    "    # timestamp_col=True,\n",
    "    # drop_na_rows=True,\n",
    "    best='lighter-chex_f1',\n",
    "    beam_size=0,\n",
    "    # best_col=True, beam_col=True,\n",
    "    remove_timestamp=True,\n",
    ").set_index('run_name').sort_index(key=_order_runs)\n",
    "if TRANSPOSE:\n",
    "    df = df.transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../datasets/common/constants.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRANSPOSE:\n",
    "    d = df.rename(index={base: f'{base}-macro'}).rename(\n",
    "        columns=lambda x: '\\tableDiseaseColname{%s}' % get_official_run_name(x),\n",
    "        index=get_renamer([\n",
    "            (r'{}-macro'.format(base), 'Macro average'),\n",
    "            (r'{}-(\\w+)'.format(base), r'\\1'),\n",
    "        ])\n",
    "    )\n",
    "    d = bold_best_value_by_row(d)\n",
    "    n_models = len(d.columns)\n",
    "    d.columns.rename(bold(f'{base.capitalize()}-scores'), inplace=True)\n",
    "    table = d.to_latex(\n",
    "        float_format='%.3f',\n",
    "        column_format='l' + 'c' * n_models,\n",
    "        # na_rep='-',\n",
    "        # index=False,\n",
    "        escape=False,\n",
    "    )\n",
    "else:\n",
    "    d = df.rename(columns={base: f'Macro'}).rename(\n",
    "        columns=lambda x: '\\tableDiseaseColname{%s}' % ABN_SHORTCUTS.get(x.strip('f1-'), x),\n",
    "        index=get_official_run_name,\n",
    "    )\n",
    "    d = bold_best_value_by_column(d)\n",
    "    table = d.to_latex(\n",
    "        float_format='%.3f',\n",
    "        column_format='l' + 'c' * len(d.columns),\n",
    "        # na_rep='-',\n",
    "        # index=False,\n",
    "        escape=False,\n",
    "    )\n",
    "table = re.sub(r' +', ' ', table, flags=re.M)\n",
    "table = re.sub(\n",
    "    r'^(Macro)',\n",
    "    r'\\midrule\\n\\1', table, flags=re.M,\n",
    ")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_strs = [\n",
    "    (r'_precnn-\\d{4}-\\d{6}', ''),\n",
    "    (r'_lr[\\d\\.]+', ''),\n",
    "    (r'_lr-emb[\\d\\.]+', ''),\n",
    "    ('_size256', ''),\n",
    "    ('-v2', ''),\n",
    "    ('_front', ''),\n",
    "    (r'__[\\w\\-]*', ''),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'run_name',\n",
    "    'time_per_epoch', 'total_time',\n",
    "    'current_epoch', 'final_epoch',\n",
    "    'batch_size', 'device', 'visible',\n",
    "]\n",
    "res = filter_training_stats(\n",
    "    contains='__base',\n",
    "    columns=cols,\n",
    "    rename_runs=replace_strs,\n",
    ")\n",
    "res = res.replace(r'^\\d{4}_\\d{6}_(.*)', r'\\1', regex=True)\n",
    "res = res.set_index('run_name').rename(index=rename_runs)\n",
    "res.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Compare runtime chexpert vs holistic chexpert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def subtract_cols(df, cols_a, cols_b, drop_na_rows=True):\n",
    "    array_a = df[cols_a].to_numpy()\n",
    "    array_b = df[cols_b].to_numpy()\n",
    "    \n",
    "    df_2 = df[KEY_COLS].copy()\n",
    "    df_2 = pd.concat([df_2, pd.DataFrame(array_a - array_b, columns=cols_a)], axis=1)\n",
    "    \n",
    "    if drop_na_rows:\n",
    "        df_2.dropna(axis=0, inplace=True, how='any')\n",
    "    \n",
    "    return df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "metric = 'f1'\n",
    "\n",
    "runtime_chexpert = [c for c in RESULTS_DF.columns if c.startswith(f'chex_{metric}')]\n",
    "holistic_chexpert = [c for c in RESULTS_DF.columns if c.startswith(metric)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = RESULTS_DF\n",
    "df = df.loc[~df['run_name'].str.contains('dummy')]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "set(df['run_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = subtract_cols(df, runtime_chexpert, holistic_chexpert)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "run_name = '0112_154506_lstm-v2_lr0.001_densenet-121-v2_noes'\n",
    "debug = False\n",
    "d1 = load_rg_outputs(run_name, debug=debug, free=True)\n",
    "d2 = load_rg_outputs(run_name, debug=debug, free=False)\n",
    "len(d1), len(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c1 = Counter(d1['filename'])\n",
    "c2 = Counter(d2['filename'])\n",
    "len(c1), len(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for fname in c1.keys():\n",
    "    v1 = c1[fname]\n",
    "    v2 = c2[fname]\n",
    "    if v1 != v2:\n",
    "        print('Wrong: ', fname, v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "set(d2['dataset_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Pretty-print (latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "replace_strs = [\n",
    "    (r'^\\d{4}_\\d{6}_', ''),\n",
    "    ('most-similar-image', '1nn'),\n",
    "    ('_lr[\\d\\.]+', ''),\n",
    "    ('_size256', ''),\n",
    "    (r'_\\d{4}_\\d{6}_.*', ''),\n",
    "    ('dummy-', ''),\n",
    "    ('common', 'top'),\n",
    "    ('-v2', ''),\n",
    "    (r'top-(\\w)\\w+-(\\d+)', r'top-\\1-\\2'),\n",
    "    ('_densenet-121', ''),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "columns = ['bleu', 'rougeL', 'ciderD'] + CHEXPERT_METRICS + MIRQI_METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = filter_results(dataset_type='test',\n",
    "                    free=True,\n",
    "                    metrics=columns,\n",
    "                    contains='(?=_lstm-att-v2.*densenet|_lstm-v2.*densenet|dummy)',\n",
    "                    drop='0915_173951|0915_174222|0916_104739',\n",
    "                    drop_na_rows=True,\n",
    "                    rename_runs=replace_strs,\n",
    "                   )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "shorten_cols = lambda s: s.replace('MIRQI-v2', 'v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(df.set_index('run_name').rename(\n",
    "    index=rename_runs,\n",
    "    columns=shorten_cols,\n",
    ").sort_index().to_latex(\n",
    "    columns=[shorten_cols(c) for c in columns],\n",
    "    float_format='%.3f',\n",
    "    column_format='l' + 'c' * len(columns),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contains = 'covid-x'\n",
    "# contains = 'cxr14'\n",
    "# contains = 'e0'\n",
    "# contains = '0717_120222_covid-x_densenet-121_lr1e-06_os_aug-covid'\n",
    "# contains = '0717_101812_covid-x_densenet-121_lr1e-06_os-max2_aug-covid'\n",
    "# run_name = '0717_120222_covid-x_densenet-121_lr1e-06_os_aug-covid' # WINNER\n",
    "\n",
    "# contains = '0717_101812_covid-x_densenet-121_lr1e-06_os-max2_aug-covid'\n",
    "# contains = 'covid-uc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    'roc_auc', 'pr_auc', # 'hamming', #\n",
    "]\n",
    "# metrics = [\n",
    "#     'acc', 'roc_auc', 'prec', 'recall', 'roc_auc_Cardiomegaly', 'roc_auc_Pneumonia',\n",
    "#     'recall_Cardiomegaly', 'recall_Pneumonia',\n",
    "#     'iobb-masks', 'iobb-masks-Cardiomegaly', 'iobb-masks-Pneumonia',\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_strs = [\n",
    "    # (r'^\\d{4}_\\d{6}_', ''),\n",
    "    # (r'_precnn-\\d{4}-\\d{6}', ''),\n",
    "    (r'_lr[e\\-\\d\\.]+', ''),\n",
    "    # (r'(cxr14|chexpert|iu-x-ray)_', ''),\n",
    "    ('_size256', ''),\n",
    "    (r'_cl-wbce_seg-w', ''),\n",
    "    (r'_seg-unw', ''),\n",
    "    # (r'_aug\\d-(touch|double|single)', ''),\n",
    "    ('_shuffle', ''),\n",
    "    ('_sch-(roc|pr)[\\-_]auc-p\\d-f0.5(-c\\d)?', ''),\n",
    "    ('_best-(roc|pr)[\\-_]auc', ''),\n",
    "    ('_norm[SD]', ''),\n",
    "    ('_labels13', ''),\n",
    "    # ('_front', ''),\n",
    "    # (r'__[\\w\\-]*', ''),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTAINS = r'cxr14.*(?:small|tiny)|0402_062551'\n",
    "CONTAINS = 'cxr14_densenet-121'\n",
    "# CONTAINS = 'chexpert_densenet-121'\n",
    "# CONTAINS = r'chexpert' # .*(?:small|tiny)\n",
    "# CONTAINS = r'iu-x-ray.*(?:tiny)|0420_175514'\n",
    "# CONTAINS = r'iu-x-ray.*(?:small)|0420_175514'\n",
    "# CONTAINS = r'iu-x-ray_densenet-121' # 0420_175514\n",
    "\n",
    "DATASET_TYPE = 'val' if 'chex' in CONTAINS else 'test'\n",
    "\n",
    "d = filter_results(\n",
    "    contains=CONTAINS,\n",
    "    doesnt_contain=['hint', 'balance', 'Cardiomeg', 'Pneumonia'],\n",
    "    dataset_type=DATASET_TYPE,\n",
    "    metrics=metrics,\n",
    "    drop_key_cols=True,\n",
    "    # rename_runs=replace_strs,\n",
    ").sort_values('pr_auc', ascending=False)\n",
    "d.set_index('run_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta = load_metadata(RunId('0406_230221', False, 'cls'))\n",
    "meta['hparams']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check training stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_strs = [\n",
    "    (r'_lr[e\\-\\d\\.]+', ''),\n",
    "    # (r'(cxr14|chexpert|iu-x-ray)_', ''),\n",
    "    (r'_pre\\d{4}-\\d{6}', ''),\n",
    "    ('_size256', ''),\n",
    "    (r'_cl-wbce_seg-w', ''),\n",
    "    (r'_seg-unw', ''),\n",
    "    (r'_aug\\d-(touch|double|single)', ''),\n",
    "    ('_shuffle', ''),\n",
    "    ('_sch-(roc|pr)[\\-_]auc-p\\d-f0.5(-c\\d)?', ''),\n",
    "    ('_best-(roc|pr)[\\-_]auc', ''),\n",
    "    ('_norm[SD]', ''),\n",
    "    ('_labels13', ''),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'run_name',\n",
    "    'time_per_epoch', 'total_time',\n",
    "    'current_epoch', 'initial_epoch', 'final_epoch',\n",
    "    'batch_size', 'visible',\n",
    "]\n",
    "res = filter_training_stats(\n",
    "    contains=r'cxr14|chexpert',\n",
    "    columns=cols,\n",
    "    rename_runs=replace_strs,\n",
    ")\n",
    "# res = res.replace(r'^\\d{4}_\\d{6}_(.*)', r'\\1', regex=True)\n",
    "res = res.set_index('run_name').rename(index=rename_runs)\n",
    "res.sort_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['figure.facecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medai.datasets.iu_xray import DATASET_DIR as IU_DIR\n",
    "from medai.datasets.mimic_cxr import DATASET_DIR as MIMIC_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_DATASET = 'iu' # 'mimic_cxr' # 'iu'\n",
    "dataset_dir = MIMIC_DIR if 'mimic' in USE_DATASET else IU_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load vocab and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../datasets/common/constants.py\n",
    "%run ../datasets/vocab/__init__.py\n",
    "%run ../utils/nlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = load_vocab(os.path.join(dataset_dir, 'reports'), 'v4')\n",
    "REPORT_READER = ReportReader(VOCAB)\n",
    "len(VOCAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load holistic chexpert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fpath = os.path.join(dataset_dir, 'reports', 'reports_with_chexpert_labels.csv')\n",
    "df = pd.read_csv(fpath, index_col=0)\n",
    "df.replace(-1, 1, inplace=True)\n",
    "df.replace(-2, 0, inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "REPORTS_LIST = [\n",
    "    REPORT_READER.text_to_idx(report)\n",
    "    for report in df['Reports']\n",
    "]\n",
    "len(REPORTS_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def add_suffix(col):\n",
    "    if col in CHEXPERT_LABELS:\n",
    "        return f'{col}-gt'\n",
    "    return col\n",
    "df.rename(\n",
    "    columns=add_suffix,\n",
    "    inplace=True,\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run ../metrics/report_generation/chexpert.py\n",
    "%run -n ../eval_report_generation_chexpert_labeler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _compute_metrics_vs_holistic(labels):\n",
    "    columns = labels_with_suffix('gen')\n",
    "    \n",
    "    assert len(labels) == len(df)\n",
    "    \n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.cpu().numpy()\n",
    "    full_df = pd.concat([\n",
    "        df,\n",
    "        pd.DataFrame(labels, index=df.index, columns=columns),\n",
    "    ], axis=1)\n",
    "    \n",
    "    assert len(full_df) == len(labels)\n",
    "    \n",
    "    return full_df, _calculate_metrics(full_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Compare light vs holistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Calculate light-labeler chexpert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run ../metrics/report_generation/labeler_correctness/light_labeler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "labeler = ChexpertLightLabeler(VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "labels = labeler(REPORTS_LIST)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "labels[labels == -2] = 0\n",
    "labels[labels == -1] = 1\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "acc, precision, recall, f1, roc_auc, pr_auc = _compute_metrics_vs_holistic(labels)\n",
    "acc, precision, recall, f1, roc_auc, pr_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Calculate with full-labeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run ../metrics/report_generation/labeler_correctness/full_labeler.py\n",
    "%run ../utils/nlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "labeler = ChexpertFullLabeler(VOCAB)\n",
    "labeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "labels = labeler(REPORTS_LIST)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "labels[labels == -2] = 0\n",
    "labels[labels == -1] = 1\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "acc, precision, recall, f1, roc_auc, pr_auc = _compute_metrics_vs_holistic(labels)\n",
    "acc, precision, recall, f1, roc_auc, pr_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare lighter vs holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../metrics/report_generation/abn_match/chexpert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeler = ChexpertLighterLabeler(VOCAB, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "labels = labeler(REPORTS_LIST)\n",
    "labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if labels.size(1) == 13:\n",
    "    nf_column = torch.zeros(labels.size(0), device=labels.device).unsqueeze(-1)\n",
    "    labels = torch.cat((nf_column, labels), dim=1)\n",
    "labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df, (acc, precision, recall, f1, roc_auc, pr_auc) = _compute_metrics_vs_holistic(labels)\n",
    "len(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision[1:], precision[1:].mean(), recall[1:], recall[1:].mean(), f1[1:], f1[1:].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeler.diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = labeler.diseases[0]\n",
    "colgt = f'{target}-gt'\n",
    "colgen = f'{target}-gen'\n",
    "d = full_df\n",
    "d = d[((d[colgt] == 1) & (d[colgen] == 0))]\n",
    "d = d[['Reports', colgt, colgen]]\n",
    "print(len(d))\n",
    "d.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(d['Reports'])\n",
    "l[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Lighter labeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../metrics/report_generation/abn_match/chexpert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ChexpertLighterLabeler(vocab)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reports = [\n",
    "#     'heart is enlarged .',\n",
    "#     'heart is not enlarged .',\n",
    "#     'heart is upper limit',\n",
    "# ]\n",
    "sample_sentences = sentences[-50:-45]\n",
    "reports_as_one = list([' '.join(sample_sentences)])\n",
    "reports_as_many = list(sample_sentences)\n",
    "for r in reports_as_many:\n",
    "    print(r)\n",
    "reports_as_one = [reader.text_to_idx(r) for r in reports_as_one]\n",
    "reports_as_many = [reader.text_to_idx(r) for r in reports_as_many]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l(reports_as_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l(reports_as_many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = reader.text_to_idx('heart size is large , no pneumothorax')\n",
    "res = l([report]).tolist()[0]\n",
    "list(zip(res, l.diseases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All reports/sentences\n",
    "\n",
    "TODO: merge with the above code??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_REPORTS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FULL_REPORTS:\n",
    "    name = 'reports_with_chexpert_labels.csv'\n",
    "    TARGET_COL = 'Reports'\n",
    "else:\n",
    "    name = 'sentences_with_chexpert_labels.csv'\n",
    "    TARGET_COL = 'sentence'\n",
    "fpath = os.path.join(IU_DIR, 'reports', name)\n",
    "df.replace(-2, 0, inplace=True)\n",
    "df = pd.read_csv(fpath, index_col=0 if FULL_REPORTS else None)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [reader.text_to_idx(s) for s in df[TARGET_COL]]\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../metrics/report_generation/labeler_correctness/lighter_labeler/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ChexpertLighterLabeler(vocab, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "labels = l(texts)\n",
    "labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [f'lighter-{d}' for d in l.diseases]\n",
    "df[cols] = labels.cpu().numpy()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_labels = df[l.diseases].to_numpy().astype(np.int8)\n",
    "gt_labels = torch.tensor(gt_labels, device='cpu')\n",
    "gt_labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = ((labels == 1) & (gt_labels == 1)).sum(0)\n",
    "fp = ((labels == 1) & (gt_labels == 0)).sum(0)\n",
    "tn = ((labels == 0) & (gt_labels == 0)).sum(0)\n",
    "fn = ((labels == 0) & (gt_labels == 1)).sum(0)\n",
    "tp.size(), fp.size(), tn.size(), fn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease = l.diseases[0]\n",
    "print(disease)\n",
    "lighter_disease = f'lighter-{disease}'\n",
    "show_cols = [TARGET_COL, 'No Finding', disease, lighter_disease]\n",
    "d = df\n",
    "d = d.loc[((d[disease] == 1) & (d[lighter_disease] == 0))]\n",
    "d = d[show_cols].sort_values(TARGET_COL, key=lambda x: x.str.len())\n",
    "print(len(d))\n",
    "d.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(d[TARGET_COL].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug medical-labeler\n",
    "\n",
    "With both torch and numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPLabeler:\n",
    "    use_numpy = True\n",
    "    def __call__(self, reports):\n",
    "        return np.zeros((len(reports), 14))\n",
    "    \n",
    "class TorchLabeler:\n",
    "    use_numpy = False\n",
    "    def __call__(self, reports):\n",
    "        return torch.zeros(len(reports), 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TorchLabeler().__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_gt = [\n",
    "    [1, 2, 3, 4],\n",
    "    [1, 2, 3, 5, 6, 7, 4],\n",
    "]\n",
    "reports_gen = [\n",
    "    [1, 2, 3, 4, 6],\n",
    "    [1, 2],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../metrics/report_generation/labeler_correctness/metric.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = MedicalLabelerCorrectness(NPLabeler(), device='cuda')\n",
    "l.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.update((reports_gen, reports_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = l.compute()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['acc'].mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['acc'][3].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

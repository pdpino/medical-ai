{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save paper results\n",
    "\n",
    "Save baseline/paper results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../datasets/common/constants.py\n",
    "%run ../utils/__init__.py\n",
    "%run ../utils/files.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_metrics(folder, filename, results_dict):\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(results_dict, f, indent=2)\n",
    "    print(f'Saved dict to {filepath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mirqi_metrics(folder, results):\n",
    "    _save_metrics(folder, 'mirqi-metrics-free.json', results)\n",
    "\n",
    "def save_chexpert_metrics(folder, results):\n",
    "    _save_metrics(folder, 'chexpert-metrics-free.json', results)\n",
    "\n",
    "def save_runtime_metrics(folder, results):\n",
    "    _save_metrics(folder, 'metrics-free.json', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_folder(dataset, paper):\n",
    "    assert dataset in ('iu-x-ray', 'mimic-cxr')\n",
    "    run_name = f'{dataset}_paper_{paper}'\n",
    "    folder = get_results_folder(RunId(run_name, False, 'rg'), save_mode=True)\n",
    "    return folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_woNF(metrics, prefixes, diseases=CHEXPERT_DISEASES, verbose=False):\n",
    "    if isinstance(prefixes, str):\n",
    "        prefixes = (prefixes,)\n",
    "\n",
    "    macro_avg_woNF = {}\n",
    "\n",
    "    for prefix in prefixes:\n",
    "        keys = [\n",
    "            f'{prefix}-{disease}'\n",
    "            for disease in diseases\n",
    "            if disease.lower() != 'no finding'\n",
    "        ]\n",
    "        macro_avg = np.mean([metrics[k] for k in keys])\n",
    "            \n",
    "        if verbose:\n",
    "            print(f'Prefix={prefix}, averaging: {keys}')\n",
    "        macro_avg_woNF[f'{prefix}-woNF'] = macro_avg\n",
    "    return macro_avg_woNF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper MIRQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = get_paper_folder('iu-x-ray', 'zhang-et-al-mirqi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu1, bleu2, bleu3, bleu4 = 0.441, 0.291, 0.203, 0.147\n",
    "runtime_results = {\n",
    "    'test': {\n",
    "        'bleu1': bleu1, 'bleu2': bleu2, 'bleu3': bleu3, 'bleu4': bleu4,\n",
    "        'bleu': np.mean([bleu1, bleu2, bleu3, bleu4]),\n",
    "        'ciderD': 0.304,\n",
    "        'rougeL': 0.367,\n",
    "    }\n",
    "}\n",
    "mirqi_results = {\n",
    "    'test': {\n",
    "        'MIRQI-r': 0.483,\n",
    "        'MIRQI-p': 0.490,\n",
    "        'MIRQI-f': 0.478,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_mirqi_metrics(folder, mirqi_results)\n",
    "save_runtime_metrics(folder, runtime_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lovelace et al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = get_paper_folder('mimic-cxr', 'lovelace-et-al')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using their transformer w/fine-tuning ablation\n",
    "bleu1, bleu2, bleu3, bleu4 = 0.415, 0.272, 0.193, 0.146\n",
    "runtime_results = {\n",
    "    'test': {\n",
    "        'bleu1': bleu1, 'bleu2': bleu2, 'bleu3': bleu3, 'bleu4': bleu4,\n",
    "        'bleu': np.mean([bleu1, bleu2, bleu3, bleu4]),\n",
    "        'ciderD': 0.316, # not sure if Cider-D or Cider\n",
    "        'rougeL': 0.318,\n",
    "    }\n",
    "}\n",
    "_values = {\n",
    "    'f1': 22.8,\n",
    "    'prec': 33.3,\n",
    "    'recall': 21.7,\n",
    "\n",
    "    'f1-Atelectasis': 32.2,\n",
    "    'f1-Cardiomegaly': 43.3,\n",
    "    'f1-Consolidation': 7.3,\n",
    "    'f1-Edema': 29.8,\n",
    "    'f1-Enlarged Cardiomediastinum': 5.9,\n",
    "    'f1-Fracture': 0,\n",
    "    'f1-Lung Lesion': 1.4,\n",
    "    'f1-Lung Opacity': 17.1,\n",
    "    'f1-No Finding': 54.1,\n",
    "    'f1-Pleural Effusion': 48.0,\n",
    "    'f1-Pleural Other': 0.9,\n",
    "    'f1-Pneumonia': 3.9,\n",
    "    'f1-Pneumothorax': 9.8,\n",
    "    'f1-Support Devices': 66.0,\n",
    "\n",
    "    'prec-Atelectasis': 43.0,\n",
    "    'prec-Cardiomegaly': 46.9,\n",
    "    'prec-Consolidation': 15.7,\n",
    "    'prec-Edema': 37.6,\n",
    "    'prec-Enlarged Cardiomediastinum': 12.3,\n",
    "    'prec-Fracture': 0,\n",
    "    'prec-Lung Lesion': 23.8,\n",
    "    'prec-Lung Opacity': 64.0,\n",
    "    'prec-No Finding': 39.0,\n",
    "    'prec-Pleural Effusion': 71.2,\n",
    "    'prec-Pleural Other': 16.1,\n",
    "    'prec-Pneumonia': 7,\n",
    "    'prec-Pneumothorax': 12.9,\n",
    "    'prec-Support Devices': 77.0,\n",
    "\n",
    "    'recall-Atelectasis': 25.8,\n",
    "    'recall-Cardiomegaly': 40.2,\n",
    "    'recall-Consolidation': 4.8,\n",
    "    'recall-Edema': 24.6,\n",
    "    'recall-Enlarged Cardiomediastinum': 3.9,\n",
    "    'recall-Fracture': 0,\n",
    "    'recall-Lung Lesion': 0.7,\n",
    "    'recall-Lung Opacity': 9.9,\n",
    "    'recall-No Finding': 88.2,\n",
    "    'recall-Pleural Effusion': 36.2,\n",
    "    'recall-Pleural Other': 0.5,\n",
    "    'recall-Pneumonia': 2.7,\n",
    "    'recall-Pneumothorax': 7.8,\n",
    "    'recall-Support Devices': 57.8,\n",
    "}\n",
    "woNF = calculate_avg_woNF(_values, ['f1', 'recall', 'prec'])\n",
    "_values.update(woNF)\n",
    "chexpert_results = {\n",
    "    'test': {\n",
    "        k: value / 100\n",
    "        for k, value in _values.items()\n",
    "    },\n",
    "}\n",
    "woNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chexpert_metrics(folder, chexpert_results)\n",
    "save_runtime_metrics(folder, runtime_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boag et al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = get_paper_folder('mimic-cxr', 'boag-et-al-1nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using their 1-NN model\n",
    "bleu1, bleu2, bleu3, bleu4 = 0.305, 0.171, 0.098, 0.057\n",
    "runtime_results = {\n",
    "    'test': {\n",
    "        'bleu1': bleu1, 'bleu2': bleu2, 'bleu3': bleu3, 'bleu4': bleu4,\n",
    "        'bleu': np.mean([bleu1, bleu2, bleu3, bleu4]),\n",
    "        'ciderD': 0.755, # not sure if Cider-D or Cider\n",
    "    }\n",
    "}\n",
    "_values = {\n",
    "    'acc': 0.818,\n",
    "    'prec': 0.253,\n",
    "    'f1': 0.258,\n",
    "\n",
    "    'f1-Support Devices': 0.527,\n",
    "    'f1-Lung Opacity': 0.417,\n",
    "    'f1-Cardiomegaly': 0.445,\n",
    "    'f1-Atelectasis': 0.375,\n",
    "    'f1-No Finding': 0.455,\n",
    "    'f1-Pleural Effusion': 0.532,\n",
    "    'f1-Edema': 0.286,\n",
    "    'f1-Enlarged Cardiomediastinum': 0.142,\n",
    "    'f1-Pneumonia': 0.08,\n",
    "    'f1-Pneumothorax': 0.111,\n",
    "    'f1-Fracture': 0.060,\n",
    "    'f1-Lung Lesion': 0.062,\n",
    "    'f1-Consolidation': 0.085,\n",
    "    'f1-Pleural Other': 0.039,\n",
    "}\n",
    "woNF = calculate_avg_woNF(_values, 'f1')\n",
    "_values.update(woNF)\n",
    "chexpert_results = {\n",
    "    'test': _values,\n",
    "}\n",
    "woNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chexpert_metrics(folder, chexpert_results)\n",
    "save_runtime_metrics(folder, runtime_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cnn-rnn-beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = get_paper_folder('mimic-cxr', 'boag-et-al-cnn-rnn-beam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using their CNN-RNN-beam\n",
    "bleu1, bleu2, bleu3, bleu4 = 0.305, 0.201, 0.137, 0.092\n",
    "runtime_results = {\n",
    "    'test': {\n",
    "        'bleu1': bleu1, 'bleu2': bleu2, 'bleu3': bleu3, 'bleu4': bleu4,\n",
    "        'bleu': np.mean([bleu1, bleu2, bleu3, bleu4]),\n",
    "        'ciderD': 0.850, # not sure if Cider-D or Cider\n",
    "    }\n",
    "}\n",
    "_values = {\n",
    "    'acc': 0.837, \n",
    "    'prec': 0.304,\n",
    "    'f1': 0.186,\n",
    "\n",
    "    'f1-Support Devices': 0.613,\n",
    "    'f1-Lung Opacity': 0.077,\n",
    "    'f1-Cardiomegaly': 0.390,\n",
    "    'f1-Atelectasis': 0.146,\n",
    "    'f1-No Finding': 0.407,\n",
    "    'f1-Pleural Effusion': 0.473,\n",
    "    'f1-Edema': 0.271,\n",
    "    'f1-Enlarged Cardiomediastinum': 0.134,\n",
    "    'f1-Pneumonia': 0.03,\n",
    "    'f1-Pneumothorax': 0.043,\n",
    "    'f1-Fracture': 0.001,\n",
    "    'f1-Lung Lesion': 0.001, # less than that\n",
    "    'f1-Consolidation': 0.014,\n",
    "    'f1-Pleural Other': 0.001, # less than that\n",
    "}\n",
    "woNF = calculate_avg_woNF(_values, ['f1'], CHEXPERT_DISEASES)\n",
    "_values.update(woNF)\n",
    "chexpert_results = {\n",
    "    'test': _values,\n",
    "}\n",
    "woNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chexpert_metrics(folder, chexpert_results)\n",
    "save_runtime_metrics(folder, runtime_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liu et al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = get_paper_folder('mimic-cxr', 'liu-et-al-ccr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using their CCR ablation\n",
    "bleu1, bleu2, bleu3, bleu4 = 0.294, 0.190, 0.134, 0.094\n",
    "runtime_results = {\n",
    "    'test': {\n",
    "        'bleu1': bleu1, 'bleu2': bleu2, 'bleu3': bleu3, 'bleu4': bleu4,\n",
    "        'bleu': np.mean([bleu1, bleu2, bleu3, bleu4]),\n",
    "        'ciderD': 0.956,\n",
    "        'rougeL': 0.284,\n",
    "    }\n",
    "}\n",
    "_values = {\n",
    "    'acc': 0.868,\n",
    "    'prec': 0.313,\n",
    "    'recall': 0.126,\n",
    "\n",
    "    'prec-No Finding': 0.491,\n",
    "    'prec-Enlarged Cardiomediastinum': 0.202,\n",
    "    'prec-Cardiomegaly': 0.678,\n",
    "    'prec-Lung Lesion': 0,\n",
    "    'prec-Lung Opacity': 0.640,\n",
    "    'prec-Edema': 0.280,\n",
    "    'prec-Consolidation': 0.037,\n",
    "    'prec-Pneumonia': 0,\n",
    "    'prec-Atelectasis': 0.476,\n",
    "    'prec-Pneumothorax': 0.039,\n",
    "    'prec-Pleural Effusion': 0.683,\n",
    "    'prec-Pleural Other': 0,\n",
    "    'prec-Fracture': 0,\n",
    "    'prec-Support Devices': 0.849,\n",
    "}\n",
    "woNF = calculate_avg_woNF(_values, ['prec'], CHEXPERT_DISEASES, verbose=False)\n",
    "_values.update(woNF)\n",
    "chexpert_results = {\n",
    "    'test': _values,\n",
    "}\n",
    "woNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chexpert_metrics(folder, chexpert_results)\n",
    "save_runtime_metrics(folder, runtime_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = get_paper_folder('mimic-cxr', 'liu-et-al-full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using their CCR ablation\n",
    "bleu1, bleu2, bleu3, bleu4 = 0.313, 0.206, 0.146, 0.103\n",
    "runtime_results = {\n",
    "    'test': {\n",
    "        'bleu1': bleu1, 'bleu2': bleu2, 'bleu3': bleu3, 'bleu4': bleu4,\n",
    "        'bleu': np.mean([bleu1, bleu2, bleu3, bleu4]),\n",
    "        'ciderD': 1.046,\n",
    "        'rougeL': 0.306,\n",
    "    }\n",
    "}\n",
    "_values = {\n",
    "    'acc': 0.867,\n",
    "    'prec': 0.309,\n",
    "    'recall': 0.134,\n",
    "\n",
    "    'prec-No Finding': 0.405,\n",
    "    'prec-Enlarged Cardiomediastinum': 0.167,\n",
    "    'prec-Cardiomegaly': 0.704,\n",
    "    'prec-Lung Lesion': 0,\n",
    "    'prec-Lung Opacity': 0.460,\n",
    "    'prec-Edema': 0,\n",
    "    'prec-Consolidation': 0,\n",
    "    'prec-Pneumonia': 0.4,\n",
    "    'prec-Atelectasis': 0.521,\n",
    "    'prec-Pneumothorax': 0.098,\n",
    "    'prec-Pleural Effusion': 0.689,\n",
    "    'prec-Pleural Other': 0,\n",
    "    'prec-Fracture': 0,\n",
    "    'prec-Support Devices': 0.880,\n",
    "}\n",
    "woNF = calculate_avg_woNF(_values, ['prec'], CHEXPERT_DISEASES, verbose=False)\n",
    "woNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_values.update(woNF)\n",
    "\n",
    "chexpert_results = {\n",
    "    'test': _values,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chexpert_metrics(folder, chexpert_results)\n",
    "save_runtime_metrics(folder, runtime_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ni et al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1(values, diseases):\n",
    "    f1s = dict()\n",
    "    for disease in diseases:\n",
    "        prec = values[f'prec-{disease}']\n",
    "        recall = values[f'recall-{disease}']\n",
    "        \n",
    "        f1 = 2 * (prec * recall) / (prec + recall)\n",
    "        f1s[f'f1-{disease}'] = f1\n",
    "    f1s['f1'] = np.mean(list(f1s.values()))\n",
    "    return f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = get_paper_folder('mimic-cxr', 'ni-et-al')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIMIC-CXR dataset but only with abnormal findings!!!\n",
    "# approx 30k samples in total\n",
    "# CVSE + mutual exclusivity ablation \n",
    "bleu4, bleu1 = 0.036, 0.192\n",
    "# meteor = 0.077\n",
    "runtime_results = {\n",
    "    'test': {\n",
    "        'bleu1': bleu1, 'bleu4': bleu4,\n",
    "        'rougeL': 0.153,\n",
    "    }\n",
    "}\n",
    "_values = {\n",
    "    'acc': 0.863,\n",
    "    'prec': 0.317,\n",
    "    'recall': 0.224,\n",
    "\n",
    "    'acc-No Finding': 0.769,\n",
    "    'acc-Enlarged Cardiomediastinum': 0.926,\n",
    "    'acc-Cardiomegaly': 0.801,\n",
    "    'acc-Lung Lesion': 0.921,\n",
    "    'acc-Lung Opacity': 0.692,\n",
    "    'acc-Edema': 0.920,\n",
    "    'acc-Consolidation': 0.876,\n",
    "    'acc-Pneumonia': 0.859,\n",
    "    'acc-Atelectasis': 0.773,\n",
    "    'acc-Pneumothorax': 0.964,\n",
    "    'acc-Pleural Effusion': 0.894,\n",
    "    'acc-Pleural Other': 0.962,\n",
    "    'acc-Fracture': 0.917,\n",
    "    'acc-Support Devices': 0.808,\n",
    "\n",
    "    'prec-No Finding': 0.346,\n",
    "    'prec-Enlarged Cardiomediastinum': 0.063,\n",
    "    'prec-Cardiomegaly': 0.512,\n",
    "    'prec-Lung Lesion': 0.192,\n",
    "    'prec-Lung Opacity': 0.635,\n",
    "    'prec-Edema': 0.405,\n",
    "    'prec-Consolidation': 0.130,\n",
    "    'prec-Pneumonia': 0.364,\n",
    "    'prec-Atelectasis': 0.525,\n",
    "    'prec-Pneumothorax': 0.073,\n",
    "    'prec-Pleural Effusion': 0.640,\n",
    "    'prec-Pleural Other': 0.145,\n",
    "    'prec-Fracture': 0.063,\n",
    "    'prec-Support Devices': 0.348,\n",
    "\n",
    "    'recall-No Finding': 0.265,\n",
    "    'recall-Enlarged Cardiomediastinum': 0.060,\n",
    "    'recall-Cardiomegaly': 0.606,\n",
    "    'recall-Lung Lesion': 0.121,\n",
    "    'recall-Lung Opacity': 0.237,\n",
    "    'recall-Edema': 0.206,\n",
    "    'recall-Consolidation': 0.181,\n",
    "    'recall-Pneumonia': 0.214,\n",
    "    'recall-Atelectasis': 0.320,\n",
    "    'recall-Pneumothorax': 0.051,\n",
    "    'recall-Pleural Effusion': 0.465,\n",
    "    'recall-Pleural Other': 0.036,\n",
    "    'recall-Fracture': 0.050,\n",
    "    'recall-Support Devices': 0.321,\n",
    "}\n",
    "f1 = calculate_f1(_values, CHEXPERT_DISEASES)\n",
    "_values.update(f1)\n",
    "woNF = calculate_avg_woNF(_values, ['recall', 'f1', 'prec'], CHEXPERT_DISEASES, verbose=False)\n",
    "_values.update(woNF)\n",
    "\n",
    "chexpert_results = {\n",
    "    'test': _values,\n",
    "}\n",
    "f1, woNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chexpert_metrics(folder, chexpert_results)\n",
    "save_runtime_metrics(folder, runtime_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chen et al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = get_paper_folder('mimic-cxr', 'chen-et-al')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu1, bleu2, bleu3, bleu4 = 0.353, 0.218, 0.145, 0.103\n",
    "# meteor = 0.142 # unused!\n",
    "runtime_results = {\n",
    "    'test': {\n",
    "        'bleu1': bleu1, 'bleu2': bleu2, 'bleu3': bleu3, 'bleu4': bleu4,\n",
    "        'bleu': np.mean([bleu1, bleu2, bleu3, bleu4]),\n",
    "        'rougeL': 0.277,\n",
    "    }\n",
    "}\n",
    "chexpert_results = {\n",
    "    'test': {\n",
    "        'f1': 0.276,\n",
    "        'prec': 0.333,\n",
    "        'recall': 0.273,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chexpert_metrics(folder, chexpert_results)\n",
    "save_runtime_metrics(folder, runtime_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RTEx paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_folder = get_paper_folder('mimic-cxr', 'rtex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_results = {\n",
    "    'test': {\n",
    "        'bleu4': 5.9 / 100, # Assume is bleu4\n",
    "        'rougeL': 20.5 / 100\n",
    "    }\n",
    "}\n",
    "chexpert_results = {\n",
    "    'test': {\n",
    "        'prec': 0.229,\n",
    "        'recall': 0.284,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chexpert_metrics(mimic_folder, chexpert_results)\n",
    "save_runtime_metrics(mimic_folder, runtime_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iu_folder = get_paper_folder('iu-x-ray', 'rtex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_results = {\n",
    "    'test': {\n",
    "        'bleu4': 5.5 / 100,\n",
    "        'rougeL': 20.2 / 100\n",
    "    }\n",
    "}\n",
    "chexpert_results = {\n",
    "    'test': {\n",
    "        'prec': 0.193,\n",
    "        'recall': 0.222,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chexpert_metrics(iu_folder, chexpert_results)\n",
    "save_runtime_metrics(iu_folder, runtime_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey IU papers\n",
    "\n",
    "Papers that only report NLP metrics in IU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iu_folder = get_paper_folder('iu-x-ray', 'rtex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_RESULTS = [\n",
    "    # paper, bleu1, bleu2, bleu3, bleu4, rougeL, cider-D\n",
    "    # ('coatt', 0.517, 0.386, 0.306, 0.247, 0.447, 0.327), # findings+impression\n",
    "    ('coatt_re-impl-hrgr', 0.455, 0.288, 0.205, 0.154, 0.369, 0.277),\n",
    "    ('coatt_re-impl-huang-et-al', 0.429, 0.295, 0.201, 0.148, 0.340, 0.278),\n",
    "    ('coatt_re-impl-a3fn', 0.421, 0.324, 0.225, 0.174, 0.341, 0.331),\n",
    "#     ('hrgr', 0.438, 0.298, 0.208, 0.151, 0.369, 0.343),\n",
    "#     ('kerp', 0.482, 0.325, 0.226, 0.162, 0.339, 0.280),\n",
    "#     ('tienet', 0.330, 0.194, 0.124, 0.081, 0.311, 1.334), # Reported in Liu et al.\n",
    "#     ('rtmic', 0.350, 0.234, 0.143, 0.096, None, 0.323), # Cider, not -D\n",
    "#     ('clara', 0.471, 0.324, 0.214, 0.199, None, 0.359),\n",
    "#     ('syeda-et-al', 0.560, 0.510, 0.500, 0.490, 0.580, None), # findings+impression, apparently\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in PAPER_RESULTS:\n",
    "    paper, bleu1, bleu2, bleu3, bleu4, rougeL, ciderD = result\n",
    "    folder = get_paper_folder('iu-x-ray', paper)\n",
    "    \n",
    "    d = {\n",
    "        'bleu1': bleu1, 'bleu2': bleu2, 'bleu3': bleu3, 'bleu4': bleu4,\n",
    "        'bleu': np.mean([bleu1, bleu2, bleu3, bleu4]), # will fail if any is None\n",
    "    }\n",
    "    if ciderD is not None:\n",
    "        d['ciderD'] = ciderD\n",
    "    if rougeL is not None:\n",
    "        d['rougeL'] = rougeL\n",
    "    \n",
    "    runtime_results = {'test': d}\n",
    "    \n",
    "    save_runtime_metrics(folder, runtime_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

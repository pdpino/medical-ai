{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01cf08d1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd87ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f989e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHEXPERT_LABELS = [\n",
    "    'No Finding',\n",
    "    'Enlarged Cardiomediastinum',\n",
    "    'Cardiomegaly',\n",
    "    'Lung Lesion',\n",
    "    'Lung Opacity',\n",
    "    'Edema',\n",
    "    'Consolidation',\n",
    "    'Pneumonia',\n",
    "    'Atelectasis',\n",
    "    'Pneumothorax',\n",
    "    'Pleural Effusion',\n",
    "    'Pleural Other',\n",
    "    'Fracture',\n",
    "    'Support Devices',\n",
    "]\n",
    "CHEXPERT_LABELS_5 = [\n",
    "    'Atelectasis',\n",
    "    'Cardiomegaly',\n",
    "    'Consolidation',\n",
    "    'Edema',\n",
    "    'Pleural Effusion',\n",
    "]\n",
    "CHEXPERT_LABELS_6 = CHEXPERT_LABELS_5 + ['Lung Opacity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2d0613",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = '/home/pdpino/workspace-medical-ai/report_generation/nlp-chex-gold-sentences'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23743a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9278fac0",
   "metadata": {},
   "source": [
    "# Load sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738d3348",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'mimic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa22bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'iu':\n",
    "    path = '/mnt/workspace/iu-x-ray/dataset-pdpino/sentences_with_chexpert_labels.csv'\n",
    "    sentences_df = pd.read_csv(path)\n",
    "else:\n",
    "    path1 = '/mnt/data/mimic-cxr-jpg/reports/sentences_with_chexpert_labels.csv'\n",
    "    path2 = '/mnt/data/mimic-cxr-jpg/reports/sentences.csv'\n",
    "    sentences_df = pd.read_csv(path1).merge(pd.read_csv(path2), on='sentence')\n",
    "for abn in CHEXPERT_LABELS:\n",
    "    sentences_df[abn] = sentences_df[abn].astype(int)\n",
    "print(f'Using dataset {dataset}, n_sentences={len(sentences_df)}')\n",
    "sentences_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fca4c6",
   "metadata": {},
   "source": [
    "# Sample random sentences\n",
    "\n",
    "Make sure all abnormalities and all valuations are well represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38dea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_per_abn_per_val = 100\n",
    "N_total = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1871fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_df(df, n):\n",
    "    return random.sample(list(df.index), n)\n",
    "    # TODO: handle repetitions properly\n",
    "    # return random.sample(list(df.index), n, counts=df['appearances'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aaa581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_n_samples(df_target, chosen_indexes, n_target):\n",
    "    chosen_for_target = set(chosen_indexes).intersection(df_target.index)\n",
    "    \n",
    "    n_missing = n_target - len(chosen_for_target)\n",
    "\n",
    "    if n_missing > 0:\n",
    "        df_wo_repeating = df_target[~df_target.index.isin(chosen_for_target)]\n",
    "\n",
    "        n_grab = min(n_missing, len(df_wo_repeating))\n",
    "        if n_grab > 0:\n",
    "            return sample_from_df(df_wo_repeating, n_grab)\n",
    "\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c85920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALUATIONS = (1, -1, 0, -2)\n",
    "# ABNORMALITIES = tuple(CHEXPERT_LABELS[1:])\n",
    "ABNORMALITIES = CHEXPERT_LABELS_5\n",
    "\n",
    "def sample_sentences(df, n_total=1000, n_per_target=100):\n",
    "    chosen_indexes = []\n",
    "\n",
    "    for valuation in (1, -1, 0, -2):\n",
    "        for abn in ABNORMALITIES:\n",
    "            df_target = df.loc[df[abn] == valuation]\n",
    "            chosen_indexes.extend(grab_n_samples(df_target, chosen_indexes, n_per_target))\n",
    "\n",
    "    chosen_indexes.extend(grab_n_samples(df, chosen_indexes, n_total))\n",
    "\n",
    "    random.shuffle(chosen_indexes)\n",
    "    return df.iloc[chosen_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da0a992",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sample_sentences(sentences_df)[['sentence'] + CHEXPERT_LABELS]\n",
    "len(df), len(df.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f0e9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5f99f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_per_abn(df, labels=CHEXPERT_LABELS):\n",
    "    return pd.concat([\n",
    "        df[abn].value_counts()\n",
    "        for abn in labels   \n",
    "    ], axis=1).fillna(0).astype(int).transpose()\n",
    "count_per_abn(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a66f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname_filled = f'{folder}/{dataset}-filled.csv'\n",
    "fname_empty = f'{FOLDER}/{dataset}-empty.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871a2775",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df[['sentence']]\n",
    "# for col in CHEXPERT_LABELS_5 + ['Any other finding', '', 'Missing Context']:\n",
    "#     df_out[col] = \"\"\n",
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3305d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_csv(fname_empty, index=False)\n",
    "fname_empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29932f3c",
   "metadata": {},
   "source": [
    "# Clean answers and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36030541",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Sentence'] + CHEXPERT_LABELS_6 # + ['Not understood or malformed']\n",
    "dfs = {}\n",
    "should_ignore = list()\n",
    "for expert in (1,2):\n",
    "    fname = f'cxr-sentence-assessment-expert{expert}.csv'\n",
    "    df = pd.read_csv(f'{FOLDER}/{fname}')\n",
    "    should_ignore.extend(list(df.loc[df['Not understood or malformed'] == True]['Sentence']))\n",
    "    \n",
    "    df = df[cols]\n",
    "    df = df.replace('Abnormal', 1).replace('Normal', 0).replace('Uncertain', -1).fillna(-2)\n",
    "    df = df.rename({'Sentence': 'sentence'}, axis=1)\n",
    "    df = df.astype(int, errors='ignore')\n",
    "    dfs[expert] = df\n",
    "    \n",
    "for expert in dfs.keys():\n",
    "    df = dfs[expert]\n",
    "    df = df.loc[~df['sentence'].isin(should_ignore)]\n",
    "    dfs[expert] = df\n",
    "len(dfs[1]), len(dfs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddbb6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for expert in dfs.keys():\n",
    "    dfs[expert].to_csv(f'{FOLDER}/mimic-expert{expert}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c3cd9",
   "metadata": {},
   "source": [
    "# Analyze expert answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f05b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for expert in (1, 2):\n",
    "    dfs[expert] = pd.read_csv(f'{FOLDER}/mimic-expert{expert}.csv')\n",
    "dfs[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62975ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chex_gt = sentences_df[['sentence'] + CHEXPERT_LABELS_6].rename({\n",
    "    abn: f'{abn}_chex'\n",
    "    for abn in CHEXPERT_LABELS_6\n",
    "}, axis=1)\n",
    "chex_gt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d58fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "gts = dfs[1].merge(dfs[2], on='sentence', suffixes=(\"_exp1\", \"_exp2\")).merge(chex_gt, on='sentence')\n",
    "gts2 = gts.replace(-1, 1).replace(-2, 0)\n",
    "len(gts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10a14d9",
   "metadata": {},
   "source": [
    "## Compute Kappa agreement\n",
    "  - use 2x2 and 4x4\n",
    "  - per abnormality\n",
    "  - chexpert vs each expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7187d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d7b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kappas(gts, score_fn=cohen_kappa_score):\n",
    "    kappas_df = pd.DataFrame()\n",
    "    for abn in CHEXPERT_LABELS_6:\n",
    "        values_exp1 = gts[f'{abn}_exp1']\n",
    "        values_exp2 = gts[f'{abn}_exp2']\n",
    "        values_chex = gts[f'{abn}_chex']\n",
    "\n",
    "        score_exps = score_fn(values_exp1, values_exp2)\n",
    "        score_exp1_chex = score_fn(values_exp1, values_chex)\n",
    "        score_exp2_chex = score_fn(values_exp2, values_chex)\n",
    "        \n",
    "        kappas_df.loc[abn, 'R1 - R2'] = score_exps\n",
    "        kappas_df.loc[abn, 'R1 vs CheX'] = score_exp1_chex\n",
    "        kappas_df.loc[abn, 'R2 vs CheX'] = score_exp2_chex\n",
    "        \n",
    "    return kappas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a369368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kappas4 = compute_kappas(gts)\n",
    "kappas4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc3dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kappas2 = compute_kappas(gts2)\n",
    "kappas2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740c5cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson2 = compute_kappas(gts2, score_fn=lambda x, y: pearsonr(x, y)[0])\n",
    "pearson2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6af7731",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kappas2.style.format(precision=3).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b4ea6a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## CheXpert test set\n",
    "\n",
    "- are all sentences in test set? (how much are missing?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a385ea",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run ../../datasets/mimic_cxr.py\n",
    "%run ../../utils/nlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a152d3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mimic_dataset = MIMICCXRDataset(dataset_type='test', do_not_load_image=True)\n",
    "len(mimic_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20a38b1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reports = [\n",
    "    mimic_dataset._reports[row['study_id']]['clean_text']\n",
    "    for _, row in mimic_dataset.master_df.iterrows()\n",
    "]\n",
    "len(reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca0a0d5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TEST_SENTENCES = set(\n",
    "    sentence\n",
    "    for report in reports\n",
    "    for sentence in split_sentences_text(report)\n",
    ")\n",
    "len(TEST_SENTENCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb78d68c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "GT_SENTENCES = list(gts['sentence'])\n",
    "len(GT_SENTENCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afde93d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(TEST_SENTENCES.intersection(GT_SENTENCES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316b97e0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "count = []\n",
    "for report in reports:\n",
    "    sentences = split_sentences_text(report)\n",
    "    n_appearances = sum(int(sentence in GT_SENTENCES) for sentence in sentences)\n",
    "    count.append((report, n_appearances, len(sentences)))\n",
    "len(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f4872d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[\n",
    "    (n, m)\n",
    "    for report, n, m in count\n",
    "    if n > 0\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a54919",
   "metadata": {},
   "source": [
    "## Compute CheX against experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a7fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as prf1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b33bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f1, _ = prf1s(gt, target, zero_division=0, average=None)\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1641f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../metrics/report_generation/chexpert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcf575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [f'{abn}_chex' for abn in CHEXPERT_LABELS_6]\n",
    "target = gts2[target_cols].to_numpy()\n",
    "\n",
    "results = pd.DataFrame(index=CHEXPERT_LABELS_6)\n",
    "for expert in (1, 2):\n",
    "    gt_cols = [f'{abn}_exp{expert}' for abn in CHEXPERT_LABELS_6]\n",
    "    gt = gts2[gt_cols].to_numpy()\n",
    "    \n",
    "    acc, precision, recall, f1, roc_auc, pr_auc = calculate_metrics(gt, target)\n",
    "    \n",
    "    results[f'prec-expert{expert}'] = precision\n",
    "    results[f'recall-expert{expert}'] = recall\n",
    "    results[f'f1-expert{expert}'] = f1\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dad0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.style.format('{:.3f}').to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a10ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

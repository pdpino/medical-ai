{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "import importlib\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.facecolor'] = 'white'\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../utils/__init__.py\n",
    "config_logging(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../datasets/common/constants.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n ./nlp_in_chexpert_groups.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n ./nlp_in_chexpert_groups.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info = init_dataset_info('iu')\n",
    "dataset_info.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = init_experiment('Cardiomegaly', dataset_info)\n",
    "exp_LO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "kwargs = {\n",
    "    # 'metric': 'bleu',\n",
    "    # 'metric': 'rouge',\n",
    "    'metric': 'cider-IDF',\n",
    "    'k_times': 500,\n",
    "    # 'k_gts': 1,\n",
    "    'max_n': 500,\n",
    "}\n",
    "exp.append(calc_score_matrices(exp.grouped_2, dataset_info, groups=(0, 1), **kwargs))\n",
    "# exp.append(calc_score_matrices(exp.grouped, dataset_info, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp[-1].cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze/plot experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = load_experiment_pickle('iu-cardiomegaly')\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.results[-1].cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_I = -1\n",
    "METRIC_I = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_heatmap(exp, result_i=RESULT_I, metric_i=METRIC_I)\n",
    "\n",
    "if len(exp.results) > 1:\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plot_heatmap(exp, result_i=-2, metric_i=METRIC_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info.log_ref_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plot_hists(\n",
    "    exp, [\n",
    "        (0, 0), (0, 1),\n",
    "        # (0, 0), (1, 0),\n",
    "        # (1, 1), (0, 1),\n",
    "        # (1, 0), (0, 0),\n",
    "    ],\n",
    "    result_i=RESULT_I, metric_i=METRIC_I,\n",
    "    xlabel=False, bins=50, range=(0, 1),\n",
    "    add_n_to_label=True,\n",
    ")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plot_hists(\n",
    "    exp, [\n",
    "        (1, 1), (1, 0),\n",
    "        # (1, 1), (0, 1),\n",
    "        # (0, 0), (1, 0),\n",
    "        # (-2, -1), (-1, -1),\n",
    "    ],\n",
    "    result_i=RESULT_I, metric_i=METRIC_I, add_n_to_label=True,\n",
    "    title=False, bins=50, range=(0, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempting to optimize threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1\n",
    "\n",
    "(failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = exp[0]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target1, target2 = (0, 0), (0, 1) # TN, FP (specificity)\n",
    "target1, target2 = (1, 1), (1, 0) # TP, FN (precision)\n",
    "\n",
    "arr1 = result.dists[target1]\n",
    "arr2 = result.dists[target2]\n",
    "arr1.shape, arr2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert target1[0] == target1[1]\n",
    "CORRECT = target1[0]\n",
    "INCORRECT = 1 - CORRECT\n",
    "\n",
    "merged = [(value, CORRECT) for value in arr1] + [(value, INCORRECT) for value in arr2]\n",
    "merged = sorted(merged, reverse=bool(not CORRECT))\n",
    "merged[:2], merged[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_threshs = []\n",
    "denominator = 0 # TP + FN\n",
    "numerator = 0 # TP\n",
    "for value, label in merged:\n",
    "    current_thresh = value\n",
    "    if label == CORRECT: # add 1 TP\n",
    "        numerator += 1\n",
    "\n",
    "    denominator += 1\n",
    "\n",
    "    all_threshs.append((current_thresh, numerator / denominator))\n",
    "all_threshs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(all_threshs, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = tuple(zip(*all_threshs))\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Attempt 2\n",
    "\n",
    "with sklearn\n",
    "Failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve as pr_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pred, gt = tuple(zip(*merged))\n",
    "pred = np.array(pred)\n",
    "gt = np.array(gt)\n",
    "pred.shape, gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pred /= 10 # CIDER re-scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "precision, recall, thresholds = pr_curve(gt, pred, pos_label=CORRECT)\n",
    "precision.shape, recall.shape, thresholds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f1 = divide_arrays(2*precision*recall, precision + recall)\n",
    "f1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "best_idx = f1.argmax()\n",
    "best_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "thresholds[best_idx], f1[best_idx], precision[best_idx], recall[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Attempt 3: accuracy/prec/recall\n",
    "\n",
    "CheXpert 4-class classification task --> is a binary classification task in NLP scores\n",
    "(i.e. NLP scores tell less information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp = load_experiment_pickle('mimic-cardiomegaly')\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = exp.results[-1]\n",
    "result.metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result.dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "merged = [\n",
    "    # Value, correct-or-not, original-key\n",
    "    (value, 1, (0, 0)) for value in result.dists[(0, 0)]\n",
    "] + [\n",
    "    (value, 1, (1, 1)) for value in result.dists[(1, 1)]\n",
    "] + [\n",
    "    (value, 0, (0, 1)) for value in result.dists[(0, 1)]\n",
    "] + [\n",
    "    (value, 0, (1, 0)) for value in result.dists[(1, 0)]\n",
    "]\n",
    "merged = sorted(merged)\n",
    "len(merged), merged[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_correct = sum(1 for _, correct, _ in merged if correct)\n",
    "n_incorrect = sum(1 for _, correct, _ in merged if not correct)\n",
    "n_correct, n_incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def smart_division(a, b):\n",
    "    if b == 0:\n",
    "        return 0\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_threshs = []\n",
    "\n",
    "# At first, the threshold is at 0\n",
    "# --> No negative predictions, all positive predictions\n",
    "# --> TN = FN = 0\n",
    "TP = sum(1 for _, correct, _ in merged if correct)\n",
    "FP = sum(1 for _, correct, _ in merged if not correct)\n",
    "TN, FN = 0, 0\n",
    "\n",
    "total = len(merged)\n",
    "\n",
    "assert TP + FP + FN + TN == total, f'Begin: {TP + FP + FN + TN} vs {total}'\n",
    "\n",
    "for value, correct, _ in merged:\n",
    "    current_thresh = value\n",
    "\n",
    "    if correct:\n",
    "        TP -= 1\n",
    "        FN += 1\n",
    "    else:\n",
    "        TN += 1\n",
    "        FP -= 1\n",
    "\n",
    "    assert TP + FP + FN + TN == total, f'Thresh={value}: {TP + FP + FN + TN} vs {total}'\n",
    "        \n",
    "    acc = (TP + TN) / total\n",
    "    prec = smart_division(TP, TP + FP)\n",
    "    recall = smart_division(TP, TP + FN)\n",
    "    f1 = smart_division(2*prec*recall, prec+recall)\n",
    "    spec = smart_division(TN, TN + FP)\n",
    "    npv = smart_division(TN, TN + FN)\n",
    "    f1_neg = smart_division(2*npv*spec, spec+npv)\n",
    "    CM = (TP, FN, FP, TN)\n",
    "\n",
    "    all_threshs.append({\n",
    "        'thresh': current_thresh,\n",
    "        'acc': acc,\n",
    "        'prec': prec,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'npv': npv,\n",
    "        'spec': spec,\n",
    "        'f1_neg': f1_neg,\n",
    "        'CM': CM,\n",
    "    })\n",
    "all_threshs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max(all_threshs, key=lambda x: x['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sl = lambda k: tuple(zip(*[(x['thresh'], x[k]) for x in all_threshs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "keys = ('prec', 'recall', 'acc', 'f1') # 'f1', \n",
    "# keys = ('acc', )\n",
    "# keys = ('npv', 'spec', 'f1_neg')\n",
    "for k in keys:\n",
    "    thresh, y = sl(k)\n",
    "    plt.plot(thresh, y, label=k)\n",
    "plt.legend()\n",
    "plt.xlabel('Thresh')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Optimize by')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "best = max(all_threshs, key=lambda x: x['acc'])\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_cm(cm, title=None):\n",
    "    TP, FN, FP, TN = cm\n",
    "    ticks = ['Entailment', 'Contradiction']\n",
    "    sns.heatmap([[TP, FN], [FP, TN]], annot=True, square=True, cmap='Blues',\n",
    "                xticklabels=ticks, yticklabels=ticks, fmt=',',\n",
    "               )\n",
    "    plt.ylabel('Real')\n",
    "    plt.xlabel('Scored by Metric')\n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_cm(best['CM'], title=f'CM for {exp.abnormality} with {get_pretty_metric(result.metric)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 4: use AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_gt_pred_for_roc(result, metric_i=0, keys=None):\n",
    "    pred = []\n",
    "    gt = []\n",
    "    if keys is None:\n",
    "        keys = list(result.dists.keys())\n",
    "\n",
    "    for a, b in keys:\n",
    "        elements = result.dists[(a, b)]\n",
    "        if elements.ndim == 2:\n",
    "            elements = elements[metric_i] # BLEU case\n",
    "        pred += list(elements)\n",
    "\n",
    "        entailment = int(a == b)\n",
    "        gt += [entailment] * len(elements)\n",
    "\n",
    "    return gt, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = exp.results[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt, pred = prepare_gt_pred_for_roc(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(gt, pred)\n",
    "\n",
    "J_stat = tpr - fpr\n",
    "best_idx = J_stat.argmax()\n",
    "\n",
    "thresholds[best_idx], J_stat[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc = roc_auc_score(gt, pred)\n",
    "roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute AUC for all abnormalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiments\n",
    "def load_experiments(dataset_name):\n",
    "    exp_by_abn = {}\n",
    "    errors = defaultdict(list)\n",
    "    for abnormality in CHEXPERT_DISEASES[1:]:\n",
    "        fname = f'{dataset_name}-{abnormality.replace(\" \", \"-\").lower()}'\n",
    "        if not exist_experiment_pickle(fname):\n",
    "            errors['not-found'].append(fname)\n",
    "            continue\n",
    "        exp = load_experiment_pickle(fname)\n",
    "        exp_by_abn[abnormality] = exp\n",
    "        \n",
    "    if len(errors['not-found']):\n",
    "        print('Not found: ', errors['not-found'])\n",
    "        \n",
    "    return exp_by_abn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_by_abn_iu = load_experiments('iu')\n",
    "len(exp_by_abn_iu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show = True\n",
    "dataset_name = 'iu'\n",
    "target_sampler = None # 'random-gen_k500_n500'\n",
    "target_groups = None # [-2, -1, 0, 1]\n",
    "# keys = [(0, 0), (0, 1), (1, 1), (1, 0)]\n",
    "keys = None\n",
    "\n",
    "final_records = []\n",
    "\n",
    "for abnormality in CHEXPERT_DISEASES[1:]:\n",
    "    if abnormality not in exp_by_abn_iu:\n",
    "        continue\n",
    "    exp = exp_by_abn_iu[abnormality]\n",
    "\n",
    "    for result in tqdm(exp.results, desc=abnormality, disable=not show):\n",
    "        if target_sampler is not None and result.sampler != target_sampler:\n",
    "            continue\n",
    "        if target_groups is not None and sorted(result.groups) != target_groups:\n",
    "            continue\n",
    "\n",
    "        gt, pred = prepare_gt_pred_for_roc(result, keys=keys)\n",
    "        roc = roc_auc_score(gt, pred)\n",
    "        \n",
    "        final_records.append((abnormality, result.metric, result.groups, result.sampler, roc))\n",
    "        \n",
    "        if result.metric == 'bleu':\n",
    "            gt, pred = prepare_gt_pred_for_roc(result, metric_i=3, keys=keys)\n",
    "            roc = roc_auc_score(gt, pred)\n",
    "            final_records.append((\n",
    "                abnormality, f'{result.metric}-4', result.groups, result.sampler, roc,\n",
    "            ))\n",
    "            \n",
    "len(final_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_records[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['disease', 'metric', 'groups', 'sampler', 'roc']\n",
    "df = pd.DataFrame(final_records, columns=cols)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['groups'] == (0, 1)]\n",
    "df = df.loc[df['sampler'] == 'random-gen_k500_n500']\n",
    "del df['sampler'], df['groups']\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df.groupby('disease').apply(lambda subdf: {\n",
    "    row[1]: row[2] # row is: [disease, metric, roc]\n",
    "    for row in list(subdf.values)\n",
    "}).apply(pd.Series)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = d.to_latex()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(exp_by_abn, abnormality, metric,\n",
    "               groups=[0, 1], sampler='random-gen_k500_n500'):\n",
    "    if abnormality not in exp_by_abn:\n",
    "        print(f'No exp for {abnormality}')\n",
    "        return None, None\n",
    "    groups = list(groups)\n",
    "    exp = exp_by_abn[abnormality]\n",
    "\n",
    "    for i, result in enumerate(exp.results):\n",
    "        if sampler is not None and result.sampler != sampler:\n",
    "            continue\n",
    "        if groups is not None and sorted(result.groups) != groups:\n",
    "            continue\n",
    "        if result.metric != metric:\n",
    "            continue\n",
    "            \n",
    "        return exp, i\n",
    "    \n",
    "    print('No experiment found with conditions')\n",
    "    return exp, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp, result_i = get_result(exp_by_abn_iu, 'Atelectasis', 'bleu')\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(exp, result_i=result_i, metric_i=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hists(exp, keys=[(0, 0), (0, 1)], result_i=result_i, metric_i=3, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, mannwhitneyu, f_oneway, kruskal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp = load_experiment_pickle('mimic-cardiomegaly')\n",
    "len(exp.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(exp, result_i=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_I = -1\n",
    "result = exp[EXP_I]\n",
    "result.metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key1 = (0, 0)\n",
    "key2 = (0, 1)\n",
    "group1 = result.dists[key1]\n",
    "group2 = result.dists[key2]\n",
    "if result.metric == 'bleu':\n",
    "    group1 = group1[0]\n",
    "    group2 = group2[0]\n",
    "group1.shape, group2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hists(exp, [key1, key2], result_i=EXP_I, bins=50, range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = mannwhitneyu(group1, group2)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = ttest_ind(group1, group2, equal_var=False)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [result.dists[k] for k in [(0, 0), (0, 1), (1, 0), (1, 1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova = f_oneway(*groups)\n",
    "anova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kru = kruskal(*groups)\n",
    "kru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

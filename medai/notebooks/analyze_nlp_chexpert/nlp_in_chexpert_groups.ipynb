{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "import importlib\n",
    "import json\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.facecolor'] = 'white'\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.rcParams.update({'font.family': 'serif', 'font.sans-serif': ['CMU', 'Helvetica']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../utils/__init__.py\n",
    "config_logging(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../datasets/common/constants.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n ./nlp_in_chexpert_groups.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHEXPERT_LABELS_6 = [\n",
    "    'Cardiomegaly',\n",
    "    'Consolidation',\n",
    "    'Edema',\n",
    "    'Pleural Effusion',\n",
    "    'Atelectasis',\n",
    "    'Lung Opacity',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_image_fpath(exp, result_i, metric_i, prefix, suffix=''):\n",
    "    print('Using prefix: ', prefix)\n",
    "\n",
    "    metric = exp.results[result_i].metric\n",
    "    if metric == 'bleu':\n",
    "        metric += f'{metric_i+1}'\n",
    "\n",
    "    fname = '-'.join([s for s in (\n",
    "        'nlp-vs-chex',\n",
    "        prefix,\n",
    "        metric,\n",
    "        ABN_SHORTCUTS[exp.abnormality].lower(),\n",
    "        exp.dataset,\n",
    "        suffix,\n",
    "    ) if s])\n",
    "\n",
    "    fpath = os.path.join(FIGURES_DIR, f'{fname}.pdf')\n",
    "    \n",
    "    print('Filepath: ', fpath)\n",
    "    \n",
    "    return fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_result_index(results, metric_name, n_groups=2):\n",
    "    filtered_results = [\n",
    "        (i, r) for i, r in enumerate(results)\n",
    "        if r.metric == metric_name and len(r.groups) == n_groups\n",
    "    ]\n",
    "    if len(filtered_results) == 0:\n",
    "        return -1\n",
    "\n",
    "    if len(filtered_results) == 1:\n",
    "        index, _ = filtered_results[0]\n",
    "        return index\n",
    "    \n",
    "    # Find the one with largest k\n",
    "    def extract_k(sampler):\n",
    "        match = re.search(r'_k(\\d+)_', sampler)\n",
    "        if not match or not match.group(1):\n",
    "            return -1000\n",
    "        return int(match.group(1))\n",
    "\n",
    "    filtered_results = [\n",
    "        (extract_k(r.sampler), i)\n",
    "        for i, r in filtered_results\n",
    "    ]\n",
    "    _, index = max(filtered_results)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Debug running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run -n ./nlp_in_chexpert_groups.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dataset_info = init_dataset_info('mimic-expert1')\n",
    "dataset_info.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp = init_experiment('Cardiomegaly', dataset_info)\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "kwargs = {\n",
    "    # 'metric': 'bleu',\n",
    "    # 'metric': 'rouge', #'cider-IDF', #'bleurt',\n",
    "    'metric': 'bleu',\n",
    "    'sampler': 'random-gen',\n",
    "    'k_times': 200,\n",
    "    'abnormality': exp.abnormality,\n",
    "    # 'k_gts': 1,\n",
    "    'max_n': 200,\n",
    "}\n",
    "exp.append(calc_score_matrices(exp.grouped_2, dataset_info, groups=(0,1), **kwargs))\n",
    "# exp.append(calc_score_matrices(exp.grouped, dataset_info, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp[-1].cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Plot experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Plot one example\n",
    "\n",
    "One matrix at the left, 2 histograms at the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp = load_experiment_pickle('mimic-expert1-lung-opacity')\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[(i, r.metric, r.groups, r.sampler) for i, r in enumerate(exp.results)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "RESULT_I = 14\n",
    "METRIC_I = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run -n ./nlp_in_chexpert_groups.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_suptitle(exp, result_i, metric_i):\n",
    "    result = exp.results[result_i]\n",
    "    pretty_metric = get_pretty_metric(result.metric, metric_i=metric_i)\n",
    "    dataset = 'IU X-ray' if exp.dataset == 'iu' else 'MIMIC-CXR'\n",
    "    # ({dataset} dataset)\n",
    "    return f'{pretty_metric} in {exp.abnormality} sentences'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 6))\n",
    "shape = (2, 2) # Axes shape\n",
    "ax1 = plt.subplot2grid(shape, (0, 0), rowspan=2)\n",
    "ax_hist1 = plt.subplot2grid(shape, (0, 1))\n",
    "ax_hist2 = plt.subplot2grid(shape, (1, 1))\n",
    "\n",
    "_kw = {'xlabel_fontsize': 14, 'ylabel_fontsize': 14, 'title_fontsize': 15,\n",
    "       'result_i': RESULT_I, 'metric_i': METRIC_I,\n",
    "      }\n",
    "plot_heatmap(exp, ax=ax1, title=False, annot_kws={'fontsize':13}, **_kw)\n",
    "\n",
    "_kw = {'add_n_to_label': False, 'bins': 50, 'legend_fontsize': 12,\n",
    "       'range': (0,1),\n",
    "       **_kw}\n",
    "plot_hists(exp,\n",
    "    [\n",
    "        (0, 0), (0, 1),\n",
    "    ], title=False, xlabel=False, ax=ax_hist1, **_kw)\n",
    "plot_hists(exp,\n",
    "    [\n",
    "        (1, 1), (1, 0),\n",
    "    ], title=False, ax=ax_hist2, **_kw) # , range=(0,2)\n",
    "\n",
    "# Set suptitle\n",
    "plt.suptitle(build_suptitle(exp, RESULT_I, METRIC_I), fontsize=17)\n",
    "\n",
    "# Set titles\n",
    "ax_hist1.set_title('Scores distribution', fontsize=_kw['title_fontsize'])\n",
    "ax1.set_title('Scores matrix', fontsize=_kw['title_fontsize'])\n",
    "\n",
    "_LOG_SCALE = False\n",
    "_SAVE = True\n",
    "\n",
    "if _LOG_SCALE:\n",
    "    ax_hist1.set_yscale('log')\n",
    "    ax_hist2.set_yscale('log')\n",
    "\n",
    "# increase fontsize of ticks in the first plot (HACKy way)\n",
    "a = ax1.figure.axes[0] # get the first plot\n",
    "a.set_xticklabels(a.get_xticklabels(), fontsize=12)\n",
    "a.set_yticklabels(a.get_yticklabels(), fontsize=12)\n",
    "\n",
    "print('Sampler used: ', exp.results[RESULT_I].sampler)\n",
    "\n",
    "_image_fpath = build_image_fpath(\n",
    "    exp, RESULT_I, METRIC_I, prefix='2023',\n",
    "    suffix='logscale' if _LOG_SCALE else '',\n",
    ")\n",
    "if _SAVE:\n",
    "    ax1.figure.savefig(_image_fpath, bbox_inches='tight')\n",
    "    print('\\tSAVED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Plot one matrix 4x4 + many histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run -n ./nlp_in_chexpert_groups.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp = load_experiment_pickle('mimic-expert1-cardiomegaly')\n",
    "# exp = load_experiment_pickle('iu-atelectasis')\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[(i, r.metric, r.groups) for i, r in enumerate(exp.results)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_SAVE = False\n",
    "\n",
    "plotable_metrics = [\n",
    "    (0, 0), # ('bleu', 0),\n",
    "    (0, 3), #('bleu', 3),\n",
    "    (2, 0), # ('rouge', 0),\n",
    "    (4, 0), # ('cider', 0),\n",
    "]\n",
    "n_metrics = len(plotable_metrics)\n",
    "\n",
    "shape = (n_metrics * 2, 3) # Axes shape\n",
    "fig = plt.figure(figsize=(shape[1]*8, shape[0]*3))\n",
    "\n",
    "for i_metric, (result_i, metric_i) in enumerate(plotable_metrics):\n",
    "    result = exp.results[result_i]\n",
    "    assert len(result.groups) == 4, result.groups\n",
    "    \n",
    "    row = i_metric * 2\n",
    "\n",
    "    ax1 = plt.subplot2grid(shape, (row, 0), rowspan=2, fig=fig)\n",
    "\n",
    "    ax_hist1 = plt.subplot2grid(shape, (row, 1))\n",
    "    ax_hist2 = plt.subplot2grid(shape, (row + 1, 1))\n",
    "    ax_hist3 = plt.subplot2grid(shape, (row, 2))\n",
    "    ax_hist4 = plt.subplot2grid(shape, (row + 1, 2))\n",
    "\n",
    "    _kw = {'xlabel_fontsize': 14, 'ylabel_fontsize': 14, 'title_fontsize': 15,\n",
    "           'result_i': result_i, 'metric_i': metric_i,\n",
    "          }\n",
    "    plot_heatmap(exp, ax=ax1, title=False, annot_kws={'fontsize':13}, **_kw)\n",
    "\n",
    "    _kw = {**_kw, 'labels_fontsize': 14, 'title': False, \n",
    "           'correct_color': 'forestgreen', 'incorrect_color': 'red',\n",
    "           'ylog': ((result.metric == 'bleu' and metric_i == 3) or result.metric == 'cider-IDF'),\n",
    "           }\n",
    "    plot_boxplots(exp, [(-2, -2), (-2, 0), (-2, -1), (-2, 1)], xlabel=False, ax=ax_hist1, **_kw)\n",
    "    plot_boxplots(exp, [(0, 0), (0, -2), (0, -1), (0, 1)], ax=ax_hist2, **_kw)\n",
    "    plot_boxplots(exp, [(-1, -1), (-1, 1), (-1, 0), (-1, -2)], xlabel=False, ax=ax_hist3, **_kw)\n",
    "    plot_boxplots(exp, [(1, 1), (1, -1), (1, 0), (1, -2)], ax=ax_hist4, **_kw)\n",
    "\n",
    "    # Set titles\n",
    "    pretty_metric = get_pretty_metric(result.metric, metric_i=metric_i)\n",
    "    ax_hist1.set_title(f'{pretty_metric} distributions', fontsize=_kw['title_fontsize'])\n",
    "    ax_hist3.set_title(f'{pretty_metric} distributions', fontsize=_kw['title_fontsize'])\n",
    "\n",
    "    ax1.set_title(f'{pretty_metric} matrix', fontsize=_kw['title_fontsize'])\n",
    "\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "# Set suptitle\n",
    "dataset = 'IU X-ray' if exp.dataset == 'iu' else 'MIMIC-CXR'\n",
    "suptitle = f'{exp.abnormality} sentences ({dataset} dataset)'\n",
    "plt.suptitle(suptitle, fontsize=17, y=1.02)\n",
    "\n",
    "_image_fpath = os.path.join(\n",
    "    FIGURES_DIR,\n",
    "    f'nlp-vs-chex-manyH-4x4-{exp.dataset}-{ABN_SHORTCUTS[exp.abnormality].lower()}.pdf',\n",
    ")\n",
    "\n",
    "if _SAVE:\n",
    "    fig.savefig(_image_fpath, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Plot many histograms 2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run -n ./nlp_in_chexpert_groups.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp_by_abn = load_experiments('mimic-expert1')\n",
    "len(exp_by_abn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ABNORMALITIES = [\n",
    "    'Atelectasis',\n",
    "    'Consolidation',\n",
    "    'Edema',\n",
    "]\n",
    "PLOTABLE_METRICS = [\n",
    "    ('bleu', 0),\n",
    "    # ('bleu', 3),\n",
    "    # ('rouge', 0),\n",
    "    #('cider-IDF', 0),\n",
    "    ('bleurt', 0),\n",
    "    ('bertscore', 2),\n",
    "]\n",
    "n_metrics = len(PLOTABLE_METRICS)\n",
    "n_abns = len(ABNORMALITIES)\n",
    "\n",
    "shape = (n_abns * 2, n_metrics * 2) # Axes shape\n",
    "fig = plt.figure(figsize=(shape[1]*7, shape[0]*3))\n",
    "\n",
    "for i_abn, abnormality in enumerate(ABNORMALITIES):\n",
    "    exp = exp_by_abn[abnormality]\n",
    "    for i_plot, (metric_name, metric_i) in enumerate(PLOTABLE_METRICS):\n",
    "        result_i = find_result_index(exp.results, metric_name)\n",
    "        if result_i == -1:\n",
    "            #print(\n",
    "            #    f'Metric {metric_name} not found in {exp.abnormality}, see: ',\n",
    "            #    [r.metric for r in exp.results])\n",
    "            continue\n",
    "        \n",
    "        result = exp.results[result_i]\n",
    "        assert len(result.groups) == 2, result.groups\n",
    "\n",
    "        row = i_abn * 2\n",
    "        col = i_plot * 2\n",
    "\n",
    "        ax1 = plt.subplot2grid(shape, (row, col), rowspan=2, fig=fig, xmargin=0.1, ymargin=0.1)\n",
    "        ax_hist1 = plt.subplot2grid(shape, (row, col+1))\n",
    "        ax_hist2 = plt.subplot2grid(shape, (row + 1, col+1))\n",
    "\n",
    "        _kw = {'xlabel_fontsize': 14, 'ylabel_fontsize': 14, 'title_fontsize': 15,\n",
    "               'result_i': result_i, 'metric_i': metric_i, 'title': False,\n",
    "              }\n",
    "        plot_heatmap(exp, ax=ax1, ticks_fontsize=12,\n",
    "                     ylabel=False, xlabel=False, annot_kws={'fontsize':18}, **_kw)\n",
    "\n",
    "        is_cider = result.metric == 'cider-IDF'\n",
    "        is_bleu = result.metric == 'bleu'\n",
    "        is_bleu4 = is_bleu and (metric_i == 3)\n",
    "        _kw = {'add_n_to_label': False, 'bins': 50, 'legend_fontsize': 12,\n",
    "               'range': (0,1) if not is_cider else (0, 10),\n",
    "               'xlog': is_bleu4 or is_cider,\n",
    "               **_kw}\n",
    "        plot_hists(exp, [(0, 0), (0, 1)], xlabel=False, ax=ax_hist1, **_kw)\n",
    "        plot_hists(exp, [(1, 1), (1, 0)], ax=ax_hist2, **_kw)\n",
    "\n",
    "        # Set titles\n",
    "        pretty_metric = get_pretty_metric(result.metric, metric_i=metric_i)\n",
    "        ax_hist1.set_title(f'{pretty_metric} distributions', fontsize=_kw['title_fontsize'])\n",
    "        ax1.set_title(f'{pretty_metric} in {exp.abnormality}', fontsize=_kw['title_fontsize'])\n",
    "\n",
    "        if i_plot == 0:\n",
    "            ax1.set_ylabel(exp.abnormality, fontsize=_kw['ylabel_fontsize'])\n",
    "        \n",
    "plt.tight_layout()\n",
    "\n",
    "# Set suptitle\n",
    "suptitle = f'Distributions in sentences from {\"IU X-ray\" if dataset == \"iu\" else \"MIMIC-CXR\"}'\n",
    "plt.suptitle(suptitle, fontsize=17, y=1.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_image_fpath = os.path.join(\n",
    "    FIGURES_DIR,\n",
    "    'nlp-vs-chex-2023-manyH-mimic-expert1.pdf'\n",
    ")\n",
    "\n",
    "fig.savefig(_image_fpath, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Plot many matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "GROUPS = [0, 1]\n",
    "# groups = [-2, 0, -1, 1]\n",
    "SAMPLER = 'random-gen_k500_n500'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_cubes_df(exp_by_abn, abnormalities, metrics, sampler=SAMPLER, groups=GROUPS):\n",
    "    cubes_df = pd.DataFrame(index=abnormalities, columns=metrics)\n",
    "\n",
    "    for abn in abnormalities:\n",
    "        exp = exp_by_abn[abn]\n",
    "\n",
    "        results = [\n",
    "            r\n",
    "            for r in exp.results\n",
    "            if (sampler is None or r.sampler == sampler) and \\\n",
    "                (groups is None or sorted(r.groups) == sorted(groups))\n",
    "        ]\n",
    "        # Not necessary to sort\n",
    "        # results = sorted(results, key=lambda x: keydict[x.metric])\n",
    "\n",
    "        if len(results) != len(metrics):\n",
    "            err = f'Diff amount of results than metrics: {abn}, {len(results)}, {len(metrics)}'\n",
    "            print(err)\n",
    "            continue\n",
    "\n",
    "        for result in results:\n",
    "            cubes_df.loc[abn, result.metric] = result.cube\n",
    "\n",
    "    assert cubes_df.isnull().any(axis=0).any(axis=0) == False\n",
    "    return cubes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_limits_per_abnormality(df, abns, metrics):\n",
    "    cubes_per_abn = defaultdict(list)\n",
    "    for abn in abns:\n",
    "        for m in metrics:\n",
    "            cube = df.loc[abn, m]\n",
    "            cubes_per_abn[abn].append(cube[0])\n",
    "            if m == 'bleu':\n",
    "                cubes_per_abn[abn].append(cube[3])\n",
    "    cubes_per_abn = {abn: np.array(l) for abn, l in cubes_per_abn.items()}\n",
    "    limits_per_abn = {abn: (cube.min(), cube.max()) for abn, cube in cubes_per_abn.items()}\n",
    "    return limits_per_abn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_many_matrices(exp_by_abn, abnormalities, metrics,\n",
    "                       bleu_all=False, groups=GROUPS, sampler=SAMPLER,\n",
    "                       outer=None, fig=None,\n",
    "                      ):\n",
    "    print('Building cube...')\n",
    "    cubes_df = build_cubes_df(exp_by_abn, abnormalities, metrics, groups=groups,\n",
    "                              sampler=sampler)\n",
    "\n",
    "    limits = get_limits_per_abnormality(cubes_df, abnormalities, ['bleu', 'rouge'])\n",
    "    # limits_cider = get_limits_per_abnormality(cubes_df, abns, ['cider-IDF'])\n",
    "    \n",
    "    TICKS = [KEY_TO_LABEL[k] for k in groups]\n",
    "\n",
    "    metrics_plotable = [(0, 'bleu')]\n",
    "    if bleu_all:\n",
    "        metrics_plotable.extend([(1, 'bleu'), (2, 'bleu')])\n",
    "    metrics_plotable.extend([(3, 'bleu'), (0, 'rouge'), (0, 'cider-IDF')])\n",
    "    \n",
    "    n_rows = len(abnormalities)\n",
    "    n_cols = len(metrics_plotable)\n",
    "\n",
    "    if outer is not None:\n",
    "        # Supports using both GridSpec and\n",
    "        if fig is None:\n",
    "            fig = plt.gcf()\n",
    "\n",
    "        inner = gridspec.GridSpecFromSubplotSpec(\n",
    "            n_rows, n_cols, subplot_spec=outer, wspace=0.15, hspace=0.1)\n",
    "        get_ax = lambda i, j: plt.Subplot(fig, inner[i, j])\n",
    "    else:\n",
    "        # f = plt.figure(figsize=(n_cols*5, n_rows*5))\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*5, n_rows*5))\n",
    "        get_ax = lambda i, j: axes[i][j]\n",
    "\n",
    "    for abn_i, abn in enumerate(abnormalities):\n",
    "        for metric_col_j, (metric_i, metric) in enumerate(metrics_plotable):\n",
    "            cube = cubes_df.loc[abn, metric]\n",
    "\n",
    "            # cbar params\n",
    "            include_cbar = (metric_col_j >= n_cols - 2) # rouge and CIDEr\n",
    "            if 'cider' in metric:\n",
    "                cbar_params = {'cmap': 'Blues'} # 'vmin': MIN_CIDER, 'vmax': MAX_CIDER, \n",
    "            else:\n",
    "                min_value, max_value = limits[abn]\n",
    "                cbar_params = {'vmin': min_value, 'vmax': max_value, 'cmap': 'YlOrRd'}\n",
    "                # cbar_params = {'vmin': 0, 'vmax': max_value + 0.2, 'cmap': 'YlOrRd'}\n",
    "\n",
    "            # plt.subplot(n_rows, n_cols, abn_i * n_cols + metric_col_j + 1)\n",
    "            ax = get_ax(abn_i, metric_col_j)\n",
    "            a = sns.heatmap(\n",
    "                cube[metric_i], annot=True, square=True,\n",
    "                xticklabels=TICKS, yticklabels=TICKS, fmt='.3f', # robust=True,\n",
    "                cbar=True,\n",
    "                annot_kws={'fontsize':15},\n",
    "                ax=ax,\n",
    "                **cbar_params,\n",
    "            )\n",
    "            a.set_xticklabels(a.get_xticklabels(), fontsize=13)\n",
    "            a.set_yticklabels(a.get_yticklabels(), fontsize=13)\n",
    "\n",
    "            title_metric = True # (abn_i == 0)\n",
    "            include_ylabel = (metric_col_j == 0)\n",
    "            include_xlabel = False # True # (abn_i == n_rows - 1)\n",
    "\n",
    "            if title_metric:\n",
    "                pretty_metric = get_pretty_metric(metric, metric_i=metric_i, include_range=True)\n",
    "                ax.set_title(pretty_metric, fontsize=18)\n",
    "\n",
    "            if include_xlabel:\n",
    "                ax.set_xlabel('Generated', fontsize=18)\n",
    "\n",
    "            if include_ylabel:\n",
    "                ax.set_ylabel(f'{abn}', fontsize=18) # \\nGround Truth\n",
    "                \n",
    "            if outer is not None:\n",
    "                fig.add_subplot(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# exp_by_abn_iu = load_experiments('iu')\n",
    "# exp_by_abn_mimic = load_experiments('mimic')\n",
    "len(exp_by_abn_iu), len(exp_by_abn_mimic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp_by_abn, dataset_name = exp_by_abn_iu, 'iu'\n",
    "# exp_by_abn, dataset_name = exp_by_abn_mimic, 'mimic'\n",
    "# abnormalities = ['Atelectasis', 'Cardiomegaly', 'Pleural Effusion']\n",
    "abns_half1 = CHEXPERT_DISEASES[1:7]\n",
    "abns_half2 = CHEXPERT_DISEASES[7:]\n",
    "metrics = ['bleu', 'rouge', 'cider-IDF']\n",
    "# groups = [0, 1]\n",
    "groups = [-2, -1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_kw = {\n",
    "    'metrics': metrics, 'bleu_all': True,\n",
    "    # 'save': True, # 'suptitley': 0.90,\n",
    "    'groups': groups,\n",
    "    'sampler': ('random-gen_k500_n500' if dataset_name == 'iu' else 'random-gen_k50_n100'),\n",
    "}\n",
    "\n",
    "n_abns = max(len(abns_half1), len(abns_half2))\n",
    "n_cols = 2 * 6\n",
    "fig = plt.figure(figsize=(n_cols * 5, n_abns * 5))\n",
    "outer = gridspec.GridSpec(1, 2, wspace=0.15, hspace=0.1)\n",
    "\n",
    "plot_many_matrices(exp_by_abn, abns_half1, outer=outer[0], fig=fig, **_kw)\n",
    "plot_many_matrices(exp_by_abn, abns_half2, outer=outer[1], fig=fig, **_kw)\n",
    "\n",
    "dataset = 'IU X-ray' if dataset_name == 'iu' else 'MIMIC-CXR'\n",
    "suptitle = f'Matrices for multiple abnormalities and NLP metrics ({dataset} dataset)'\n",
    "plt.suptitle(suptitle, fontsize=26, y=0.9)\n",
    "\n",
    "# HACK: there is a dummy bbox outside # remove it manually\n",
    "ax_dummy = fig.axes[0]\n",
    "if not bool(ax_dummy.get_label()) and not bool(ax_dummy.get_title()):\n",
    "    # (make sure it does not remove an important axis!)\n",
    "    ax_dummy.remove()\n",
    "\n",
    "_save = True\n",
    "_close = False\n",
    "\n",
    "_fig_fpath = os.path.join(\n",
    "    FIGURES_DIR,\n",
    "    f'nlp-vs-chex-all-{len(groups)}x{len(groups)}-{dataset_name}.pdf',\n",
    ")\n",
    "print(f'Filepath (save={_save}): {_fig_fpath}')\n",
    "if _save:\n",
    "    fig.savefig(_fig_fpath, bbox_inches='tight')\n",
    "    \n",
    "if _close:\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Plot matrices for each experiment in pdfs\n",
    "\n",
    "For each experiment, all its matrices in a PDF file\n",
    "\n",
    "DEPRECATED: generates too many PDFs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_exp_matrices(exp, target_groups=4, save=False, n_rows=2):\n",
    "    def _find_result(m):\n",
    "        ress = [\n",
    "            i\n",
    "            for i, r in enumerate(exp.results)\n",
    "            if r.metric == m and len(r.groups) == target_groups\n",
    "        ]\n",
    "        assert len(ress) == 1, ress\n",
    "        return ress[0]\n",
    "\n",
    "    bleu_result = _find_result('bleu')\n",
    "    targets = [\n",
    "        (bleu_result, 0),\n",
    "        (bleu_result, 1),\n",
    "        (bleu_result, 2),\n",
    "        (bleu_result, 3),\n",
    "        (_find_result('rouge'), 0),\n",
    "        (_find_result('cider-IDF'), 0),\n",
    "    ]\n",
    "\n",
    "    n_cols = math.ceil(len(targets) / n_rows)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows)) \n",
    "    axes = axes.flatten()\n",
    "\n",
    "    _kw = {'xlabel_fontsize': 14, 'ylabel_fontsize': 14, 'title_fontsize': 16}\n",
    "\n",
    "    for ax_i, (ax, (result_i, metric_i)) in enumerate(zip(axes, targets)):\n",
    "        plot_heatmap(exp, ax=ax, result_i=result_i, metric_i=metric_i,\n",
    "                     xlabel=(ax_i // n_cols + 1 == n_rows),\n",
    "                     ylabel=(ax_i % n_cols == 0),\n",
    "                     title=False, annot_kws={'fontsize':12}, **_kw)\n",
    "\n",
    "        pretty_metric = get_pretty_metric(exp[result_i].metric, metric_i=metric_i)\n",
    "        ax.set_title(pretty_metric, fontsize=_kw['title_fontsize'])\n",
    "\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), fontsize=12)\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), fontsize=12)\n",
    "\n",
    "    # Set suptitle\n",
    "    suptitle = f'{exp.abnormality} in {\"IU X-ray\" if exp.dataset == \"iu\" else \"MIMIC-CXR\"}'\n",
    "    plt.suptitle(suptitle, fontsize=20, y=0.94)\n",
    "\n",
    "    save = True\n",
    "\n",
    "    prefix = f'nlp-vs-chex-matrices-{target_groups}'\n",
    "    _image_fpath = os.path.join(\n",
    "        FIGURES_DIR, f'{prefix}-{ABN_SHORTCUTS[exp.abnormality].lower()}-{exp.dataset}.pdf',\n",
    "    )\n",
    "    print('Filepath: ', _image_fpath)\n",
    "    if save:\n",
    "        fig.savefig(_image_fpath, bbox_inches='tight')\n",
    "        \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# exp_by_abn = load_experiments('iu')\n",
    "exp_by_abn = load_experiments('mimic')\n",
    "len(exp_by_abn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for exp in exp_by_abn.values():\n",
    "    fig = plot_exp_matrices(exp)\n",
    "    plt.close(fig) # Prevent showing inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Plot matrices 2023 (BLEURT, BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp_by_abn = load_experiments('mimic-expert1')\n",
    "len(exp_by_abn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp_by_abn.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ABNORMALITIES = [\n",
    "    'Cardiomegaly',\n",
    "    'Atelectasis',\n",
    "    'Pleural Effusion',\n",
    "]\n",
    "\n",
    "PLOTABLE_METRICS = [\n",
    "    ('bleu', 0),\n",
    "    # ('rouge', 0),\n",
    "    ('cider-IDF', 0),\n",
    "    ('bleurt', 0),\n",
    "    ('bertscore', 2),\n",
    "]\n",
    "\n",
    "n_rows = len(ABNORMALITIES)\n",
    "n_cols = len(PLOTABLE_METRICS)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*5, n_rows*5))\n",
    "get_ax = lambda i, j: axes[i][j]\n",
    "\n",
    "for abn_i, abn in enumerate(ABNORMALITIES):\n",
    "    exp = exp_by_abn[abn]\n",
    "\n",
    "    for metric_col_j, (metric_name, metric_i) in enumerate(PLOTABLE_METRICS):\n",
    "        result_i = find_result_index(exp.results, metric_name)\n",
    "\n",
    "        result = exp.results[result_i]\n",
    "        ticks = [get_pretty_valuation(k) for k in result.groups]\n",
    "\n",
    "        # cbar params\n",
    "        include_cbar = (metric_col_j >= n_cols - 2) # rouge and CIDEr\n",
    "        cmap_name = 'Blues' if 'cider' in metric_name else 'YlOrRd'\n",
    "        # if 'cider' in metric:\n",
    "        #     cbar_params = {'cmap': 'Blues'} # 'vmin': MIN_CIDER, 'vmax': MAX_CIDER, \n",
    "        # else:\n",
    "        #     # min_value, max_value = limits[abn]\n",
    "        #   cbar_params = {'vmin': min_value, 'vmax': max_value, 'cmap': 'YlOrRd'}\n",
    "        #    # cbar_params = {'vmin': 0, 'vmax': max_value + 0.2, 'cmap': 'YlOrRd'}\n",
    "\n",
    "        ax = get_ax(abn_i, metric_col_j)\n",
    "        a = sns.heatmap(\n",
    "            result.cube[metric_i], annot=True, square=True,\n",
    "            xticklabels=ticks, yticklabels=ticks, fmt='.3f', # robust=True,\n",
    "            cbar=True,\n",
    "            annot_kws={'fontsize':15},\n",
    "            ax=ax,\n",
    "            cmap=cmap_name,\n",
    "        )\n",
    "        a.set_xticklabels(a.get_xticklabels(), fontsize=13)\n",
    "        a.set_yticklabels(a.get_yticklabels(), fontsize=13)\n",
    "\n",
    "        title_metric = True # (abn_i == 0)\n",
    "        include_ylabel = (metric_col_j == 0)\n",
    "        include_xlabel = False # True # (abn_i == n_rows - 1)\n",
    "\n",
    "        if title_metric:\n",
    "            pretty_metric = get_pretty_metric(metric_name, metric_i=metric_i, include_range=True)\n",
    "            ax.set_title(pretty_metric, fontsize=18)\n",
    "\n",
    "        if include_xlabel:\n",
    "            ax.set_xlabel('Generated', fontsize=18)\n",
    "\n",
    "        if include_ylabel:\n",
    "            ax.set_ylabel(f'{abn}', fontsize=18) # \\nGround Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_fig_fpath = os.path.join(\n",
    "    FIGURES_DIR,\n",
    "    'nlp-vs-chex-2023-many2x2-mimic-expert1.pdf',\n",
    ")\n",
    "fig.savefig(_fig_fpath, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Attempting to optimize threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Attempt 1\n",
    "\n",
    "(failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = exp[0]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# target1, target2 = (0, 0), (0, 1) # TN, FP (specificity)\n",
    "target1, target2 = (1, 1), (1, 0) # TP, FN (precision)\n",
    "\n",
    "arr1 = result.dists[target1]\n",
    "arr2 = result.dists[target2]\n",
    "arr1.shape, arr2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "assert target1[0] == target1[1]\n",
    "CORRECT = target1[0]\n",
    "INCORRECT = 1 - CORRECT\n",
    "\n",
    "merged = [(value, CORRECT) for value in arr1] + [(value, INCORRECT) for value in arr2]\n",
    "merged = sorted(merged, reverse=bool(not CORRECT))\n",
    "merged[:2], merged[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_threshs = []\n",
    "denominator = 0 # TP + FN\n",
    "numerator = 0 # TP\n",
    "for value, label in merged:\n",
    "    current_thresh = value\n",
    "    if label == CORRECT: # add 1 TP\n",
    "        numerator += 1\n",
    "\n",
    "    denominator += 1\n",
    "\n",
    "    all_threshs.append((current_thresh, numerator / denominator))\n",
    "all_threshs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max(all_threshs, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x, y = tuple(zip(*all_threshs))\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Attempt 2\n",
    "\n",
    "with sklearn\n",
    "Failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve as pr_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pred, gt = tuple(zip(*merged))\n",
    "pred = np.array(pred)\n",
    "gt = np.array(gt)\n",
    "pred.shape, gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pred /= 10 # CIDER re-scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "precision, recall, thresholds = pr_curve(gt, pred, pos_label=CORRECT)\n",
    "precision.shape, recall.shape, thresholds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f1 = divide_arrays(2*precision*recall, precision + recall)\n",
    "f1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "best_idx = f1.argmax()\n",
    "best_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "thresholds[best_idx], f1[best_idx], precision[best_idx], recall[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Attempt 3: accuracy/prec/recall\n",
    "\n",
    "CheXpert 4-class classification task --> is a binary classification task in NLP scores\n",
    "(i.e. NLP scores tell less information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp = load_experiment_pickle('mimic-cardiomegaly')\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = exp.results[-1]\n",
    "result.metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result.dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "merged = [\n",
    "    # Value, correct-or-not, original-key\n",
    "    (value, 1, (0, 0)) for value in result.dists[(0, 0)]\n",
    "] + [\n",
    "    (value, 1, (1, 1)) for value in result.dists[(1, 1)]\n",
    "] + [\n",
    "    (value, 0, (0, 1)) for value in result.dists[(0, 1)]\n",
    "] + [\n",
    "    (value, 0, (1, 0)) for value in result.dists[(1, 0)]\n",
    "]\n",
    "merged = sorted(merged)\n",
    "len(merged), merged[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_correct = sum(1 for _, correct, _ in merged if correct)\n",
    "n_incorrect = sum(1 for _, correct, _ in merged if not correct)\n",
    "n_correct, n_incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def smart_division(a, b):\n",
    "    if b == 0:\n",
    "        return 0\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_threshs = []\n",
    "\n",
    "# At first, the threshold is at 0\n",
    "# --> No negative predictions, all positive predictions\n",
    "# --> TN = FN = 0\n",
    "TP = sum(1 for _, correct, _ in merged if correct)\n",
    "FP = sum(1 for _, correct, _ in merged if not correct)\n",
    "TN, FN = 0, 0\n",
    "\n",
    "total = len(merged)\n",
    "\n",
    "assert TP + FP + FN + TN == total, f'Begin: {TP + FP + FN + TN} vs {total}'\n",
    "\n",
    "for value, correct, _ in merged:\n",
    "    current_thresh = value\n",
    "\n",
    "    if correct:\n",
    "        TP -= 1\n",
    "        FN += 1\n",
    "    else:\n",
    "        TN += 1\n",
    "        FP -= 1\n",
    "\n",
    "    assert TP + FP + FN + TN == total, f'Thresh={value}: {TP + FP + FN + TN} vs {total}'\n",
    "        \n",
    "    acc = (TP + TN) / total\n",
    "    prec = smart_division(TP, TP + FP)\n",
    "    recall = smart_division(TP, TP + FN)\n",
    "    f1 = smart_division(2*prec*recall, prec+recall)\n",
    "    spec = smart_division(TN, TN + FP)\n",
    "    npv = smart_division(TN, TN + FN)\n",
    "    f1_neg = smart_division(2*npv*spec, spec+npv)\n",
    "    CM = (TP, FN, FP, TN)\n",
    "\n",
    "    all_threshs.append({\n",
    "        'thresh': current_thresh,\n",
    "        'acc': acc,\n",
    "        'prec': prec,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'npv': npv,\n",
    "        'spec': spec,\n",
    "        'f1_neg': f1_neg,\n",
    "        'CM': CM,\n",
    "    })\n",
    "all_threshs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max(all_threshs, key=lambda x: x['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sl = lambda k: tuple(zip(*[(x['thresh'], x[k]) for x in all_threshs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "keys = ('prec', 'recall', 'acc', 'f1') # 'f1', \n",
    "# keys = ('acc', )\n",
    "# keys = ('npv', 'spec', 'f1_neg')\n",
    "for k in keys:\n",
    "    thresh, y = sl(k)\n",
    "    plt.plot(thresh, y, label=k)\n",
    "plt.legend()\n",
    "plt.xlabel('Thresh')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Optimize by')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "best = max(all_threshs, key=lambda x: x['acc'])\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_cm(cm, title=None):\n",
    "    TP, FN, FP, TN = cm\n",
    "    ticks = ['Entailment', 'Contradiction']\n",
    "    sns.heatmap([[TP, FN], [FP, TN]], annot=True, square=True, cmap='Blues',\n",
    "                xticklabels=ticks, yticklabels=ticks, fmt=',',\n",
    "               )\n",
    "    plt.ylabel('Real')\n",
    "    plt.xlabel('Scored by Metric')\n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_cm(best['CM'], title=f'CM for {exp.abnormality} with {get_pretty_metric(result.metric)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ROC-AUC\n",
    "Instead of thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_gt_pred_for_roc(result, metric_i=0, keys=None):\n",
    "    pred = []\n",
    "    gt = []\n",
    "    if keys is None:\n",
    "        keys = list(result.dists.keys())\n",
    "        \n",
    "    for a, b in keys:\n",
    "        elements = result.dists[(a, b)]\n",
    "        if elements.ndim == 2:\n",
    "            elements = elements[metric_i] # BLEU case\n",
    "        pred += list(elements)\n",
    "\n",
    "        entailment = int(a == b)\n",
    "        gt += [entailment] * len(elements)\n",
    "\n",
    "    return gt, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## One sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp = load_experiment_pickle('mimic-expert1-atelectasis')\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[(i, r.metric, r.groups, r.sampler) for i, r in enumerate(exp.results)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "RESULT_I = 19\n",
    "METRIC_I = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = exp.results[RESULT_I]\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result.dists.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d1 = result.dists[(1, 1)]\n",
    "d2 = result.dists[(1, 0)]\n",
    "d1.sum(), d2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gt, pred = prepare_gt_pred_for_roc(result,\n",
    "                                   # keys=[(0, 0), (0, 1)],\n",
    "                                   # keys=[(1, 1), (1, 0)],\n",
    "                                  )\n",
    "len(gt), len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(gt, pred)\n",
    "\n",
    "J_stat = tpr - fpr\n",
    "best_idx = J_stat.argmax()\n",
    "\n",
    "thresholds[best_idx], J_stat[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "roc = roc_auc_score(gt, pred)\n",
    "roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Compute AUC for all abnormalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_name = 'mimic-expert1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp_by_abn = load_experiments(dataset_name)\n",
    "len(exp_by_abn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show = True\n",
    "# target_sampler = None # 'random-gen_k500_n500'\n",
    "# target_sampler = 'random-gen_k100_n100'\n",
    "target_sampler = None\n",
    "target_groups = [0, 1] # [-2, -1, 0, 1]\n",
    "set_of_keys = [\n",
    "    [(0, 0), (0, 1)],\n",
    "    [(1, 1), (1, 0)],\n",
    "    [(0, 0), (0, 1), (1, 1), (1, 0)],\n",
    "]\n",
    "\n",
    "_METRIC_INDEXES = {\n",
    "    'bleu': [(0, 'bleu-1'), (3, 'bleu-4')],\n",
    "    'bertscore': [(2, 'bertscore-f1')],\n",
    "    # 'chexpert': [(0, 'chex-acc')], # has no samples!\n",
    "}\n",
    "metric_indexes = lambda name: _METRIC_INDEXES.get(name, [(0, name)])\n",
    "\n",
    "final_records = []\n",
    "\n",
    "for abnormality, exp in exp_by_abn.items():\n",
    "    for result in tqdm(exp.results, desc=abnormality, disable=not show):\n",
    "        if target_sampler is not None and not result.sampler.startswith(target_sampler):\n",
    "            continue\n",
    "        if target_groups is not None and sorted(result.groups) != target_groups:\n",
    "            continue\n",
    "        \n",
    "        for keys in set_of_keys:\n",
    "            for metric_i, name in metric_indexes(result.metric):\n",
    "                gt, pred = prepare_gt_pred_for_roc(result, metric_i=metric_i, keys=keys)\n",
    "                if len(gt) == 0 and len(pred) == 0 and result.metric == 'chexpert':\n",
    "                    continue\n",
    "                roc = roc_auc_score(gt, pred)\n",
    "\n",
    "                final_records.append((\n",
    "                    abnormality, name, result.groups, result.sampler, tuple(keys), roc,\n",
    "                ))\n",
    "\n",
    "len(final_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols = ['disease', 'metric', 'groups', 'sampler', 'keys', 'roc']\n",
    "df = pd.DataFrame(final_records, columns=cols)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Counter(df['groups']), Counter(df['sampler']), Counter(df['metric']), Counter(df['keys'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df.loc[df['groups'] == (0, 1)]\n",
    "df = df.loc[df['sampler'] != 'random-gen_k10_n10--1234']\n",
    "df = df.loc[df['sampler'] != 'random-gen_k50_n50--1234']\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_full = df.loc[df['keys'] == ((0, 0), (0, 1), (1, 1), (1, 0))]\n",
    "df_recall = df.loc[df['keys'] == ((1, 1), (1, 0))]\n",
    "df_spec = df.loc[df['keys'] == ((0, 0), (0, 1))]\n",
    "len(df_recall), len(df_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_renamer(replace_strs):\n",
    "    def _rename_run(run_name):\n",
    "        s = run_name\n",
    "        for target, replace_with in replace_strs:\n",
    "            s = re.sub(target, replace_with, s)\n",
    "        return s\n",
    "    return _rename_run\n",
    "\n",
    "def bold(s):\n",
    "    return '\\textbf{' + s + '}'\n",
    "\n",
    "shorten_cols = get_renamer([\n",
    "    ('cider-IDF', 'C-D'),\n",
    "    ('bleu-4', 'B-4'),\n",
    "    (r'bleu-1', 'B-1'),\n",
    "    ('rouge', 'R-L'),\n",
    "    ('disease', 'Abnormality'),\n",
    "])\n",
    "def latexify_cols(col):\n",
    "    return bold(shorten_cols(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def df_to_table(df):\n",
    "    if not (df.groupby(['disease', 'groups', 'metric', 'keys']).apply(len).values == 1).all():\n",
    "        print('Warning: more than one sampler per metric')\n",
    "        return None\n",
    "        \n",
    "    cols = list(df.columns)\n",
    "    metric_col = cols.index('metric')\n",
    "    roc_col = cols.index('roc')\n",
    "    df = df.groupby('disease').apply(lambda subdf: {\n",
    "        row[metric_col]: row[roc_col]\n",
    "        for row in list(subdf.values)\n",
    "    }).apply(pd.Series)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "table_full = df_to_table(df_full)\n",
    "table_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "table_recall = df_to_table(df_recall)\n",
    "table_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "table_spec = df_to_table(df_spec)\n",
    "table_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def table_to_latex(table):\n",
    "    s = table.reset_index().rename(columns=latexify_cols).to_latex(\n",
    "        float_format='%.3f',\n",
    "        escape=False,\n",
    "        index=False,\n",
    "        column_format='l' + 'c' * len(table.columns),\n",
    "    )\n",
    "    s = re.sub(r' +', ' ', s, flags=re.M)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# long_table = pd.concat([table_recall, table_spec], axis=1)\n",
    "table_to_latex(table_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ROC-AUC randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load results from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n ./nlp_in_chexpert_groups.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_RESULTS_FOLDER = os.path.join(WORKSPACE_DIR, 'report_generation', 'nlp-controlled-corpus')\n",
    "filenames = [\n",
    "    fname\n",
    "    for fname in os.listdir(_RESULTS_FOLDER)\n",
    "    if fname.startswith('roc-random-mimic-expert1')\n",
    "]\n",
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "for fname in filenames:\n",
    "    results = pd.concat([results, pd.read_csv(os.path.join(_RESULTS_FOLDER, fname))], axis=0)\n",
    "print(len(results))\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_GROUP_BY = ['abnormality', 'metric_name', 'metric_i', 'task']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_means = results.groupby(_GROUP_BY)['roc'].mean()\n",
    "roc_lower = results.groupby(_GROUP_BY)['roc'].apply(lambda s: np.percentile(s.values, 2.5))\n",
    "roc_lower.name = 'roc_lower'\n",
    "roc_upper = results.groupby(_GROUP_BY)['roc'].apply(lambda s: np.percentile(s.values, 97.5))\n",
    "roc_upper.name = 'roc_upper'\n",
    "len(roc_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocs_with_ci = pd.concat([roc_means, roc_lower, roc_upper], axis=1)\n",
    "rocs_with_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocs_with_ci.loc[(slice(None), slice(None), slice(None), 'spec'), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print to table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_SHOW_CI = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS = ['full', 'recall', 'spec']\n",
    "index = pd.MultiIndex.from_product([TASKS, CHEXPERT_LABELS_6], names=['abnormality', 'task'])\n",
    "columns = ['BLEU-1', 'BLEU-4', 'ROUGE-L', 'CIDEr-D', 'BLEURT', 'BERTscore-F1', 'CheXpert-acc']\n",
    "out_df = pd.DataFrame(index=index, columns=columns)\n",
    "for _, row in rocs_with_ci.reset_index().iterrows():\n",
    "    metric = get_pretty_metric(row.metric_name, row.metric_i)\n",
    "    \n",
    "    roc_str = f'{row.roc:.3f}'\n",
    "    if _SHOW_CI:\n",
    "        roc_str += f' [{row.roc_lower:.3f},{row.roc_upper:.3f}]'\n",
    "        \n",
    "    if row.abnormality == 'Atelectasis' and row.task == 'recall':\n",
    "        bold = row.metric_name == 'bleurt'\n",
    "    else:\n",
    "        bold = row.metric_name == 'chexpert'\n",
    "    \n",
    "    if bold:\n",
    "        roc_str = '\\\\textbf{%s}' % roc_str\n",
    "    out_df.loc[(row.task, row.abnormality), metric] = roc_str\n",
    "\n",
    "out_df = out_df.sort_index()\n",
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_SUB_HEADERS = {\n",
    "'full': '%% full\\n\\\\\\\\midrule\\n\\\\\\\\multicolumn{8}{c}{\\\\\\\\textbf{Discriminate correct samples}: \\\\\\\\fullLikeProba{}} \\\\\\\\\\\\\\\\',\n",
    "'recall': '%% recall\\n\\\\\\\\midrule\\n\\\\\\\\multicolumn{8}{c}{\\\\\\\\textbf{Discriminate abnormal}: \\\\\\\\recallLikeProba{}} \\\\\\\\\\\\\\\\',\n",
    "'spec': '%% spec\\n\\\\\\\\midrule\\n\\\\\\\\multicolumn{8}{c}{\\\\\\\\textbf{Discriminate healthy}: \\\\\\\\specLikeProba{}} \\\\\\\\\\\\\\\\',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_latex(df):\n",
    "    s = df.style.to_latex()\n",
    "    s = re.sub(r'\\\\multirow\\[c\\]\\{\\d+\\}\\{\\*\\}\\{(\\w+)\\}', r'\\n%% \\1 \\n', s)\n",
    "    for k, v in _SUB_HEADERS.items():\n",
    "        s = re.sub(f'\\%\\% {k}', v, s)\n",
    "    s = re.sub(r'multicolumn\\{\\d\\}', 'multicolumn{' + str(len(out_df[cols2].columns) + 1) + '}', s)\n",
    "    s = re.sub(r'^\\s+&\\s+', '    ', s, flags=re.MULTILINE)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols1 = ['BLEU-1', 'BLEU-4', 'ROUGE-L', 'CheXpert-acc']\n",
    "cols2 = ['CIDEr-D', 'BLEURT', 'BERTscore-F1', 'CheXpert-acc']\n",
    "to_latex(out_df[cols1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, mannwhitneyu, f_oneway, kruskal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# exp = load_experiment_pickle('mimic-cardiomegaly')\n",
    "len(exp.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_heatmap(exp, result_i=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "EXP_I = -1\n",
    "result = exp[EXP_I]\n",
    "result.metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "key1 = (0, 0)\n",
    "key2 = (0, 1)\n",
    "group1 = result.dists[key1]\n",
    "group2 = result.dists[key2]\n",
    "if result.metric == 'bleu':\n",
    "    group1 = group1[0]\n",
    "    group2 = group2[0]\n",
    "group1.shape, group2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_hists(exp, [key1, key2], result_i=EXP_I, bins=50, range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r = mannwhitneyu(group1, group2)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r = ttest_ind(group1, group2, equal_var=False)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "groups = [result.dists[k] for k in [(0, 0), (0, 1), (1, 0), (1, 1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "anova = f_oneway(*groups)\n",
    "anova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "kru = kruskal(*groups)\n",
    "kru"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

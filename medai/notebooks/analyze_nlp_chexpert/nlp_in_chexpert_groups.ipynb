{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "import importlib\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.facecolor'] = 'white'\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.rcParams.update({'font.family': 'serif', 'font.sans-serif': ['CMU', 'Helvetica']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../utils/__init__.py\n",
    "config_logging(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../datasets/common/constants.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n ./nlp_in_chexpert_groups.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load experiments\n",
    "def load_experiments(dataset_name):\n",
    "    exp_by_abn = {}\n",
    "    errors = defaultdict(list)\n",
    "    for abnormality in CHEXPERT_DISEASES[1:]:\n",
    "        fname = f'{dataset_name}-{abnormality.replace(\" \", \"-\").lower()}'\n",
    "        if not exist_experiment_pickle(fname):\n",
    "            errors['not-found'].append(fname)\n",
    "            continue\n",
    "        exp = load_experiment_pickle(fname)\n",
    "        exp_by_abn[abnormality] = exp\n",
    "        \n",
    "    if len(errors['not-found']):\n",
    "        print('Not found: ', errors['not-found'])\n",
    "        \n",
    "    return exp_by_abn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Debug running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run -n ./nlp_in_chexpert_groups.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_info = init_dataset_info('iu')\n",
    "dataset_info.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp = init_experiment('Cardiomegaly', dataset_info)\n",
    "exp_LO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "kwargs = {\n",
    "    # 'metric': 'bleu',\n",
    "    # 'metric': 'rouge',\n",
    "    'metric': 'cider-IDF',\n",
    "    'k_times': 500,\n",
    "    # 'k_gts': 1,\n",
    "    'max_n': 500,\n",
    "}\n",
    "exp.append(calc_score_matrices(exp.grouped_2, dataset_info, groups=(0, 1), **kwargs))\n",
    "# exp.append(calc_score_matrices(exp.grouped, dataset_info, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp[-1].cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp = load_experiment_pickle('iu-lung-opacity')\n",
    "exp = load_experiment_pickle('mimic-cardiomegaly')\n",
    "# exp = load_experiment_pickle('iu-cardiomegaly')\n",
    "# exp = load_experiment_pickle('mimic-lung-lesion')\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i, r.metric, r.groups) for i, r in enumerate(exp.results)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_I = 1\n",
    "METRIC_I = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_heatmap(exp, result_i=RESULT_I, metric_i=METRIC_I, annot_kws={'fontsize':13})\n",
    "\n",
    "# if len(exp.results) > 1:\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plot_heatmap(exp, result_i=-2, metric_i=METRIC_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n ./nlp_in_chexpert_groups.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_image_fpath(exp, result_i, metric_i, suffix=''):\n",
    "    metric = exp.results[result_i].metric\n",
    "    if metric == 'bleu':\n",
    "        metric += f'{metric_i+1}'\n",
    "    abn = ABN_SHORTCUTS[exp.abnormality].lower()\n",
    "    data = exp.dataset\n",
    "\n",
    "    name = f'nlp-vs-chex-{metric}-{abn}-{data}'\n",
    "    if suffix:\n",
    "        name += f'-{suffix}'\n",
    "    \n",
    "    fpath = os.path.join(FIGURES_DIR, f'{name}.pdf')\n",
    "    \n",
    "    print('Filepath: ', fpath)\n",
    "    \n",
    "    return fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_suptitle(exp, result_i, metric_i):\n",
    "    result = exp.results[result_i]\n",
    "    pretty_metric = get_pretty_metric(result.metric, metric_i=metric_i)\n",
    "    dataset = 'IU X-ray' if exp.dataset == 'iu' else 'MIMIC-CXR'\n",
    "    return f'{pretty_metric} in {exp.abnormality} sentences ({dataset} dataset)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 6))\n",
    "shape = (2, 2) # Axes shape\n",
    "ax1 = plt.subplot2grid(shape, (0, 0), rowspan=2)\n",
    "ax_hist1 = plt.subplot2grid(shape, (0, 1))\n",
    "ax_hist2 = plt.subplot2grid(shape, (1, 1))\n",
    "\n",
    "_kw = {'xlabel_fontsize': 14, 'ylabel_fontsize': 14, 'title_fontsize': 15,\n",
    "       'result_i': RESULT_I, 'metric_i': METRIC_I,\n",
    "      }\n",
    "plot_heatmap(exp, ax=ax1, title=False, annot_kws={'fontsize':13}, **_kw)\n",
    "\n",
    "_kw = {'add_n_to_label': False, 'bins': 50, 'legend_fontsize': 12,\n",
    "       'range': (0,1),\n",
    "       **_kw}\n",
    "plot_hists(exp,\n",
    "    [\n",
    "        (0, 0), (0, 1),\n",
    "    ], title=False, xlabel=False, ax=ax_hist1, **_kw)\n",
    "plot_hists(exp,\n",
    "    [\n",
    "        (1, 1), (1, 0),\n",
    "    ], title=False, ax=ax_hist2, **_kw) # , range=(0,2)\n",
    "\n",
    "# Set suptitle\n",
    "plt.suptitle(build_suptitle(exp, RESULT_I, METRIC_I), fontsize=17)\n",
    "\n",
    "# Set titles\n",
    "ax_hist1.set_title('Scores distribution', fontsize=_kw['title_fontsize'])\n",
    "ax1.set_title('Scores matrix', fontsize=_kw['title_fontsize'])\n",
    "\n",
    "_LOG_SCALE = False\n",
    "_SAVE = True\n",
    "\n",
    "if _LOG_SCALE:\n",
    "    ax_hist1.set_yscale('log')\n",
    "    ax_hist2.set_yscale('log')\n",
    "\n",
    "# increase fontsize of ticks in the first plot (HACKy way)\n",
    "a = ax1.figure.axes[0] # get the first plot\n",
    "a.set_xticklabels(a.get_xticklabels(), fontsize=12)\n",
    "a.set_yticklabels(a.get_yticklabels(), fontsize=12)\n",
    "\n",
    "_image_fpath = build_image_fpath(exp, RESULT_I, METRIC_I,\n",
    "                                 suffix='logscale' if _LOG_SCALE else '')\n",
    "if _SAVE:\n",
    "    ax1.figure.savefig(_image_fpath, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot many matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUPS = [0, 1]\n",
    "# groups = [-2, 0, -1, 1]\n",
    "SAMPLER = 'random-gen_k500_n500'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keydict = { metric: i for i, metric in enumerate(metrics) } # Not necessary\n",
    "\n",
    "def build_cubes_df(exp_by_abn, abnormalities, metrics, sampler=SAMPLER, groups=GROUPS):\n",
    "    cubes_df = pd.DataFrame(index=abnormalities, columns=metrics)\n",
    "\n",
    "    for abn in abnormalities:\n",
    "        exp = exp_by_abn[abn]\n",
    "\n",
    "        results = [\n",
    "            r\n",
    "            for r in exp.results\n",
    "            if (sampler is None or r.sampler == sampler) and \\\n",
    "                (groups is None or sorted(r.groups) == sorted(groups))\n",
    "        ]\n",
    "        # Not necessary to sort\n",
    "        # results = sorted(results, key=lambda x: keydict[x.metric])\n",
    "\n",
    "        if len(results) != len(metrics):\n",
    "            err = f'Diff amount of results than metrics: {abn}, {len(results)}, {len(metrics)}'\n",
    "            print(err)\n",
    "            continue\n",
    "\n",
    "        for result in results:\n",
    "            cubes_df.loc[abn, result.metric] = result.cube\n",
    "\n",
    "    assert cubes_df.isnull().any(axis=0).any(axis=0) == False\n",
    "    return cubes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_limits_per_abnormality(df, abns, metrics):\n",
    "    cubes_per_abn = defaultdict(list)\n",
    "    for abn in abns:\n",
    "        for m in metrics:\n",
    "            cube = df.loc[abn, m]\n",
    "            cubes_per_abn[abn].append(cube[0])\n",
    "            if m == 'bleu':\n",
    "                cubes_per_abn[abn].append(cube[3])\n",
    "    cubes_per_abn = {abn: np.array(l) for abn, l in cubes_per_abn.items()}\n",
    "    limits_per_abn = {abn: (cube.min(), cube.max()) for abn, cube in cubes_per_abn.items()}\n",
    "    return limits_per_abn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_many_matrices(exp_by_abn, abnormalities, metrics,\n",
    "                       bleu_all=False, groups=GROUPS, sampler=SAMPLER,\n",
    "                       outer=None, fig=None,\n",
    "                      ):    \n",
    "    print('Building cube...')\n",
    "    cubes_df = build_cubes_df(exp_by_abn, abnormalities, metrics, groups=groups,\n",
    "                              sampler=sampler)\n",
    "\n",
    "    limits = get_limits_per_abnormality(cubes_df, abnormalities, ['bleu', 'rouge'])\n",
    "    # limits_cider = get_limits_per_abnormality(cubes_df, abns, ['cider-IDF'])\n",
    "    \n",
    "    TICKS = [KEY_TO_LABEL[k] for k in groups]\n",
    "\n",
    "    metrics_plotable = [(0, 'bleu')]\n",
    "    if bleu_all:\n",
    "        metrics_plotable.extend([(1, 'bleu'), (2, 'bleu')])\n",
    "    metrics_plotable.extend([(3, 'bleu'), (0, 'rouge'), (0, 'cider-IDF')])\n",
    "    \n",
    "    n_rows = len(abnormalities)\n",
    "    n_cols = len(metrics_plotable)\n",
    "\n",
    "    if outer is not None:\n",
    "        # Supports using both GridSpec and\n",
    "        if fig is None:\n",
    "            fig = plt.gcf()\n",
    "\n",
    "        inner = gridspec.GridSpecFromSubplotSpec(\n",
    "            n_rows, n_cols, subplot_spec=outer, wspace=0.15, hspace=0.1)\n",
    "        get_ax = lambda i, j: plt.Subplot(fig, inner[i, j])\n",
    "    else:\n",
    "        # f = plt.figure(figsize=(n_cols*5, n_rows*5))\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*5, n_rows*5))\n",
    "        get_ax = lambda i, j: axes[i][j]\n",
    "\n",
    "    for abn_i, abn in enumerate(abnormalities):\n",
    "        for metric_col_j, (metric_i, metric) in enumerate(metrics_plotable):\n",
    "            cube = cubes_df.loc[abn, metric]\n",
    "\n",
    "            # cbar params\n",
    "            include_cbar = (metric_col_j >= n_cols - 2) # rouge and CIDEr\n",
    "            if 'cider' in metric:\n",
    "                cbar_params = {'cmap': 'Blues'} # 'vmin': MIN_CIDER, 'vmax': MAX_CIDER, \n",
    "            else:\n",
    "                min_value, max_value = limits[abn]\n",
    "                cbar_params = {'vmin': min_value, 'vmax': max_value, 'cmap': 'YlOrRd'}\n",
    "                # cbar_params = {'vmin': 0, 'vmax': max_value + 0.2, 'cmap': 'YlOrRd'}\n",
    "\n",
    "            # plt.subplot(n_rows, n_cols, abn_i * n_cols + metric_col_j + 1)\n",
    "            ax = get_ax(abn_i, metric_col_j)\n",
    "            a = sns.heatmap(\n",
    "                cube[metric_i], annot=True, square=True,\n",
    "                xticklabels=TICKS, yticklabels=TICKS, fmt='.3f', # robust=True,\n",
    "                cbar=True,\n",
    "                annot_kws={'fontsize':15},\n",
    "                ax=ax,\n",
    "                **cbar_params,\n",
    "            )\n",
    "            a.set_xticklabels(a.get_xticklabels(), fontsize=13)\n",
    "            a.set_yticklabels(a.get_yticklabels(), fontsize=13)\n",
    "\n",
    "            title_metric = True # (abn_i == 0)\n",
    "            include_ylabel = (metric_col_j == 0)\n",
    "            include_xlabel = False # True # (abn_i == n_rows - 1)\n",
    "\n",
    "            if title_metric:\n",
    "                pretty_metric = get_pretty_metric(metric, metric_i=metric_i, include_range=True)\n",
    "                ax.set_title(pretty_metric, fontsize=18)\n",
    "\n",
    "            if include_xlabel:\n",
    "                ax.set_xlabel('Generated', fontsize=18)\n",
    "\n",
    "            if include_ylabel:\n",
    "                ax.set_ylabel(f'{abn}', fontsize=18) # \\nGround Truth\n",
    "                \n",
    "            if outer is not None:\n",
    "                fig.add_subplot(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_by_abn_iu = load_experiments('iu')\n",
    "# exp_by_abn_mimic = load_experiments('mimic')\n",
    "len(exp_by_abn_iu), len(exp_by_abn_mimic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_by_abn, dataset_name = exp_by_abn_iu, 'iu'\n",
    "# exp_by_abn, dataset_name = exp_by_abn_mimic, 'mimic'\n",
    "# abnormalities = ['Atelectasis', 'Cardiomegaly', 'Pleural Effusion']\n",
    "abns_half1 = CHEXPERT_DISEASES[1:7]\n",
    "abns_half2 = CHEXPERT_DISEASES[7:]\n",
    "metrics = ['bleu', 'rouge', 'cider-IDF']\n",
    "# groups = [0, 1]\n",
    "groups = [-2, -1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_kw = {\n",
    "    'metrics': metrics, 'bleu_all': True,\n",
    "    # 'save': True, # 'suptitley': 0.90,\n",
    "    'groups': groups,\n",
    "    'sampler': ('random-gen_k500_n500' if dataset_name == 'iu' else 'random-gen_k50_n100'),\n",
    "}\n",
    "\n",
    "n_abns = max(len(abns_half1), len(abns_half2))\n",
    "n_cols = 2 * 6\n",
    "fig = plt.figure(figsize=(n_cols * 5, n_abns * 5))\n",
    "outer = gridspec.GridSpec(1, 2, wspace=0.15, hspace=0.1)\n",
    "\n",
    "plot_many_matrices(exp_by_abn, abns_half1, outer=outer[0], fig=fig, **_kw)\n",
    "plot_many_matrices(exp_by_abn, abns_half2, outer=outer[1], fig=fig, **_kw)\n",
    "\n",
    "dataset = 'IU X-ray' if dataset_name == 'iu' else 'MIMIC-CXR'\n",
    "suptitle = f'Matrices for multiple abnormalities and NLP metrics ({dataset} dataset)'\n",
    "plt.suptitle(suptitle, fontsize=26, y=0.9)\n",
    "\n",
    "# HACK: there is a dummy bbox outside # remove it manually\n",
    "ax_dummy = fig.axes[0]\n",
    "if not bool(ax_dummy.get_label()) and not bool(ax_dummy.get_title()):\n",
    "    # (make sure it does not remove an important axis!)\n",
    "    ax_dummy.remove()\n",
    "\n",
    "_save = True\n",
    "_close = False\n",
    "\n",
    "_fig_fpath = os.path.join(\n",
    "    FIGURES_DIR,\n",
    "    f'nlp-vs-chex-all-{len(groups)}x{len(groups)}-{dataset_name}.pdf',\n",
    ")\n",
    "print(f'Filepath (save={_save}): {_fig_fpath}')\n",
    "if _save:\n",
    "    fig.savefig(_fig_fpath, bbox_inches='tight')\n",
    "    \n",
    "if _close:\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _save_many_to_file(fig, groups, abnormalities, metrics_plotable, suffix='', save=False):\n",
    "#     # Build image_fpath\n",
    "#     _fig_fname = '-'.join(s for s in [\n",
    "#         'nlp-vs-chex-many',\n",
    "#         f'{len(groups)}x{len(groups)}',\n",
    "#         dataset_name,\n",
    "#         f'{len(abnormalities)}A',\n",
    "#         f'{len(metrics)}M',\n",
    "#         suffix,\n",
    "#     ] if s)\n",
    "#     _fig_fpath = os.path.join(\n",
    "#         FIGURES_DIR,\n",
    "#         f'{_fig_fname}.pdf',\n",
    "#     )\n",
    "#     print(f'Filepath (save={save}): {_fig_fpath}')\n",
    "#     if save:\n",
    "#         fig.savefig(_fig_fpath, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Plot matrices for each experiment in pdfs\n",
    "\n",
    "For each experiment, all its matrices in a PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_exp_matrices(exp, target_groups=4, save=False, n_rows=2):\n",
    "    def _find_result(m):\n",
    "        ress = [\n",
    "            i\n",
    "            for i, r in enumerate(exp.results)\n",
    "            if r.metric == m and len(r.groups) == target_groups\n",
    "        ]\n",
    "        assert len(ress) == 1, ress\n",
    "        return ress[0]\n",
    "\n",
    "    bleu_result = _find_result('bleu')\n",
    "    targets = [\n",
    "        (bleu_result, 0),\n",
    "        (bleu_result, 1),\n",
    "        (bleu_result, 2),\n",
    "        (bleu_result, 3),\n",
    "        (_find_result('rouge'), 0),\n",
    "        (_find_result('cider-IDF'), 0),\n",
    "    ]\n",
    "\n",
    "    n_cols = math.ceil(len(targets) / n_rows)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows)) \n",
    "    axes = axes.flatten()\n",
    "\n",
    "    _kw = {'xlabel_fontsize': 14, 'ylabel_fontsize': 14, 'title_fontsize': 16}\n",
    "\n",
    "    for ax_i, (ax, (result_i, metric_i)) in enumerate(zip(axes, targets)):\n",
    "        plot_heatmap(exp, ax=ax, result_i=result_i, metric_i=metric_i,\n",
    "                     xlabel=(ax_i // n_cols + 1 == n_rows),\n",
    "                     ylabel=(ax_i % n_cols == 0),\n",
    "                     title=False, annot_kws={'fontsize':12}, **_kw)\n",
    "\n",
    "        pretty_metric = get_pretty_metric(exp[result_i].metric, metric_i=metric_i)\n",
    "        ax.set_title(pretty_metric, fontsize=_kw['title_fontsize'])\n",
    "\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), fontsize=12)\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), fontsize=12)\n",
    "\n",
    "    # Set suptitle\n",
    "    suptitle = f'{exp.abnormality} in {\"IU X-ray\" if exp.dataset == \"iu\" else \"MIMIC-CXR\"}'\n",
    "    plt.suptitle(suptitle, fontsize=20, y=0.94)\n",
    "\n",
    "    save = True\n",
    "\n",
    "    prefix = f'nlp-vs-chex-matrices-{target_groups}'\n",
    "    _image_fpath = os.path.join(\n",
    "        FIGURES_DIR, f'{prefix}-{ABN_SHORTCUTS[exp.abnormality].lower()}-{exp.dataset}.pdf',\n",
    "    )\n",
    "    print('Filepath: ', _image_fpath)\n",
    "    if save:\n",
    "        fig.savefig(_image_fpath, bbox_inches='tight')\n",
    "        \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# exp_by_abn = load_experiments('iu')\n",
    "exp_by_abn = load_experiments('mimic')\n",
    "len(exp_by_abn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for exp in exp_by_abn.values():\n",
    "    fig = plot_exp_matrices(exp)\n",
    "    plt.close(fig) # Prevent showing inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempting to optimize threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Attempt 1\n",
    "\n",
    "(failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = exp[0]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# target1, target2 = (0, 0), (0, 1) # TN, FP (specificity)\n",
    "target1, target2 = (1, 1), (1, 0) # TP, FN (precision)\n",
    "\n",
    "arr1 = result.dists[target1]\n",
    "arr2 = result.dists[target2]\n",
    "arr1.shape, arr2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "assert target1[0] == target1[1]\n",
    "CORRECT = target1[0]\n",
    "INCORRECT = 1 - CORRECT\n",
    "\n",
    "merged = [(value, CORRECT) for value in arr1] + [(value, INCORRECT) for value in arr2]\n",
    "merged = sorted(merged, reverse=bool(not CORRECT))\n",
    "merged[:2], merged[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_threshs = []\n",
    "denominator = 0 # TP + FN\n",
    "numerator = 0 # TP\n",
    "for value, label in merged:\n",
    "    current_thresh = value\n",
    "    if label == CORRECT: # add 1 TP\n",
    "        numerator += 1\n",
    "\n",
    "    denominator += 1\n",
    "\n",
    "    all_threshs.append((current_thresh, numerator / denominator))\n",
    "all_threshs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max(all_threshs, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x, y = tuple(zip(*all_threshs))\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Attempt 2\n",
    "\n",
    "with sklearn\n",
    "Failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve as pr_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pred, gt = tuple(zip(*merged))\n",
    "pred = np.array(pred)\n",
    "gt = np.array(gt)\n",
    "pred.shape, gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pred /= 10 # CIDER re-scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "precision, recall, thresholds = pr_curve(gt, pred, pos_label=CORRECT)\n",
    "precision.shape, recall.shape, thresholds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f1 = divide_arrays(2*precision*recall, precision + recall)\n",
    "f1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "best_idx = f1.argmax()\n",
    "best_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "thresholds[best_idx], f1[best_idx], precision[best_idx], recall[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Attempt 3: accuracy/prec/recall\n",
    "\n",
    "CheXpert 4-class classification task --> is a binary classification task in NLP scores\n",
    "(i.e. NLP scores tell less information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp = load_experiment_pickle('mimic-cardiomegaly')\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = exp.results[-1]\n",
    "result.metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result.dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "merged = [\n",
    "    # Value, correct-or-not, original-key\n",
    "    (value, 1, (0, 0)) for value in result.dists[(0, 0)]\n",
    "] + [\n",
    "    (value, 1, (1, 1)) for value in result.dists[(1, 1)]\n",
    "] + [\n",
    "    (value, 0, (0, 1)) for value in result.dists[(0, 1)]\n",
    "] + [\n",
    "    (value, 0, (1, 0)) for value in result.dists[(1, 0)]\n",
    "]\n",
    "merged = sorted(merged)\n",
    "len(merged), merged[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_correct = sum(1 for _, correct, _ in merged if correct)\n",
    "n_incorrect = sum(1 for _, correct, _ in merged if not correct)\n",
    "n_correct, n_incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def smart_division(a, b):\n",
    "    if b == 0:\n",
    "        return 0\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_threshs = []\n",
    "\n",
    "# At first, the threshold is at 0\n",
    "# --> No negative predictions, all positive predictions\n",
    "# --> TN = FN = 0\n",
    "TP = sum(1 for _, correct, _ in merged if correct)\n",
    "FP = sum(1 for _, correct, _ in merged if not correct)\n",
    "TN, FN = 0, 0\n",
    "\n",
    "total = len(merged)\n",
    "\n",
    "assert TP + FP + FN + TN == total, f'Begin: {TP + FP + FN + TN} vs {total}'\n",
    "\n",
    "for value, correct, _ in merged:\n",
    "    current_thresh = value\n",
    "\n",
    "    if correct:\n",
    "        TP -= 1\n",
    "        FN += 1\n",
    "    else:\n",
    "        TN += 1\n",
    "        FP -= 1\n",
    "\n",
    "    assert TP + FP + FN + TN == total, f'Thresh={value}: {TP + FP + FN + TN} vs {total}'\n",
    "        \n",
    "    acc = (TP + TN) / total\n",
    "    prec = smart_division(TP, TP + FP)\n",
    "    recall = smart_division(TP, TP + FN)\n",
    "    f1 = smart_division(2*prec*recall, prec+recall)\n",
    "    spec = smart_division(TN, TN + FP)\n",
    "    npv = smart_division(TN, TN + FN)\n",
    "    f1_neg = smart_division(2*npv*spec, spec+npv)\n",
    "    CM = (TP, FN, FP, TN)\n",
    "\n",
    "    all_threshs.append({\n",
    "        'thresh': current_thresh,\n",
    "        'acc': acc,\n",
    "        'prec': prec,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'npv': npv,\n",
    "        'spec': spec,\n",
    "        'f1_neg': f1_neg,\n",
    "        'CM': CM,\n",
    "    })\n",
    "all_threshs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max(all_threshs, key=lambda x: x['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sl = lambda k: tuple(zip(*[(x['thresh'], x[k]) for x in all_threshs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "keys = ('prec', 'recall', 'acc', 'f1') # 'f1', \n",
    "# keys = ('acc', )\n",
    "# keys = ('npv', 'spec', 'f1_neg')\n",
    "for k in keys:\n",
    "    thresh, y = sl(k)\n",
    "    plt.plot(thresh, y, label=k)\n",
    "plt.legend()\n",
    "plt.xlabel('Thresh')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Optimize by')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "best = max(all_threshs, key=lambda x: x['acc'])\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_cm(cm, title=None):\n",
    "    TP, FN, FP, TN = cm\n",
    "    ticks = ['Entailment', 'Contradiction']\n",
    "    sns.heatmap([[TP, FN], [FP, TN]], annot=True, square=True, cmap='Blues',\n",
    "                xticklabels=ticks, yticklabels=ticks, fmt=',',\n",
    "               )\n",
    "    plt.ylabel('Real')\n",
    "    plt.xlabel('Scored by Metric')\n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_cm(best['CM'], title=f'CM for {exp.abnormality} with {get_pretty_metric(result.metric)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Attempt 4: use AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def prepare_gt_pred_for_roc(result, metric_i=0, keys=None):\n",
    "    pred = []\n",
    "    gt = []\n",
    "    if keys is None:\n",
    "        keys = list(result.dists.keys())\n",
    "\n",
    "    for a, b in keys:\n",
    "        elements = result.dists[(a, b)]\n",
    "        if elements.ndim == 2:\n",
    "            elements = elements[metric_i] # BLEU case\n",
    "        pred += list(elements)\n",
    "\n",
    "        entailment = int(a == b)\n",
    "        gt += [entailment] * len(elements)\n",
    "\n",
    "    return gt, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = exp.results[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gt, pred = prepare_gt_pred_for_roc(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(gt, pred)\n",
    "\n",
    "J_stat = tpr - fpr\n",
    "best_idx = J_stat.argmax()\n",
    "\n",
    "thresholds[best_idx], J_stat[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "roc = roc_auc_score(gt, pred)\n",
    "roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Compute AUC for all abnormalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_name = 'mimic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp_by_abn_iu = load_experiments(dataset_name)\n",
    "len(exp_by_abn_iu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show = True\n",
    "target_sampler = None # 'random-gen_k500_n500'\n",
    "target_groups = [0, 1] # [-2, -1, 0, 1]\n",
    "# keys = [(0, 0), (0, 1), (1, 1), (1, 0)]\n",
    "# keys = None\n",
    "set_of_keys = [\n",
    "    ((0, 0), (0, 1)),\n",
    "    ((1, 1), (1, 0)),\n",
    "]\n",
    "\n",
    "final_records = []\n",
    "\n",
    "for abnormality in CHEXPERT_DISEASES[1:]:\n",
    "    if abnormality not in exp_by_abn_iu:\n",
    "        continue\n",
    "    exp = exp_by_abn_iu[abnormality]\n",
    "\n",
    "    for result in tqdm(exp.results, desc=abnormality, disable=not show):\n",
    "        if target_sampler is not None and result.sampler != target_sampler:\n",
    "            continue\n",
    "        if target_groups is not None and sorted(result.groups) != target_groups:\n",
    "            continue\n",
    "        \n",
    "        for keys in set_of_keys:\n",
    "            gt, pred = prepare_gt_pred_for_roc(result, keys=keys)\n",
    "            roc = roc_auc_score(gt, pred)\n",
    "\n",
    "            final_records.append((\n",
    "                abnormality, result.metric, result.groups, result.sampler, keys, roc)\n",
    "            )\n",
    "\n",
    "            if result.metric == 'bleu':\n",
    "                # HACK\n",
    "                gt, pred = prepare_gt_pred_for_roc(result, metric_i=3, keys=keys)\n",
    "                roc = roc_auc_score(gt, pred)\n",
    "                final_records.append((\n",
    "                    abnormality, f'{result.metric}-4', result.groups, result.sampler, keys,\n",
    "                    roc,\n",
    "                ))\n",
    "\n",
    "len(final_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "final_records[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols = ['disease', 'metric', 'groups', 'sampler', 'keys', 'roc']\n",
    "df = pd.DataFrame(final_records, columns=cols)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Counter(df['groups']), Counter(df['sampler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df.loc[df['groups'] == (0, 1)]\n",
    "# df = df.loc[df['sampler'] == 'random-gen_k500_n500'] # IU\n",
    "df = df.loc[df['sampler'] == 'random-gen_k50_n100'] # MIMIC\n",
    "del df['sampler'], df['groups']\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_recall = df.loc[df['keys'] == ((1, 1), (1, 0))]\n",
    "df_spec = df.loc[df['keys'] == ((0, 0), (0, 1))]\n",
    "len(df_recall), len(df_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_renamer(replace_strs):\n",
    "    def _rename_run(run_name):\n",
    "        s = run_name\n",
    "        for target, replace_with in replace_strs:\n",
    "            s = re.sub(target, replace_with, s)\n",
    "        return s\n",
    "    return _rename_run\n",
    "\n",
    "def bold(s):\n",
    "    return '\\textbf{' + s + '}'\n",
    "\n",
    "shorten_cols = get_renamer([\n",
    "    ('cider-IDF', 'C-D'),\n",
    "    ('bleu-4', 'B-4'),\n",
    "    (r'bleu\\b', 'B-1'),\n",
    "    ('rouge', 'R-L'),\n",
    "    ('disease', 'Abnormality'),\n",
    "])\n",
    "def latexify_cols(col):\n",
    "    return bold(shorten_cols(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def df_to_table(df):\n",
    "    cols = list(df.columns)\n",
    "    metric_col = cols.index('metric')\n",
    "    roc_col = cols.index('roc')\n",
    "    df = df.groupby('disease').apply(lambda subdf: {\n",
    "        row[metric_col]: row[roc_col]\n",
    "        for row in list(subdf.values)\n",
    "    }).apply(pd.Series)\n",
    "    return df\n",
    "def table_to_latex(table):\n",
    "    s = table.reset_index().rename(columns=latexify_cols).to_latex(\n",
    "        float_format='%.3f',\n",
    "        escape=False,\n",
    "        index=False,\n",
    "        column_format='l' + 'c' * len(table.columns),\n",
    "    )\n",
    "    s = re.sub(r' +', ' ', s, flags=re.M)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "table_recall = df_to_table(df_recall)\n",
    "table_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "table_spec = df_to_table(df_spec)\n",
    "table_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "long_table = pd.concat([table_recall, table_spec], axis=1)\n",
    "table_to_latex(long_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_result(exp_by_abn, abnormality, metric,\n",
    "               groups=[0, 1], sampler='random-gen_k500_n500'):\n",
    "    if abnormality not in exp_by_abn:\n",
    "        print(f'No exp for {abnormality}')\n",
    "        return None, None\n",
    "    groups = list(groups)\n",
    "    exp = exp_by_abn[abnormality]\n",
    "\n",
    "    for i, result in enumerate(exp.results):\n",
    "        if sampler is not None and result.sampler != sampler:\n",
    "            continue\n",
    "        if groups is not None and sorted(result.groups) != groups:\n",
    "            continue\n",
    "        if result.metric != metric:\n",
    "            continue\n",
    "            \n",
    "        return exp, i\n",
    "    \n",
    "    print('No experiment found with conditions')\n",
    "    return exp, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp, result_i = get_result(exp_by_abn_iu, 'Atelectasis', 'bleu')\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_heatmap(exp, result_i=result_i, metric_i=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_hists(exp, keys=[(0, 0), (0, 1)], result_i=result_i, metric_i=3, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, mannwhitneyu, f_oneway, kruskal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# exp = load_experiment_pickle('mimic-cardiomegaly')\n",
    "len(exp.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_heatmap(exp, result_i=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "EXP_I = -1\n",
    "result = exp[EXP_I]\n",
    "result.metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "key1 = (0, 0)\n",
    "key2 = (0, 1)\n",
    "group1 = result.dists[key1]\n",
    "group2 = result.dists[key2]\n",
    "if result.metric == 'bleu':\n",
    "    group1 = group1[0]\n",
    "    group2 = group2[0]\n",
    "group1.shape, group2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_hists(exp, [key1, key2], result_i=EXP_I, bins=50, range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r = mannwhitneyu(group1, group2)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r = ttest_ind(group1, group2, equal_var=False)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "groups = [result.dists[k] for k in [(0, 0), (0, 1), (1, 0), (1, 1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "anova = f_oneway(*groups)\n",
    "anova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "kru = kruskal(*groups)\n",
    "kru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

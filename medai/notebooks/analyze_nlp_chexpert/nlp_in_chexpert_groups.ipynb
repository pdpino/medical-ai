{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "import importlib\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.facecolor'] = 'white'\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../utils/__init__.py\n",
    "config_logging(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../datasets/common/constants.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n ./nlp_in_chexpert_groups.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiments\n",
    "def load_experiments(dataset_name):\n",
    "    exp_by_abn = {}\n",
    "    errors = defaultdict(list)\n",
    "    for abnormality in CHEXPERT_DISEASES[1:]:\n",
    "        fname = f'{dataset_name}-{abnormality.replace(\" \", \"-\").lower()}'\n",
    "        if not exist_experiment_pickle(fname):\n",
    "            errors['not-found'].append(fname)\n",
    "            continue\n",
    "        exp = load_experiment_pickle(fname)\n",
    "        exp_by_abn[abnormality] = exp\n",
    "        \n",
    "    if len(errors['not-found']):\n",
    "        print('Not found: ', errors['not-found'])\n",
    "        \n",
    "    return exp_by_abn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Debug running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run -n ./nlp_in_chexpert_groups.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_info = init_dataset_info('iu')\n",
    "dataset_info.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp = init_experiment('Cardiomegaly', dataset_info)\n",
    "exp_LO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "kwargs = {\n",
    "    # 'metric': 'bleu',\n",
    "    # 'metric': 'rouge',\n",
    "    'metric': 'cider-IDF',\n",
    "    'k_times': 500,\n",
    "    # 'k_gts': 1,\n",
    "    'max_n': 500,\n",
    "}\n",
    "exp.append(calc_score_matrices(exp.grouped_2, dataset_info, groups=(0, 1), **kwargs))\n",
    "# exp.append(calc_score_matrices(exp.grouped, dataset_info, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp[-1].cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = load_experiment_pickle('iu-cardiomegaly')\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.results[-1].cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_I = 1\n",
    "METRIC_I = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_heatmap(exp, result_i=RESULT_I, metric_i=METRIC_I)\n",
    "\n",
    "if len(exp.results) > 1:\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plot_heatmap(exp, result_i=-2, metric_i=METRIC_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plot_hists(\n",
    "    exp, [\n",
    "        (0, 0), (0, 1),\n",
    "        # (0, 0), (1, 0),\n",
    "        # (1, 1), (0, 1),\n",
    "        # (1, 0), (0, 0),\n",
    "    ],\n",
    "    result_i=RESULT_I, metric_i=METRIC_I,\n",
    "    xlabel=False, bins=50, range=(0, 1),\n",
    "    add_n_to_label=True,\n",
    ")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plot_hists(\n",
    "    exp, [\n",
    "        (1, 1), (1, 0),\n",
    "        # (1, 1), (0, 1),\n",
    "        # (0, 0), (1, 0),\n",
    "        # (-2, -1), (-1, -1),\n",
    "    ],\n",
    "    result_i=RESULT_I, metric_i=METRIC_I, add_n_to_label=True,\n",
    "    title=False, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot all 4x4 matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_by_abn_iu = load_experiments('iu')\n",
    "len(exp_by_abn_iu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_by_abn = exp_by_abn_iu\n",
    "abnormalities = CHEXPERT_DISEASES[1:]\n",
    "metrics = ['bleu', 'rouge', 'cider-IDF']\n",
    "groups = [0, 1]\n",
    "# groups = [-2, 0, -1, 1]\n",
    "sampler = 'random-gen_k500_n500'\n",
    "# groups = list(sorted(groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keydict = { metric: i for i, metric in enumerate(metrics) }\n",
    "keydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubes_df = pd.DataFrame(index=abnormalities, columns=metrics)\n",
    "\n",
    "for abn in abnormalities:\n",
    "    exp = exp_by_abn[abn]\n",
    "\n",
    "    results = [\n",
    "        r\n",
    "        for r in exp.results\n",
    "        if (sampler is None or r.sampler == sampler) and \\\n",
    "            (groups is None or sorted(r.groups) == sorted(groups))\n",
    "    ]\n",
    "    # Not necessary to sort\n",
    "    # results = sorted(results, key=lambda x: keydict[x.metric])\n",
    "\n",
    "    if len(results) != len(metrics):\n",
    "        err = f'Diff amount of results than metrics: {abn}, {len(results)}, {len(metrics)}'\n",
    "        print(err)\n",
    "        continue\n",
    "    \n",
    "    for result in results:\n",
    "        cubes_df.loc[abn, result.metric] = result.cube\n",
    "\n",
    "cubes_df.isnull().any(axis=0).any(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubes_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKS = [KEY_TO_LABEL[k] for k in groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_plotable = [(0, 'bleu'), (3, 'bleu'), (0, 'rouge'), (0, 'cider-IDF')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_DEPRECATED(df, metrics):\n",
    "    all_cubes = []\n",
    "    for m in metrics:\n",
    "        for abn in abns:\n",
    "            for cube in cubes_df.loc[abn, m].values:\n",
    "                all_cubes.append(cube[0])\n",
    "                if m == 'bleu':\n",
    "                    all_cubes.append(cube[3])\n",
    "    all_cubes = np.array(all_cubes)\n",
    "    return all_cubes.min(), all_cubes.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_limits_per_abnormality(df, abns, metrics):\n",
    "    cubes_per_abn = defaultdict(list)\n",
    "    for abn in abns:\n",
    "        for m in metrics:\n",
    "            cube = df.loc[abn, m]\n",
    "            cubes_per_abn[abn].append(cube[0])\n",
    "            if m == 'bleu':\n",
    "                cubes_per_abn[abn].append(cube[3])\n",
    "    cubes_per_abn = {abn: np.array(l) for abn, l in cubes_per_abn.items()}\n",
    "    limits_per_abn = {abn: (cube.min(), cube.max()) for abn, cube in cubes_per_abn.items()}\n",
    "    return limits_per_abn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abns = CHEXPERT_DISEASES[1:]\n",
    "abns = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limits = get_limits_per_abnormality(cubes_df, abns, ['bleu', 'rouge'])\n",
    "limits_cider = get_limits_per_abnormality(cubes_df, abns, ['cider-IDF'])\n",
    "limits #, limits_cider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIN_VALUE, MAX_VALUE = get_max(cubes_df, ['bleu', 'rouge'])\n",
    "# MIN_CIDER, MAX_CIDER = get_max(cubes_df, ['cider-IDF'])\n",
    "# MIN_VALUE, MAX_VALUE, MIN_CIDER, MAX_CIDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = len(abns)\n",
    "n_cols = len(metrics_plotable)\n",
    "plt.figure(figsize=(n_cols*5, n_rows*5))\n",
    "\n",
    "for abn_i, abn in enumerate(abns):\n",
    "    for metric_col_j, (metric_i, metric) in enumerate(metrics_plotable):\n",
    "        cube = cubes_df.loc[abn, metric]\n",
    "        \n",
    "        include_cbar = (metric_col_j >= n_cols - 2) # rouge and CIDEr\n",
    "        # cbar params\n",
    "        if 'cider' in metric:\n",
    "            cbar_params = {'cmap': 'Blues'} # 'vmin': MIN_CIDER, 'vmax': MAX_CIDER, \n",
    "        else:\n",
    "            min_value, max_value = limits[abn]\n",
    "            cbar_params = {'vmin': min_value, 'vmax': max_value, 'cmap': 'YlOrRd'}\n",
    "            # cbar_params = {'vmin': 0, 'vmax': max_value + 0.2, 'cmap': 'YlOrRd'}\n",
    "        \n",
    "        plt.subplot(n_rows, n_cols, abn_i * n_cols + metric_col_j + 1)\n",
    "        sns.heatmap(cube[metric_i], annot=True, square=True,\n",
    "                    xticklabels=TICKS, yticklabels=TICKS, fmt='.3f', # robust=True,\n",
    "                    cbar=True,\n",
    "                    **cbar_params,\n",
    "                   )\n",
    "\n",
    "        title_metric = True # (abn_i == 0)\n",
    "        include_ylabel = (metric_col_j == 0)\n",
    "        include_xlabel = True # (abn_i == n_rows - 1)\n",
    "\n",
    "        if title_metric:\n",
    "            pretty_metric = get_pretty_metric(metric, metric_i=metric_i, include_range=True)\n",
    "            plt.title(pretty_metric, fontsize=18)\n",
    "\n",
    "        if include_xlabel:\n",
    "            plt.xlabel('Generated', fontsize=18)\n",
    "\n",
    "        if include_ylabel:\n",
    "            plt.ylabel(f'{abn}\\nGround Truth', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempting to optimize threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Attempt 1\n",
    "\n",
    "(failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = exp[0]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# target1, target2 = (0, 0), (0, 1) # TN, FP (specificity)\n",
    "target1, target2 = (1, 1), (1, 0) # TP, FN (precision)\n",
    "\n",
    "arr1 = result.dists[target1]\n",
    "arr2 = result.dists[target2]\n",
    "arr1.shape, arr2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "assert target1[0] == target1[1]\n",
    "CORRECT = target1[0]\n",
    "INCORRECT = 1 - CORRECT\n",
    "\n",
    "merged = [(value, CORRECT) for value in arr1] + [(value, INCORRECT) for value in arr2]\n",
    "merged = sorted(merged, reverse=bool(not CORRECT))\n",
    "merged[:2], merged[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_threshs = []\n",
    "denominator = 0 # TP + FN\n",
    "numerator = 0 # TP\n",
    "for value, label in merged:\n",
    "    current_thresh = value\n",
    "    if label == CORRECT: # add 1 TP\n",
    "        numerator += 1\n",
    "\n",
    "    denominator += 1\n",
    "\n",
    "    all_threshs.append((current_thresh, numerator / denominator))\n",
    "all_threshs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max(all_threshs, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x, y = tuple(zip(*all_threshs))\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Attempt 2\n",
    "\n",
    "with sklearn\n",
    "Failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve as pr_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pred, gt = tuple(zip(*merged))\n",
    "pred = np.array(pred)\n",
    "gt = np.array(gt)\n",
    "pred.shape, gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pred /= 10 # CIDER re-scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "precision, recall, thresholds = pr_curve(gt, pred, pos_label=CORRECT)\n",
    "precision.shape, recall.shape, thresholds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f1 = divide_arrays(2*precision*recall, precision + recall)\n",
    "f1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "best_idx = f1.argmax()\n",
    "best_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "thresholds[best_idx], f1[best_idx], precision[best_idx], recall[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Attempt 3: accuracy/prec/recall\n",
    "\n",
    "CheXpert 4-class classification task --> is a binary classification task in NLP scores\n",
    "(i.e. NLP scores tell less information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp = load_experiment_pickle('mimic-cardiomegaly')\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = exp.results[-1]\n",
    "result.metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result.dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "merged = [\n",
    "    # Value, correct-or-not, original-key\n",
    "    (value, 1, (0, 0)) for value in result.dists[(0, 0)]\n",
    "] + [\n",
    "    (value, 1, (1, 1)) for value in result.dists[(1, 1)]\n",
    "] + [\n",
    "    (value, 0, (0, 1)) for value in result.dists[(0, 1)]\n",
    "] + [\n",
    "    (value, 0, (1, 0)) for value in result.dists[(1, 0)]\n",
    "]\n",
    "merged = sorted(merged)\n",
    "len(merged), merged[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_correct = sum(1 for _, correct, _ in merged if correct)\n",
    "n_incorrect = sum(1 for _, correct, _ in merged if not correct)\n",
    "n_correct, n_incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def smart_division(a, b):\n",
    "    if b == 0:\n",
    "        return 0\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_threshs = []\n",
    "\n",
    "# At first, the threshold is at 0\n",
    "# --> No negative predictions, all positive predictions\n",
    "# --> TN = FN = 0\n",
    "TP = sum(1 for _, correct, _ in merged if correct)\n",
    "FP = sum(1 for _, correct, _ in merged if not correct)\n",
    "TN, FN = 0, 0\n",
    "\n",
    "total = len(merged)\n",
    "\n",
    "assert TP + FP + FN + TN == total, f'Begin: {TP + FP + FN + TN} vs {total}'\n",
    "\n",
    "for value, correct, _ in merged:\n",
    "    current_thresh = value\n",
    "\n",
    "    if correct:\n",
    "        TP -= 1\n",
    "        FN += 1\n",
    "    else:\n",
    "        TN += 1\n",
    "        FP -= 1\n",
    "\n",
    "    assert TP + FP + FN + TN == total, f'Thresh={value}: {TP + FP + FN + TN} vs {total}'\n",
    "        \n",
    "    acc = (TP + TN) / total\n",
    "    prec = smart_division(TP, TP + FP)\n",
    "    recall = smart_division(TP, TP + FN)\n",
    "    f1 = smart_division(2*prec*recall, prec+recall)\n",
    "    spec = smart_division(TN, TN + FP)\n",
    "    npv = smart_division(TN, TN + FN)\n",
    "    f1_neg = smart_division(2*npv*spec, spec+npv)\n",
    "    CM = (TP, FN, FP, TN)\n",
    "\n",
    "    all_threshs.append({\n",
    "        'thresh': current_thresh,\n",
    "        'acc': acc,\n",
    "        'prec': prec,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'npv': npv,\n",
    "        'spec': spec,\n",
    "        'f1_neg': f1_neg,\n",
    "        'CM': CM,\n",
    "    })\n",
    "all_threshs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max(all_threshs, key=lambda x: x['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sl = lambda k: tuple(zip(*[(x['thresh'], x[k]) for x in all_threshs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "keys = ('prec', 'recall', 'acc', 'f1') # 'f1', \n",
    "# keys = ('acc', )\n",
    "# keys = ('npv', 'spec', 'f1_neg')\n",
    "for k in keys:\n",
    "    thresh, y = sl(k)\n",
    "    plt.plot(thresh, y, label=k)\n",
    "plt.legend()\n",
    "plt.xlabel('Thresh')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Optimize by')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "best = max(all_threshs, key=lambda x: x['acc'])\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_cm(cm, title=None):\n",
    "    TP, FN, FP, TN = cm\n",
    "    ticks = ['Entailment', 'Contradiction']\n",
    "    sns.heatmap([[TP, FN], [FP, TN]], annot=True, square=True, cmap='Blues',\n",
    "                xticklabels=ticks, yticklabels=ticks, fmt=',',\n",
    "               )\n",
    "    plt.ylabel('Real')\n",
    "    plt.xlabel('Scored by Metric')\n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_cm(best['CM'], title=f'CM for {exp.abnormality} with {get_pretty_metric(result.metric)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Attempt 4: use AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def prepare_gt_pred_for_roc(result, metric_i=0, keys=None):\n",
    "    pred = []\n",
    "    gt = []\n",
    "    if keys is None:\n",
    "        keys = list(result.dists.keys())\n",
    "\n",
    "    for a, b in keys:\n",
    "        elements = result.dists[(a, b)]\n",
    "        if elements.ndim == 2:\n",
    "            elements = elements[metric_i] # BLEU case\n",
    "        pred += list(elements)\n",
    "\n",
    "        entailment = int(a == b)\n",
    "        gt += [entailment] * len(elements)\n",
    "\n",
    "    return gt, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = exp.results[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gt, pred = prepare_gt_pred_for_roc(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(gt, pred)\n",
    "\n",
    "J_stat = tpr - fpr\n",
    "best_idx = J_stat.argmax()\n",
    "\n",
    "thresholds[best_idx], J_stat[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "roc = roc_auc_score(gt, pred)\n",
    "roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Compute AUC for all abnormalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_name = 'mimic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp_by_abn_iu = load_experiments(dataset_name)\n",
    "len(exp_by_abn_iu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show = True\n",
    "target_sampler = None # 'random-gen_k500_n500'\n",
    "target_groups = [0, 1] # [-2, -1, 0, 1]\n",
    "# keys = [(0, 0), (0, 1), (1, 1), (1, 0)]\n",
    "# keys = None\n",
    "set_of_keys = [\n",
    "    ((0, 0), (0, 1)),\n",
    "    ((1, 1), (1, 0)),\n",
    "]\n",
    "\n",
    "final_records = []\n",
    "\n",
    "for abnormality in CHEXPERT_DISEASES[1:]:\n",
    "    if abnormality not in exp_by_abn_iu:\n",
    "        continue\n",
    "    exp = exp_by_abn_iu[abnormality]\n",
    "\n",
    "    for result in tqdm(exp.results, desc=abnormality, disable=not show):\n",
    "        if target_sampler is not None and result.sampler != target_sampler:\n",
    "            continue\n",
    "        if target_groups is not None and sorted(result.groups) != target_groups:\n",
    "            continue\n",
    "        \n",
    "        for keys in set_of_keys:\n",
    "            gt, pred = prepare_gt_pred_for_roc(result, keys=keys)\n",
    "            roc = roc_auc_score(gt, pred)\n",
    "\n",
    "            final_records.append((\n",
    "                abnormality, result.metric, result.groups, result.sampler, keys, roc)\n",
    "            )\n",
    "\n",
    "            if result.metric == 'bleu':\n",
    "                # HACK\n",
    "                gt, pred = prepare_gt_pred_for_roc(result, metric_i=3, keys=keys)\n",
    "                roc = roc_auc_score(gt, pred)\n",
    "                final_records.append((\n",
    "                    abnormality, f'{result.metric}-4', result.groups, result.sampler, keys,\n",
    "                    roc,\n",
    "                ))\n",
    "\n",
    "len(final_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "final_records[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols = ['disease', 'metric', 'groups', 'sampler', 'keys', 'roc']\n",
    "df = pd.DataFrame(final_records, columns=cols)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Counter(df['groups']), Counter(df['sampler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df.loc[df['groups'] == (0, 1)]\n",
    "# df = df.loc[df['sampler'] == 'random-gen_k500_n500'] # IU\n",
    "df = df.loc[df['sampler'] == 'random-gen_k50_n100'] # MIMIC\n",
    "del df['sampler'], df['groups']\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_recall = df.loc[df['keys'] == ((1, 1), (1, 0))]\n",
    "df_spec = df.loc[df['keys'] == ((0, 0), (0, 1))]\n",
    "len(df_recall), len(df_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_renamer(replace_strs):\n",
    "    def _rename_run(run_name):\n",
    "        s = run_name\n",
    "        for target, replace_with in replace_strs:\n",
    "            s = re.sub(target, replace_with, s)\n",
    "        return s\n",
    "    return _rename_run\n",
    "\n",
    "def bold(s):\n",
    "    return '\\textbf{' + s + '}'\n",
    "\n",
    "shorten_cols = get_renamer([\n",
    "    ('cider-IDF', 'C-D'),\n",
    "    ('bleu-4', 'B-4'),\n",
    "    (r'bleu\\b', 'B-1'),\n",
    "    ('rouge', 'R-L'),\n",
    "    ('disease', 'Abnormality'),\n",
    "])\n",
    "def latexify_cols(col):\n",
    "    return bold(shorten_cols(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def df_to_table(df):\n",
    "    cols = list(df.columns)\n",
    "    metric_col = cols.index('metric')\n",
    "    roc_col = cols.index('roc')\n",
    "    df = df.groupby('disease').apply(lambda subdf: {\n",
    "        row[metric_col]: row[roc_col]\n",
    "        for row in list(subdf.values)\n",
    "    }).apply(pd.Series)\n",
    "    return df\n",
    "def table_to_latex(table):\n",
    "    s = table.reset_index().rename(columns=latexify_cols).to_latex(\n",
    "        float_format='%.3f',\n",
    "        escape=False,\n",
    "        index=False,\n",
    "        column_format='l' + 'c' * len(table.columns),\n",
    "    )\n",
    "    s = re.sub(r' +', ' ', s, flags=re.M)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "table_recall = df_to_table(df_recall)\n",
    "table_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "table_spec = df_to_table(df_spec)\n",
    "table_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "long_table = pd.concat([table_recall, table_spec], axis=1)\n",
    "table_to_latex(long_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_result(exp_by_abn, abnormality, metric,\n",
    "               groups=[0, 1], sampler='random-gen_k500_n500'):\n",
    "    if abnormality not in exp_by_abn:\n",
    "        print(f'No exp for {abnormality}')\n",
    "        return None, None\n",
    "    groups = list(groups)\n",
    "    exp = exp_by_abn[abnormality]\n",
    "\n",
    "    for i, result in enumerate(exp.results):\n",
    "        if sampler is not None and result.sampler != sampler:\n",
    "            continue\n",
    "        if groups is not None and sorted(result.groups) != groups:\n",
    "            continue\n",
    "        if result.metric != metric:\n",
    "            continue\n",
    "            \n",
    "        return exp, i\n",
    "    \n",
    "    print('No experiment found with conditions')\n",
    "    return exp, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp, result_i = get_result(exp_by_abn_iu, 'Atelectasis', 'bleu')\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_heatmap(exp, result_i=result_i, metric_i=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_hists(exp, keys=[(0, 0), (0, 1)], result_i=result_i, metric_i=3, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, mannwhitneyu, f_oneway, kruskal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# exp = load_experiment_pickle('mimic-cardiomegaly')\n",
    "len(exp.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_heatmap(exp, result_i=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "EXP_I = -1\n",
    "result = exp[EXP_I]\n",
    "result.metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "key1 = (0, 0)\n",
    "key2 = (0, 1)\n",
    "group1 = result.dists[key1]\n",
    "group2 = result.dists[key2]\n",
    "if result.metric == 'bleu':\n",
    "    group1 = group1[0]\n",
    "    group2 = group2[0]\n",
    "group1.shape, group2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_hists(exp, [key1, key2], result_i=EXP_I, bins=50, range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r = mannwhitneyu(group1, group2)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r = ttest_ind(group1, group2, equal_var=False)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "groups = [result.dists[k] for k in [(0, 0), (0, 1), (1, 0), (1, 1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "anova = f_oneway(*groups)\n",
    "anova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "kru = kruskal(*groups)\n",
    "kru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

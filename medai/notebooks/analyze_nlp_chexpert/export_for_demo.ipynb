{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create random debug data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed score data as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['bleu1', 'bleu2', 'bleu3', 'bleu4']\n",
    "n_samples_per_val = 100\n",
    "valorations = 4\n",
    "data = np.random.random(size=(n_samples_per_val * valorations, len(metrics)))\n",
    "df = pd.DataFrame(data, columns=metrics)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = n_samples_per_val\n",
    "gt = ['positive'] * n * 2 + ['negative'] * n * 2\n",
    "gen = ['positive'] * n + ['negative'] * n + ['positive'] * n + ['negative'] * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(**{'gt': gt, 'gen': gen})\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('~/debug-data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real examples to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n nlp_in_chexpert_groups.py\n",
    "%run ../../datasets/common/constants.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = os.path.join(_EXP_FOLDER, 'csv_exports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abn_to_writable = lambda x: ABN_SHORTCUTS[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP_METRICS = ['bleu1', 'bleu2', 'bleu3', 'bleu4', 'rouge', 'cider']\n",
    "VAL_TO_NAME = {-2: 'none', -1: 'unc', 0: 'neg', 1: 'pos'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'mimic'\n",
    "exp_by_abn = load_experiments(dataset_name)\n",
    "len(exp_by_abn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups2 = (0, 1)\n",
    "groups4 = (-2, -1, 0, 1)\n",
    "\n",
    "def create_base_df(abnormality, groups):\n",
    "    if groups == 2:\n",
    "        groups = groups2\n",
    "    elif groups == 4:\n",
    "        groups = groups4\n",
    "    data = [\n",
    "        (abn_to_writable(abnormality), VAL_TO_NAME[g_gt], VAL_TO_NAME[g_gen])\n",
    "        for g_gt, g_gen in product(groups, groups)\n",
    "    ]\n",
    "    df = pd.DataFrame(data, columns=['abnormality', 'gt', 'gen'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_col_name(metric_name, metric_i):\n",
    "    if metric_name == 'cider-IDF':\n",
    "        return 'cider'\n",
    "    if metric_name == 'bleu':\n",
    "        return f'bleu{metric_i+1}'\n",
    "    return metric_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_df(exp, abnormality):\n",
    "    abn_dfs = {\n",
    "        2: create_base_df(abnormality, 2), # Chex-2\n",
    "        4: create_base_df(abnormality, 4), # Chex-4\n",
    "    }\n",
    "    \n",
    "    for result in exp.results:\n",
    "        groups = result.groups\n",
    "\n",
    "        n_metrics, n, m = result.cube.shape\n",
    "        l = len(result.dists.keys())\n",
    "        assert n == m and m*m == l and m == len(groups), (n, m, l, groups)\n",
    "\n",
    "        for metric_i in range(n_metrics):\n",
    "            df_data = []\n",
    "            cube = result.cube[metric_i] # shape: n_groups, n_groups\n",
    "            \n",
    "            cube = cube.astype(np.float16)\n",
    "            \n",
    "            for (i, group_gen), (j, group_gt) in product(enumerate(groups), enumerate(groups)):\n",
    "                df_data.append((\n",
    "                    cube[j, i], # metric value\n",
    "                    VAL_TO_NAME[group_gt], # gt valuation\n",
    "                    VAL_TO_NAME[group_gen], # generated valuation\n",
    "                ))\n",
    "\n",
    "            metric_name = get_metric_col_name(result.metric, metric_i)\n",
    "            right_df = pd.DataFrame(df_data, columns=[metric_name, 'gt', 'gen'])\n",
    "\n",
    "            df = abn_dfs[len(groups)]\n",
    "            abn_dfs[len(groups)] = df.merge(right_df, how='left', on=['gt', 'gen'])\n",
    "    return abn_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiments_to_df(df_extractor):\n",
    "    cols = ['abnormality', 'gt', 'gen'] + NLP_METRICS\n",
    "    main_dfs = {\n",
    "        2: pd.DataFrame(columns=cols),\n",
    "        4: pd.DataFrame(columns=cols),\n",
    "    }\n",
    "    for abnormality in CHEXPERT_DISEASES[1:]:\n",
    "        exp = exp_by_abn[abnormality]\n",
    "        abn_dfs = df_extractor(exp, abnormality)\n",
    "\n",
    "        for chex_k in sorted(list(main_dfs.keys())):\n",
    "            d1 = main_dfs[chex_k]\n",
    "            d2 = abn_dfs[chex_k]\n",
    "            n1, n2 = len(d1), len(d2)\n",
    "            main_dfs[chex_k] = pd.concat((d1, d2), axis=0, ignore_index=True)\n",
    "\n",
    "            n3 = len(main_dfs[chex_k])\n",
    "            assert n3 == n1 + n2, (n1, n2, n3)\n",
    "\n",
    "    return main_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat summary scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dfs = experiments_to_df(create_summary_df)\n",
    "len(main_dfs[2]), len(main_dfs[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dfs[2].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chex_k, df in main_dfs.items():\n",
    "    fname = os.path.join(outdir, f'summaries-{dataset_name}-chex{chex_k}.csv')\n",
    "    df.to_csv(fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultsByGroup:\n",
    "    def __init__(self, n_groups):\n",
    "        # self.n_groups = n_groups\n",
    "        groups = (0, 1) if n_groups == 2 else (-2, -1, 0, 1)\n",
    "        \n",
    "        self._keys = list(product(groups, groups))\n",
    "        self.scores_arr = {\n",
    "            k: None\n",
    "            for k in self._keys\n",
    "        }\n",
    "        \n",
    "        self.metrics = {\n",
    "            k: []\n",
    "            for k in self._keys\n",
    "        }\n",
    "        \n",
    "    def add_metrics(self, metrics):\n",
    "        # metrics shape: n_metrics\n",
    "        self.metrics = [] # TODO\n",
    "        \n",
    "    def add_item(self, key, scores, metrics):\n",
    "        # scores shape: n_metrics, n_samples\n",
    "        # metrics shape: n_metrics\n",
    "        if scores.ndim == 1:\n",
    "            scores = np.expand_dims(scores, 0)\n",
    "        \n",
    "        assert len(metrics) == scores.shape[0], (len(metrics), scores.shape)\n",
    "\n",
    "        prev_arr = self.scores_arr[key]\n",
    "        if prev_arr is None:\n",
    "            self.scores_arr[key] = scores\n",
    "        else:\n",
    "            self.scores_arr[key] = np.concatenate((prev_arr, scores), axis=0)\n",
    "            \n",
    "        seen_metrics = self.metrics[key]\n",
    "        assert all(m not in seen_metrics for m in metrics), (metrics, seen_metrics)\n",
    "        self.metrics[key] += metrics\n",
    "        \n",
    "    def to_df(self, key):\n",
    "        columns = self.metrics[key]\n",
    "        scores = self.scores_arr[key].transpose() # shape: n_samples, n_metrics\n",
    "        \n",
    "        scores = scores.astype(np.float16)\n",
    "        \n",
    "        n_samples = scores.shape[0]\n",
    "        \n",
    "        df = pd.DataFrame(scores, columns=columns)\n",
    "        \n",
    "        gt, gen = key\n",
    "        gt, gen = VAL_TO_NAME[gt], VAL_TO_NAME[gen]\n",
    "        df['gt'] = gt\n",
    "        df['gen'] = gen\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def iter_dfs(self):\n",
    "        return [\n",
    "            self.to_df(key)\n",
    "            for key in self._keys\n",
    "        ]\n",
    "            \n",
    "\n",
    "def create_samples_df(exp, abnormality):\n",
    "    scores_by_group = {\n",
    "        2: ResultsByGroup(2),\n",
    "        4: ResultsByGroup(4),\n",
    "    }\n",
    "    \n",
    "    # results = sorted(exp.results, key=sort_metrics) # bleu, rouge, CIDEr, always in that order\n",
    "    results = exp.results\n",
    "    for result in results:\n",
    "        groups = result.groups\n",
    "        \n",
    "        if len(groups) not in scores_by_group:\n",
    "            continue\n",
    "\n",
    "        n_metrics, n, m = result.cube.shape\n",
    "        l = len(result.dists.keys())\n",
    "        assert n == m and m*m == l and m == len(groups), (n, m, l, groups)\n",
    "\n",
    "        for (i, group_gen), (j, group_gt) in product(enumerate(groups), enumerate(groups)):\n",
    "            samples_square = result.dists[(group_gt, group_gen)] # shape: n_metrics, n_samples\n",
    "            \n",
    "            metrics = [\n",
    "                get_metric_col_name(result.metric, metric_i)\n",
    "                for metric_i in range(n_metrics)\n",
    "            ]\n",
    "            \n",
    "            key = (group_gt, group_gen)\n",
    "            scores_by_group[len(groups)].add_item(key, samples_square, metrics)\n",
    "\n",
    "    chex2 = scores_by_group[2]\n",
    "    chex4 = scores_by_group[4]\n",
    "    \n",
    "    chex2 = pd.concat(chex2.iter_dfs(), axis=0, ignore_index=True)\n",
    "    chex4 = pd.concat(chex4.iter_dfs(), axis=0, ignore_index=True)\n",
    "\n",
    "    chex2['abnormality'] = abn_to_writable(abnormality)\n",
    "    chex4['abnormality'] = abn_to_writable(abnormality)\n",
    "    \n",
    "    return {\n",
    "        2: chex2,\n",
    "        4: chex4,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dfs = experiments_to_df(create_samples_df)\n",
    "len(sample_dfs[2]), len(sample_dfs[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chex_k, df in sample_dfs.items():\n",
    "    fname = os.path.join(outdir, f'samples-{dataset_name}-chex{chex_k}.csv')\n",
    "    df.to_csv(fname, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = torch.device('cpu')\n",
    "DEVICE = torch.device('cuda')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Evaluate models in subsets\n",
    "\n",
    "TODO: move this to script???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run -n train_report_generation.py\n",
    "%run datasets/__init__.py\n",
    "%run models/checkpoint/__init__.py\n",
    "%run training/report_generation/flat.py\n",
    "%run training/report_generation/hierarchical.py\n",
    "%run models/report_generation/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def eval_in_subset(run_name, compiled_model, debug=True, max_n_words=None, max_n_sentences=None,\n",
    "                   device='cuda'):\n",
    "    # Create datasets\n",
    "    vocab = compiled_model.metadata['vocab']\n",
    "    train_dataset = IUXRayDataset('train', vocab=vocab)\n",
    "    val_dataset = IUXRayDataset('val', vocab=vocab)\n",
    "    test_dataset = IUXRayDataset('test', vocab=vocab)\n",
    "    \n",
    "    # Prepare subsets\n",
    "    subset_kwargs = {\n",
    "        'max_n_words': max_n_words,\n",
    "        'max_n_sentences': max_n_sentences,\n",
    "    }\n",
    "    \n",
    "    train_subset = create_report_dataset_subset(train_dataset, **subset_kwargs)\n",
    "    val_subset = create_report_dataset_subset(val_dataset, **subset_kwargs)\n",
    "    test_subset = create_report_dataset_subset(test_dataset, **subset_kwargs)\n",
    "    \n",
    "    # Decide hierachical\n",
    "    decoder_name = compiled_model.metadata['decoder_kwargs']['decoder_name']\n",
    "    hierarchical = is_decoder_hierarchical(decoder_name)\n",
    "    if hierarchical:\n",
    "        create_dataloader = create_hierarchical_dataloader\n",
    "    else:\n",
    "        create_dataloader = create_flat_dataloader\n",
    "\n",
    "    # Create dataloaders\n",
    "    BS = 50\n",
    "    train_dataloader = create_dataloader(train_subset, batch_size=BS)\n",
    "    val_dataloader = create_dataloader(val_subset, batch_size=BS)\n",
    "    test_dataloader = create_dataloader(test_subset, batch_size=BS)\n",
    "    \n",
    "    # Create a suffix\n",
    "    if max_n_words:\n",
    "        suffix = f'max-words-{max_n_words}'\n",
    "    elif max_n_sentences:\n",
    "        suffix = f'max-sentences-{max_n_sentences}'\n",
    "        \n",
    "    evaluate_and_save(run_name,\n",
    "                      compiled_model.model,\n",
    "                      train_dataloader,\n",
    "                      val_dataloader,\n",
    "                      test_dataloader,\n",
    "                      hierarchical=hierarchical,\n",
    "                      debug=debug,\n",
    "                      device=device,\n",
    "                      suffix=suffix,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eval_n_words = [\n",
    "    20 , # --> 15%\n",
    "    25 , # --> 26%\n",
    "    27 , # --> 33%\n",
    "    33 , # --> 50%\n",
    "#     39 , # --> 66%\n",
    "#     41 , # --> 70%\n",
    "    44 , # --> 75%\n",
    "#     47 , # --> 80%\n",
    "#     58 , # --> 90%\n",
    "    # None, # --> 100%\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eval_n_sentences = [\n",
    "#     1, # 1.2324835387472564\n",
    "#     2, # 4.761100793516799\n",
    "    3, # 25.730204288367382\n",
    "    4, # 55.10720918453487\n",
    "    5, # 76.66722944453824\n",
    "    6, # 89.39726489954415\n",
    "#     7, # 95.03629917271653\n",
    "#     8, # 97.6194496032416\n",
    "#     9, # 98.86881647813608\n",
    "#     10, # 99.42596657099443\n",
    "#     11, # 99.71298328549722\n",
    "#     12, # 99.89869998311667\n",
    "#     13, # 99.96623332770555\n",
    "#     17, # 99.98311666385278\n",
    "#     18, # 100\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "run_names = [\n",
    "#     '0717_041434_lstm_lr0.0001_densenet-121',\n",
    "    '0716_211601_lstm-att_lr0.0001_densenet-121', # faltan 33 y 34\n",
    "#     '0717_015057_h-lstm_lr0.0001_densenet-121',\n",
    "#     '0716_234501_h-lstm-att_lr0.0001_densenet-121',\n",
    "]\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for run_name in run_names:\n",
    "    compiled_model = load_compiled_model_report_generation(run_name,\n",
    "                                                           debug=debug,\n",
    "                                                           multiple_gpu=True,\n",
    "                                                           device=DEVICE)\n",
    "    for n_words in tqdm(eval_n_words):\n",
    "        eval_in_subset(run_name,\n",
    "                       compiled_model,\n",
    "                       max_n_words=n_words,\n",
    "                       max_n_sentences=None,\n",
    "                       debug=debug,\n",
    "                       device=DEVICE,\n",
    "                      )\n",
    "    for n_sentences in tqdm(eval_n_sentences):\n",
    "        eval_in_subset(run_name,\n",
    "                       compiled_model,\n",
    "                       max_n_words=None,\n",
    "                       max_n_sentences=n_sentences,\n",
    "                       debug=debug,\n",
    "                       device=DEVICE,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug chexpert-labeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils/files.py\n",
    "%run ../metrics/__init__.py\n",
    "%run ../metrics/report_generation/chexpert.py\n",
    "# %run -n ../eval_report_generation_chexpert_labeler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = RunId('0428_130903', False, 'rg')\n",
    "run_id.full_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_rg_outputs(run_id, free=False)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = apply_labeler_to_df(df, caller_id='eval-notebook', dataset_name='mini-mimic')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Debug MIRQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run -n ../eval_report_generation_mirqi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Load MIRQI output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/software/MIRQI/testing2.csv')\n",
    "df.fillna('', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "attributes_gt = _attributes_to_list(df['attributes-gt'])\n",
    "attributes_gen = _attributes_to_list(df['attributes-gen'])\n",
    "len(attributes_gt), len(attributes_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['attributes-gt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run -n ../eval_report_generation_mirqi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores = MIRQI_v2(attributes_gt, attributes_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores['MIRQI-v2-attr-p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idx = 2\n",
    "attributes_gt[idx], attributes_gen[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## MIRQI Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run -n ../eval_report_generation_mirqi.py\n",
    "# %run -n ~/software/MIRQI/evaluate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### MIRQI original def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def MIRQI(gt_list, cand_list, pos_weight=0.8, attribute_weight=0.3, verbose=False):\n",
    "    \"\"\"Compute the score of matching keyword and associated attributes between gt list and candidate list.\n",
    "       It returns two scores:   MIRQI-r (recall: hits in gt)\n",
    "                                MIRQI-p (precision: correct ratio of all candidates)\n",
    "    \"\"\"\n",
    "\n",
    "    MIRQI_r = []\n",
    "    MIRQI_p = []\n",
    "    MIRQI_f = []\n",
    "\n",
    "    for gt_report_entry, cand_report_entry in zip(gt_list, cand_list):\n",
    "        attribute_cand_all = []\n",
    "\n",
    "        pos_count_in_gt = 0\n",
    "        pos_count_in_cand = 0\n",
    "        tp = 0.0\n",
    "        fp = 0.0\n",
    "        tn = 0.0\n",
    "        fn = 0.0\n",
    "\n",
    "        for gt_entity in gt_report_entry:\n",
    "            if gt_entity[2] == 'NEGATIVE':\n",
    "                continue\n",
    "            pos_count_in_gt = pos_count_in_gt + 1\n",
    "        neg_count_in_gt = len(gt_report_entry) - pos_count_in_gt\n",
    "\n",
    "        for entity_index, cand_entity in enumerate(cand_report_entry):\n",
    "            if cand_entity[2] == 'NEGATIVE':\n",
    "                for entity_index, gt_entity in enumerate(gt_report_entry):\n",
    "                    if  gt_entity[1] == cand_entity[1]:\n",
    "                        if gt_entity[2] == 'NEGATIVE':\n",
    "                            tn = tn + 1     # true negative hits\n",
    "                            break\n",
    "                        else:\n",
    "                            fn = fn + 1     # false negative hits\n",
    "                            break\n",
    "            else:\n",
    "                pos_count_in_cand = pos_count_in_cand + 1\n",
    "                for entity_index, gt_entity in enumerate(gt_report_entry):\n",
    "                    if gt_entity[1] == cand_entity[1]:\n",
    "                        if gt_entity[2] == 'NEGATIVE':\n",
    "                            fp = fp + 1     # false positive hits\n",
    "                            break\n",
    "                        else:\n",
    "                            tp = tp + 1.0 - attribute_weight    # true positive hits (key words part)\n",
    "                            # count attribute hits\n",
    "                            if gt_entity[3] == '':\n",
    "                                break\n",
    "                            attributes_all_gt = gt_entity[3].split('/')\n",
    "                            attribute_hit_count = 0\n",
    "                            for attribute in attributes_all_gt:\n",
    "                                if attribute in cand_entity[3]:\n",
    "                                    attribute_hit_count = attribute_hit_count + 1\n",
    "                            # true positive hits (attributes part)\n",
    "                            temp = attribute_hit_count/len(attributes_all_gt)*attribute_weight\n",
    "                            tp = tp + temp\n",
    "                            break\n",
    "        neg_count_in_cand = len(cand_report_entry) - pos_count_in_cand\n",
    "        #\n",
    "        # calculate score for positive/uncertain mentions\n",
    "        if pos_count_in_gt == 0 and pos_count_in_cand == 0:\n",
    "            score_r = 1.0\n",
    "            score_p = 1.0\n",
    "        elif pos_count_in_gt == 0 and pos_count_in_cand != 0:\n",
    "            score_r = 0.0\n",
    "            score_p = 0.0\n",
    "        elif pos_count_in_gt != 0 and pos_count_in_cand == 0:\n",
    "            score_r = 0.0\n",
    "            score_p = 0.0\n",
    "        else:\n",
    "            score_r = tp / (tp + fn + 0.000001)\n",
    "            score_p = tp / (tp + fp + 0.000001)\n",
    "\n",
    "        # calculate score for negative mentions\n",
    "        # if neg_count_in_cand != 0 and neg_count_in_gt != 0:\n",
    "        if tn != 0:\n",
    "            score_r = score_r * pos_weight + tn / (tn + fp + 0.000001) * (1.0 - pos_weight)\n",
    "            score_p = score_p * pos_weight + tn / (tn + fn + 0.000001) * (1.0 - pos_weight)\n",
    "\n",
    "        MIRQI_r.append(score_r)\n",
    "        MIRQI_p.append(score_p)\n",
    "        rec_prec = (score_r + score_p)\n",
    "        MIRQI_f.append(2*(score_r * score_p) / rec_prec if rec_prec != 0.0 else 0.0)\n",
    "\n",
    "    scores = {\n",
    "        'MIRQI-r': MIRQI_r,\n",
    "        'MIRQI-p': MIRQI_p,\n",
    "        'MIRQI-f': MIRQI_f,\n",
    "    }\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Robust matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Repeated nodes with different attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "report_gt = \"right effusion with mild atelectasis. left effusion is also present.\"\n",
    "entities_gt = [\n",
    "    ['effusion', 'Pleural Effusion', 'POSITIVE', 'right/present'],\n",
    "    ['effusion', 'Pleural Effusion', 'POSITIVE', 'left/pleural'],\n",
    "]\n",
    "report_gen = report_gt\n",
    "entities_gen = [\n",
    "    ['effusion', 'Pleural Effusion', 'POSITIVE', 'left/pleural'],\n",
    "    ['effusion', 'Pleural Effusion', 'POSITIVE', 'right/present'],\n",
    "]\n",
    "{\n",
    "    **MIRQI([entities_gt], [entities_gen]),\n",
    "    **MIRQI_v2([entities_gt], [entities_gen]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### GT nodes matched twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "report_gt = \"right pleural effusion.\"\n",
    "entities_gt = [\n",
    "    ['effusion', 'Pleural Effusion', 'POSITIVE', 'right'],\n",
    "]\n",
    "report_gen = \"right pleural effusion. left pleural effusion\"\n",
    "entities_gen = [\n",
    "    ['effusion', 'Pleural Effusion', 'POSITIVE', 'right'],\n",
    "    ['effusion', 'Pleural Effusion', 'POSITIVE', 'left'],\n",
    "]\n",
    "{\n",
    "    **MIRQI([entities_gt], [entities_gen]),\n",
    "    **MIRQI_v2([entities_gt], [entities_gen]),\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2,3\n",
    "!echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = torch.device('cpu')\n",
    "DEVICE = torch.device('cuda')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../datasets/iu_xray.py\n",
    "%run ../training/report_generation/hierarchical.py\n",
    "%run ../utils/nlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IUXRayDataset('train')\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Compute dataset stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_dataset_stats(dataset):\n",
    "    word_appearances = dict()\n",
    "\n",
    "    words_count = []\n",
    "    sentences_count = []\n",
    "\n",
    "    for image, report in tqdm(dataset):\n",
    "        # Save appearances\n",
    "        for word in report:\n",
    "            if word not in word_appearances:\n",
    "                word_appearances[word] = 0\n",
    "            word_appearances[word] += 1\n",
    "\n",
    "        # Must end with a dot\n",
    "        if report[-1] != END_OF_SENTENCE_IDX:\n",
    "            report.append(END_OF_SENTENCE_IDX)\n",
    "\n",
    "        # Count words\n",
    "        n_words = len(report)\n",
    "        words_count.append(n_words)\n",
    "\n",
    "        # Count sentences\n",
    "        n_sentences = report.count(END_OF_SENTENCE_IDX)\n",
    "        sentences_count.append(n_sentences)\n",
    "\n",
    "    return word_appearances, words_count, sentences_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_appearances, words_count, sentences_count = get_dataset_stats(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_counter(counter, title):\n",
    "    x = list(counter.keys())\n",
    "    y = list(counter.values())\n",
    "    \n",
    "    plt.bar(x, y)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_counter(Counter(words_count), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_counter(Counter(sentences_count), 'sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "counter = Counter(sentences_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_percentiles(counter):\n",
    "    items = sorted(counter.items())\n",
    "\n",
    "    keys, values = zip(*items)\n",
    "\n",
    "    return list(zip(keys, np.cumsum(values) / total * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_percentiles(Counter(words_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_percentiles(Counter(sentences_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Evaluate models in subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run -n train_report_generation.py\n",
    "%run datasets/__init__.py\n",
    "%run models/checkpoint/__init__.py\n",
    "%run training/report_generation/flat.py\n",
    "%run training/report_generation/hierarchical.py\n",
    "%run models/report_generation/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def eval_in_subset(run_name, compiled_model, debug=True, max_n_words=None, max_n_sentences=None,\n",
    "                   device='cuda'):\n",
    "    # Create datasets\n",
    "    vocab = compiled_model.metadata['vocab']\n",
    "    train_dataset = IUXRayDataset('train', vocab=vocab)\n",
    "    val_dataset = IUXRayDataset('val', vocab=vocab)\n",
    "    test_dataset = IUXRayDataset('test', vocab=vocab)\n",
    "    \n",
    "    # Prepare subsets\n",
    "    subset_kwargs = {\n",
    "        'max_n_words': max_n_words,\n",
    "        'max_n_sentences': max_n_sentences,\n",
    "    }\n",
    "    \n",
    "    train_subset = create_report_dataset_subset(train_dataset, **subset_kwargs)\n",
    "    val_subset = create_report_dataset_subset(val_dataset, **subset_kwargs)\n",
    "    test_subset = create_report_dataset_subset(test_dataset, **subset_kwargs)\n",
    "    \n",
    "    # Decide hierachical\n",
    "    decoder_name = compiled_model.metadata['decoder_kwargs']['decoder_name']\n",
    "    hierarchical = is_decoder_hierarchical(decoder_name)\n",
    "    if hierarchical:\n",
    "        create_dataloader = create_hierarchical_dataloader\n",
    "    else:\n",
    "        create_dataloader = create_flat_dataloader\n",
    "\n",
    "    # Create dataloaders\n",
    "    BS = 50\n",
    "    train_dataloader = create_dataloader(train_subset, batch_size=BS)\n",
    "    val_dataloader = create_dataloader(val_subset, batch_size=BS)\n",
    "    test_dataloader = create_dataloader(test_subset, batch_size=BS)\n",
    "    \n",
    "    # Create a suffix\n",
    "    if max_n_words:\n",
    "        suffix = f'max-words-{max_n_words}'\n",
    "    elif max_n_sentences:\n",
    "        suffix = f'max-sentences-{max_n_sentences}'\n",
    "        \n",
    "    evaluate_and_save(run_name,\n",
    "                      compiled_model.model,\n",
    "                      train_dataloader,\n",
    "                      val_dataloader,\n",
    "                      test_dataloader,\n",
    "                      hierarchical=hierarchical,\n",
    "                      debug=debug,\n",
    "                      device=device,\n",
    "                      suffix=suffix,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eval_n_words = [\n",
    "    20 , # --> 15%\n",
    "    25 , # --> 26%\n",
    "    27 , # --> 33%\n",
    "    33 , # --> 50%\n",
    "#     39 , # --> 66%\n",
    "#     41 , # --> 70%\n",
    "    44 , # --> 75%\n",
    "#     47 , # --> 80%\n",
    "#     58 , # --> 90%\n",
    "    # None, # --> 100%\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eval_n_sentences = [\n",
    "#     1, # 1.2324835387472564\n",
    "#     2, # 4.761100793516799\n",
    "    3, # 25.730204288367382\n",
    "    4, # 55.10720918453487\n",
    "    5, # 76.66722944453824\n",
    "    6, # 89.39726489954415\n",
    "#     7, # 95.03629917271653\n",
    "#     8, # 97.6194496032416\n",
    "#     9, # 98.86881647813608\n",
    "#     10, # 99.42596657099443\n",
    "#     11, # 99.71298328549722\n",
    "#     12, # 99.89869998311667\n",
    "#     13, # 99.96623332770555\n",
    "#     17, # 99.98311666385278\n",
    "#     18, # 100\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "run_names = [\n",
    "#     '0717_041434_lstm_lr0.0001_densenet-121',\n",
    "    '0716_211601_lstm-att_lr0.0001_densenet-121', # faltan 33 y 34\n",
    "#     '0717_015057_h-lstm_lr0.0001_densenet-121',\n",
    "#     '0716_234501_h-lstm-att_lr0.0001_densenet-121',\n",
    "]\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for run_name in run_names:\n",
    "    compiled_model = load_compiled_model_report_generation(run_name,\n",
    "                                                           debug=debug,\n",
    "                                                           multiple_gpu=True,\n",
    "                                                           device=DEVICE)\n",
    "    for n_words in tqdm(eval_n_words):\n",
    "        eval_in_subset(run_name,\n",
    "                       compiled_model,\n",
    "                       max_n_words=n_words,\n",
    "                       max_n_sentences=None,\n",
    "                       debug=debug,\n",
    "                       device=DEVICE,\n",
    "                      )\n",
    "    for n_sentences in tqdm(eval_n_sentences):\n",
    "        eval_in_subset(run_name,\n",
    "                       compiled_model,\n",
    "                       max_n_words=None,\n",
    "                       max_n_sentences=n_sentences,\n",
    "                       debug=debug,\n",
    "                       device=DEVICE,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval H-LSTM outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../models/report_generation/__init__.py\n",
    "%run ../models/checkpoint/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_name = '0716_234501_h-lstm-att_lr0.0001_densenet-121'\n",
    "# run_name = '0717_015057_h-lstm_lr0.0001_densenet-121'\n",
    "# run_name = '0720_192858_lstm_lr0.0001_densenet-121_size256'\n",
    "# run_name = '0717_041434_lstm_lr0.0001_densenet-121'\n",
    "run_name = '0716_211601_lstm-att_lr0.0001_densenet-121'\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = load_compiled_model_report_generation(run_name,\n",
    "                                                       debug=debug,\n",
    "                                                       device=DEVICE,\n",
    "                                                      )\n",
    "\n",
    "_ = compiled_model.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model.metadata['decoder_kwargs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = compiled_model.metadata['vocab']\n",
    "len(VOCAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../datasets/iu_xray.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_kwargs = {\n",
    "    'max_samples': None,\n",
    "    'frontal_only': False,\n",
    "    'image_size': (512, 512),\n",
    "    'vocab': VOCAB,\n",
    "}\n",
    "\n",
    "train_dataset = IUXRayDataset(dataset_type='train', **dataset_kwargs)\n",
    "val_dataset = IUXRayDataset(dataset_type='val', **dataset_kwargs)\n",
    "test_dataset = IUXRayDataset(dataset_type='test', **dataset_kwargs)\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine, Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../training/report_generation/hierarchical.py\n",
    "%run ../training/report_generation/flat.py\n",
    "%run ../utils/nlp.py\n",
    "%run ../utils/__init__.py\n",
    "%run ../metrics/report_generation/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_hierarchical = compiled_model.metadata['decoder_kwargs']['decoder_name'].startswith('h-')\n",
    "\n",
    "if is_hierarchical:\n",
    "    get_step_fn = get_step_fn_hierarchical\n",
    "    create_dataloader = create_hierarchical_dataloader\n",
    "else:\n",
    "    get_step_fn = get_step_fn_flat\n",
    "    create_dataloader = create_flat_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "free = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = Engine(get_step_fn(compiled_model.model, training=False, free=free, device=DEVICE))\n",
    "attach_metrics_report_generation(tester, hierarchical=is_hierarchical, free=free)\n",
    "attach_report_writer(tester, VOCAB, run_name, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.state.dataloader.dataset.dataset_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.run(dataloader, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.state.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_sample(compiled_model, image, report,\n",
    "                show=True, device=DEVICE, free=False, **kwargs):\n",
    "    report_reader = ReportReader(compiled_model.metadata['vocab'])\n",
    "    \n",
    "    is_hierarchical = compiled_model.metadata['decoder_kwargs']['decoder_name'].startswith('h-')\n",
    "    \n",
    "    # Prepare inputs\n",
    "    images = image.unsqueeze(0).to(device)\n",
    "    if is_hierarchical:\n",
    "        reports = split_sentences_and_pad(report)\n",
    "    else:\n",
    "        reports = torch.tensor(report)\n",
    "\n",
    "    reports = reports.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Pass thru model\n",
    "    if not is_hierarchical:\n",
    "        del kwargs['max_sentences']\n",
    "    tup = compiled_model.model(images, reports, free=free, **kwargs)\n",
    "    \n",
    "    # Parse outputs\n",
    "    if is_hierarchical:\n",
    "        generated = _flatten_gen_reports(tup[0], tup[1])\n",
    "    else:\n",
    "        generated = tup[0]\n",
    "        _, generated = generated.max(dim=-1)\n",
    "\n",
    "    generated = generated.squeeze(0).cpu()\n",
    "    \n",
    "    # Print result\n",
    "    original_report = report_reader.idx_to_text(report)\n",
    "    generated_report = report_reader.idx_to_text(generated)\n",
    "    if show:\n",
    "        print(original_report)\n",
    "        print('-'*20)\n",
    "        print(generated_report)\n",
    "        \n",
    "    return original_report, generated_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 200\n",
    "\n",
    "image, report = train_dataset[idx]\n",
    "\n",
    "gt, gen = eval_sample(compiled_model, image, report,\n",
    "                      free=True, max_sentences=100, max_words=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval metrics on dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.bleu import bleu_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = bleu_scorer.BleuScorer(n=4)\n",
    "s += (gen, [gt])\n",
    "scores_avg, scores_all = s.compute_score()\n",
    "scores_avg, len(scores_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dataset(dataset, max_samples=None, free=False):\n",
    "    scorer = bleu_scorer.BleuScorer(n=4)\n",
    "    report_lens = []\n",
    "    \n",
    "    n_samples = len(dataset) if max_samples is None else max_samples\n",
    "    for idx in tqdm(range(n_samples)):\n",
    "        image, report = dataset[idx]\n",
    "\n",
    "        report_lens.append(len(report))\n",
    "\n",
    "        gt, gen = eval_sample(compiled_model, image, report, show=False,\n",
    "                              free=free, max_sentences=100, max_words=20)\n",
    "\n",
    "        scorer += (gen, [gt])\n",
    "    scores_avg, scores_all = scorer.compute_score()\n",
    "    scores_all = np.array(scores_all)\n",
    "\n",
    "    return scores_avg, scores_all, report_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = eval_dataset(train_dataset, free=True)\n",
    "train_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_results = eval_dataset(val_dataset, free=True)\n",
    "val_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = eval_dataset(test_dataset, free=True)\n",
    "test_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Rouge-L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pycocoevalcap.rouge import rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idx = 800\n",
    "\n",
    "image, report = train_dataset[idx]\n",
    "\n",
    "gt, gen = eval_sample(compiled_model, image, report,\n",
    "                      free=True, max_sentences=100, max_words=100)\n",
    "\n",
    "scorer = rouge.Rouge()\n",
    "scorer.calc_score([gen], [gt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def eval_dataset(dataset, max_samples=None, free=False):\n",
    "    scorer = rouge.Rouge()\n",
    "    scores = []\n",
    "    report_lens = []\n",
    "    \n",
    "    n_samples = len(dataset) if max_samples is None else max_samples\n",
    "    for idx in tqdm(range(n_samples)):\n",
    "        image, report = dataset[idx]\n",
    "\n",
    "        report_lens.append(len(report))\n",
    "\n",
    "        gt, gen = eval_sample(compiled_model, image, report, show=False,\n",
    "                              free=free, max_sentences=100, max_words=20)\n",
    "\n",
    "        scores.append(scorer.calc_score([gen], [gt]))\n",
    "\n",
    "    return np.mean(scores), np.array(scores), report_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_results = eval_dataset(test_dataset, free=True)\n",
    "test_results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### CIDEr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pycocoevalcap.cider import cider_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idx = 800\n",
    "image, report = train_dataset[idx]\n",
    "\n",
    "gt, gen = eval_sample(compiled_model, image, report,\n",
    "                      free=True, max_sentences=100, max_words=100)\n",
    "\n",
    "scorer = cider_scorer.CiderScorer(n=4)\n",
    "scorer += (gen, [gt])\n",
    "\n",
    "idx = 0\n",
    "image, report = train_dataset[idx]\n",
    "\n",
    "gt, gen = eval_sample(compiled_model, image, report,\n",
    "                      free=True, max_sentences=100, max_words=100)\n",
    "scorer += (gen, [gt])\n",
    "\n",
    "scorer.compute_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def eval_dataset(dataset, max_samples=None, free=False):\n",
    "    scorer = cider_scorer.CiderScorer(n=4)\n",
    "    report_lens = []\n",
    "    \n",
    "    n_samples = len(dataset) if max_samples is None else max_samples\n",
    "    for idx in tqdm(range(n_samples)):\n",
    "        image, report = dataset[idx]\n",
    "\n",
    "        report_lens.append(len(report))\n",
    "\n",
    "        gt, gen = eval_sample(compiled_model, image, report, show=False,\n",
    "                              free=free, max_sentences=100, max_words=20)\n",
    "\n",
    "        scorer += (gen, [gt])\n",
    "    scores_avg, scores_all = scorer.compute_score()\n",
    "    scores_all = np.array(scores_all)\n",
    "\n",
    "    return scores_avg, scores_all, report_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val_results = eval_dataset(val_dataset, free=False)\n",
    "val_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_results = eval_dataset(test_dataset, free=False)\n",
    "test_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results = test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_scores = results[1]\n",
    "# all_scores = all_scores.mean(axis=0)\n",
    "# all_scores = all_scores[0, :]\n",
    "all_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lens = np.array(results[2])\n",
    "lens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Plot scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(lens, all_scores)\n",
    "plt.xlabel('Report len')\n",
    "plt.ylabel('Metric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Adjust linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression as LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr = LR()\n",
    "lr.fit(lens.reshape(-1, 1), all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr.coef_, lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Plot hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.hist(all_scores, bins=50)\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Occurences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results[1].mean(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Eval sample with ordered scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results = test_results\n",
    "dataset = test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores_all = results[1]\n",
    "# BLEU\n",
    "# ordered_values = [\n",
    "#     (i, *scores_all[:, i]) \n",
    "#     for i in range(scores_all.shape[1])\n",
    "# ]\n",
    "ordered_values = list(enumerate(scores_all)) # ROUGE-L, CIDEr\n",
    "len(ordered_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ordered_values = sorted(ordered_values, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idx, *scores = ordered_values[2]\n",
    "\n",
    "print(scores)\n",
    "image, report = dataset[idx]\n",
    "\n",
    "gt, gen = eval_sample(compiled_model, image, report,\n",
    "                      free=True, max_sentences=100, max_words=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Distinct words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run ../metrics/report_generation/distinct_words.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dw = DistinctWords()\n",
    "dw.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reports = torch.tensor([[1, 2, 3, 4],\n",
    "                        [21, 12, 1, 0],\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reports2 = reports + 1\n",
    "reports2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dw.update(reports)\n",
    "dw.update(reports2)\n",
    "dw.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dw.words_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Distinct sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run ../metrics/report_generation/distinct_sentences.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ds = DistinctSentences()\n",
    "ds.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reports = torch.tensor([\n",
    "    [[11, 21, 37, 4],\n",
    "     [29, 32, 52, 4],\n",
    "    ],\n",
    "    [[11, 21, 37, 4],\n",
    "     [29, 32, 52, 4],\n",
    "    ],\n",
    "    [[12, 23, 47, 4],\n",
    "     [30, 33, 53, 5],\n",
    "    ],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ds.update(reports)\n",
    "ds.update(reports + 1)\n",
    "ds.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ds.sentences_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Debug others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = np.array([[1, 0, 0],\n",
    "              [0, 1, 0],\n",
    "              [0, 1, 1],\n",
    "             ])\n",
    "b = np.array([[1, 0, 0],\n",
    "              [0, 1, 0],\n",
    "              [0, 1, 1],\n",
    "             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "roc_auc_score(a, b, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run -n ../eval_report_generation_chexpert_labeler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "run_name = '0915_172446_dummy-common-sentences-100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_run(run_name, debug=True, max_samples=30, override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Debug MIRQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run -n ../eval_report_generation_mirqi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Load MIRQI output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/software/MIRQI/testing2.csv')\n",
    "df.fillna('', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "attributes_gt = _attributes_to_list(df['attributes-gt'])\n",
    "attributes_gen = _attributes_to_list(df['attributes-gen'])\n",
    "len(attributes_gt), len(attributes_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['attributes-gt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run -n ../eval_report_generation_mirqi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores = MIRQI_v2(attributes_gt, attributes_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores['MIRQI-v2-attr-p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idx = 2\n",
    "attributes_gt[idx], attributes_gen[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## MIRQI Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run -n ../eval_report_generation_mirqi.py\n",
    "# %run -n ~/software/MIRQI/evaluate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### MIRQI original def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def MIRQI(gt_list, cand_list, pos_weight=0.8, attribute_weight=0.3, verbose=False):\n",
    "    \"\"\"Compute the score of matching keyword and associated attributes between gt list and candidate list.\n",
    "       It returns two scores:   MIRQI-r (recall: hits in gt)\n",
    "                                MIRQI-p (precision: correct ratio of all candidates)\n",
    "    \"\"\"\n",
    "\n",
    "    MIRQI_r = []\n",
    "    MIRQI_p = []\n",
    "    MIRQI_f = []\n",
    "\n",
    "    for gt_report_entry, cand_report_entry in zip(gt_list, cand_list):\n",
    "        attribute_cand_all = []\n",
    "\n",
    "        pos_count_in_gt = 0\n",
    "        pos_count_in_cand = 0\n",
    "        tp = 0.0\n",
    "        fp = 0.0\n",
    "        tn = 0.0\n",
    "        fn = 0.0\n",
    "\n",
    "        for gt_entity in gt_report_entry:\n",
    "            if gt_entity[2] == 'NEGATIVE':\n",
    "                continue\n",
    "            pos_count_in_gt = pos_count_in_gt + 1\n",
    "        neg_count_in_gt = len(gt_report_entry) - pos_count_in_gt\n",
    "\n",
    "        for entity_index, cand_entity in enumerate(cand_report_entry):\n",
    "            if cand_entity[2] == 'NEGATIVE':\n",
    "                for entity_index, gt_entity in enumerate(gt_report_entry):\n",
    "                    if  gt_entity[1] == cand_entity[1]:\n",
    "                        if gt_entity[2] == 'NEGATIVE':\n",
    "                            tn = tn + 1     # true negative hits\n",
    "                            break\n",
    "                        else:\n",
    "                            fn = fn + 1     # false negative hits\n",
    "                            break\n",
    "            else:\n",
    "                pos_count_in_cand = pos_count_in_cand + 1\n",
    "                for entity_index, gt_entity in enumerate(gt_report_entry):\n",
    "                    if gt_entity[1] == cand_entity[1]:\n",
    "                        if gt_entity[2] == 'NEGATIVE':\n",
    "                            fp = fp + 1     # false positive hits\n",
    "                            break\n",
    "                        else:\n",
    "                            tp = tp + 1.0 - attribute_weight    # true positive hits (key words part)\n",
    "                            # count attribute hits\n",
    "                            if gt_entity[3] == '':\n",
    "                                break\n",
    "                            attributes_all_gt = gt_entity[3].split('/')\n",
    "                            attribute_hit_count = 0\n",
    "                            for attribute in attributes_all_gt:\n",
    "                                if attribute in cand_entity[3]:\n",
    "                                    attribute_hit_count = attribute_hit_count + 1\n",
    "                            # true positive hits (attributes part)\n",
    "                            temp = attribute_hit_count/len(attributes_all_gt)*attribute_weight\n",
    "                            tp = tp + temp\n",
    "                            break\n",
    "        neg_count_in_cand = len(cand_report_entry) - pos_count_in_cand\n",
    "        #\n",
    "        # calculate score for positive/uncertain mentions\n",
    "        if pos_count_in_gt == 0 and pos_count_in_cand == 0:\n",
    "            score_r = 1.0\n",
    "            score_p = 1.0\n",
    "        elif pos_count_in_gt == 0 and pos_count_in_cand != 0:\n",
    "            score_r = 0.0\n",
    "            score_p = 0.0\n",
    "        elif pos_count_in_gt != 0 and pos_count_in_cand == 0:\n",
    "            score_r = 0.0\n",
    "            score_p = 0.0\n",
    "        else:\n",
    "            score_r = tp / (tp + fn + 0.000001)\n",
    "            score_p = tp / (tp + fp + 0.000001)\n",
    "\n",
    "        # calculate score for negative mentions\n",
    "        # if neg_count_in_cand != 0 and neg_count_in_gt != 0:\n",
    "        if tn != 0:\n",
    "            score_r = score_r * pos_weight + tn / (tn + fp + 0.000001) * (1.0 - pos_weight)\n",
    "            score_p = score_p * pos_weight + tn / (tn + fn + 0.000001) * (1.0 - pos_weight)\n",
    "\n",
    "        MIRQI_r.append(score_r)\n",
    "        MIRQI_p.append(score_p)\n",
    "        rec_prec = (score_r + score_p)\n",
    "        MIRQI_f.append(2*(score_r * score_p) / rec_prec if rec_prec != 0.0 else 0.0)\n",
    "\n",
    "    scores = {\n",
    "        'MIRQI-r': MIRQI_r,\n",
    "        'MIRQI-p': MIRQI_p,\n",
    "        'MIRQI-f': MIRQI_f,\n",
    "    }\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Robust matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Repeated nodes with different attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "report_gt = \"right effusion with mild atelectasis. left effusion is also present.\"\n",
    "entities_gt = [\n",
    "    ['effusion', 'Pleural Effusion', 'POSITIVE', 'right/present'],\n",
    "    ['effusion', 'Pleural Effusion', 'POSITIVE', 'left/pleural'],\n",
    "]\n",
    "report_gen = report_gt\n",
    "entities_gen = [\n",
    "    ['effusion', 'Pleural Effusion', 'POSITIVE', 'left/pleural'],\n",
    "    ['effusion', 'Pleural Effusion', 'POSITIVE', 'right/present'],\n",
    "]\n",
    "{\n",
    "    **MIRQI([entities_gt], [entities_gen]),\n",
    "    **MIRQI_v2([entities_gt], [entities_gen]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### GT nodes matched twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "report_gt = \"right pleural effusion.\"\n",
    "entities_gt = [\n",
    "    ['effusion', 'Pleural Effusion', 'POSITIVE', 'right'],\n",
    "]\n",
    "report_gen = \"right pleural effusion. left pleural effusion\"\n",
    "entities_gen = [\n",
    "    ['effusion', 'Pleural Effusion', 'POSITIVE', 'right'],\n",
    "    ['effusion', 'Pleural Effusion', 'POSITIVE', 'left'],\n",
    "]\n",
    "{\n",
    "    **MIRQI([entities_gt], [entities_gen]),\n",
    "    **MIRQI_v2([entities_gt], [entities_gen]),\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
